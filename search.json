{
  "articles": [
    {
      "path": "about.html",
      "title": "About",
      "description": "Reach out to your instructors for assistance. Learn more about them here.",
      "author": [],
      "contents": "\r\n\r\nContents\r\nLarry\r\nNour\r\nNathan\r\nThe Catscot\r\n\r\nLarry\r\n\r\n\r\n\r\nLarry has been growing out his hair.\r\nOH:\r\nNour\r\n\r\n\r\n\r\nNour, in cosplay as a butterfly. Pretty spot on, eh?\r\nOH: Thursday, 9am - 10am\r\nNathan\r\n\r\n\r\n\r\nNathan is a professional potato.\r\nOH: Tuesday, 11am - 12pm\r\nThe Catscot\r\n\r\n\r\n\r\nOur little mascot in the upper-left hand corner was Designed courtesy of 699pic at pngtree.com\r\n\r\n\r\n\r\n",
      "last_modified": "2021-08-31T12:48:40-04:00"
    },
    {
      "path": "broken.html",
      "title": "Broken Code",
      "author": [],
      "contents": "\r\nSome tips for code that won’t run\r\nDon’t hesitate to ask for help if you are having problems with software, code, etc. But be warned, your question should be specific and detailed, not just “Why won’t this line of code run?”\r\nIn my experience, unsent emails are sometimes better (and faster) than a TA. What looks like a hairy knot in your code is often quick to untangle when you start putting things in writing. This is why we ask that any email you send be both detailed and specific.\r\nHere are some other things that may help in getting your code to work:\r\nCheck that you haven’t mixed up == and =\r\nGo for a walk (seriously, maybe you’ve been sitting for too long)\r\nGoogle your error message – there’s almost always someone else who has encountered a similar problem\r\nDon’t delete and make changes to the same line of code. Copy/paste into a new chunk. Formulate a hypothesis. Make changes. Take notes on what changed. Revise your hypothesis. Repeat.\r\nLearn the R Debugger (The code for 705’s lab probably won’t need the debugger, but if you’re getting deep into things, ask Nathan about it)\r\nYou might also flip through this article on commmon problems in R Markdown, which might provide some answers.\r\n\r\n\r\n\r\n",
      "last_modified": "2021-08-31T12:48:41-04:00"
    },
    {
      "path": "data_dict.html",
      "title": "Data Dictionary",
      "description": "Data Dictionary for Kenya's 2008 Demographic and Health Survey.\n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\nData Dictionary Help\r\nKenya DHS Data Dictionary (file)\r\nCentury Month Code\r\n\r\n\r\n\r\n\r\n\r\nData Dictionary Help\r\nGuidelines for Data Collection & Data Entry - Theresa A Scott, MS (Vanderbilt University)\r\nKenya DHS Data Dictionary (file)\r\n\r\n\r\n{\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"caseid\",\"bord\",\"b3\",\"b4\",\"b5\",\"b7\",\"m2n\",\"m18\",\"m19\",\"v008\",\"v011\",\"v025\",\"v437\",\"v438\",\"v463z\",\"s109\"],[\"Used to uniquely identify each mother.\",\"Birth order\",\"Century month code for the date of birth of the child (see note below on century month codes).\",\"Sex of child.\",\"Whether child was alive or dead at the time of interview.\",\"Age at death of the child in completed months gives a calculated age at death from the reported information. This variable occupies three digits. BASE: Dead children (B5 = 0).\",\"Indicator of no prenatal care.\",\"Size of child as reported subjectively by the mother.\",\"Weight of child at birth given in kilograms with three implied decimal places. Divide by 1000 to produce weight in kilograms. Children who were not weighed are coded 9996\",\"Century month code of date of interview (see note on century month codes).\",\"Century month code of date of birth of the mother (see note on century month codes).\",\"Type of place of residence. \",\"Weight of the mother in kilograms at time of interview. There is one implied decimal place in the weight (decimal points are not included in the data file). To produce the weight in kilograms divide by 10.\",\"Height of the mother in centimeters at time of interview. There is one implied decimal place in the height (decimal points are not included in the data file). To produce the height in centimeters divide by 10.\",\"Indicator of no smoking.\",\"Education of the mother.\"],[\"Character\",\"Integer\",\"Integer\",\"1.   male; \\n2. female\",\"0.   dead; 1. alive\",\"Integer\",\"0.   some prenatal care; \\n1. no prenatal care\",\"1.   very large; \\n2. large; \\n3. average; \\n4. smaller than average; \\n5. very small; \\n8. don't know; \\n9. missing\",\"Integer (note implied decimal places)\",\"Integer\",\"Integer\",\"1.   urban; \\n2. rural\",\"Integer (note implied decimal places)\",\"Integer (note implied decimal places)\",\"0.   smokes something; \\n1. smokes nothing; \\n9. missing\",\"0.   did not attend school; \\n1. primary; \\n2. post-primary/vocational; \\n3. seoncdary / a-level; \\n4. college (middle level); \\n5. university\"]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th>Variable<\\/th>\\n      <th>Description<\\/th>\\n      <th>Coding<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"pagelength\":20,\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}\r\nCentury Month Code\r\nAll dates in the data file are expressed in terms of months and years and also as century month codes. A century month code (CMC) is the number of the month since the start of the century.\r\nFor example, January 1900 is CMC 1, January 1901 is CMC 13, January 1980 is CMC 961, September 1994 is CMC 1137.\r\nThe CMC for a date is calculated from the month and year as follows: CMC = (YY * 12) + MM for month MM in year 19YY.\r\nTo calculate the month and year from the CMC use the following formulae: YY = int((CMC - 1) / 12) MM = CMC - (YY * 12)\r\nFor Dates in 2000 and after the CMC is calculated as follows: CMC = ((YYYY-1900) * 12) + MM for month MM in year YYYY.\r\nTo calculate the month and year from the CMC use the following formulae: YYYY = int((CMC - 1) / 12)+1900 MM = CMC - ((YYYY-1900) * 12)\r\n\r\n\r\n\r\n",
      "last_modified": "2021-08-31T12:48:42-04:00"
    },
    {
      "path": "fun_dict.html",
      "title": "Function Dictionary",
      "description": "A searchable table of all relevant functions for 705 and 707 labs\n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\n\r\n\r\n\r\n\r\n{\"x\":{\"filter\":\"top\",\"vertical\":false,\"filterHTML\":\"<tr>\\n  <td data-type=\\\"character\\\" style=\\\"vertical-align: top;\\\">\\n    <div class=\\\"form-group has-feedback\\\" style=\\\"margin-bottom: auto;\\\">\\n      <input type=\\\"search\\\" placeholder=\\\"All\\\" class=\\\"form-control\\\" style=\\\"width: 100%;\\\"/>\\n      <span class=\\\"glyphicon glyphicon-remove-circle form-control-feedback\\\"><\\/span>\\n    <\\/div>\\n  <\\/td>\\n  <td data-type=\\\"character\\\" style=\\\"vertical-align: top;\\\">\\n    <div class=\\\"form-group has-feedback\\\" style=\\\"margin-bottom: auto;\\\">\\n      <input type=\\\"search\\\" placeholder=\\\"All\\\" class=\\\"form-control\\\" style=\\\"width: 100%;\\\"/>\\n      <span class=\\\"glyphicon glyphicon-remove-circle form-control-feedback\\\"><\\/span>\\n    <\\/div>\\n  <\\/td>\\n  <td data-type=\\\"character\\\" style=\\\"vertical-align: top;\\\">\\n    <div class=\\\"form-group has-feedback\\\" style=\\\"margin-bottom: auto;\\\">\\n      <input type=\\\"search\\\" placeholder=\\\"All\\\" class=\\\"form-control\\\" style=\\\"width: 100%;\\\"/>\\n      <span class=\\\"glyphicon glyphicon-remove-circle form-control-feedback\\\"><\\/span>\\n    <\\/div>\\n  <\\/td>\\n  <td data-type=\\\"character\\\" style=\\\"vertical-align: top;\\\">\\n    <div class=\\\"form-group has-feedback\\\" style=\\\"margin-bottom: auto;\\\">\\n      <input type=\\\"search\\\" placeholder=\\\"All\\\" class=\\\"form-control\\\" style=\\\"width: 100%;\\\"/>\\n      <span class=\\\"glyphicon glyphicon-remove-circle form-control-feedback\\\"><\\/span>\\n    <\\/div>\\n  <\\/td>\\n  <td data-type=\\\"character\\\" style=\\\"vertical-align: top;\\\">\\n    <div class=\\\"form-group has-feedback\\\" style=\\\"margin-bottom: auto;\\\">\\n      <input type=\\\"search\\\" placeholder=\\\"All\\\" class=\\\"form-control\\\" style=\\\"width: 100%;\\\"/>\\n      <span class=\\\"glyphicon glyphicon-remove-circle form-control-feedback\\\"><\\/span>\\n    <\\/div>\\n  <\\/td>\\n<\\/tr>\",\"data\":[[\"Import\",\"Import\",\"Import\",\"Operator\",\"Explore\",\"Explore\",\"Explore\",\"Explore\",\"Explore\",\"Explore\",\"Explore\",\"Explore\",\"Explore\",\"Stats\",\"Stats\",\"Stats\",\"Stats\",\"Operator\",\"Transform\",\"Transform\",\"Transform\",\"Transform\",\"Conditional\",\"Conditional\",\"Transform\",\"Tabular analysis\",\"Transform\",\"Plot\",\"Plot\",\"Plot\",\"Plot\",\"Export\",\"Export\",\"Explore\",\"Stats\",\"Explore\",\"Transform\",\"Transform\",\"Plot\",\"Transform\",\"Explore\",\"Stats\",\"Stats\",\"Stats\",\"Stats\",\"Tabular analysis\",\"Measures of Association\",\"Measures of Association\",\"Measures of Association\",\"Measures of Association\",\"Measures of Association\",\"Measures of Association\"],[\"install.packages()\",\"library()\",\"readRDS()\",\"&lt;-\",\"head()\",\"length()\",\"ncol()\",\"nrow()\",\"dim()\",\"names()\",\"summary()\",\"skim()\",\"summarize()\",\"mean()\",\"median()\",\"sd()\",\"IQR()\",\"%&gt;%\",\"sum()\",\"mutate()\",\"round()\",\"as.integer()\",\"case_when()\",\"ifelse()\",\"factor()\",\"table()\",\"proportions()\",\"ggplot()\",\"geom_histogram()\",\"geom_boxplot()\",\"labs()\",\"ggsave()\",\"saveRDS()\",\"class()\",\"range()\",\"is.na()\",\"filter()\",\"select()\",\"facet_wrap()\",\"group_by()\",\"count()\",\"t.test()\",\"kruskal.test()\",\"chisq.test()\",\"fisher.test()\",\"CreateTableOne()\",\"flipTable()\",\"mAssoc()\",\"riskdifference()\",\"riskratio()\",\"epi.2by2()\",\"epiHomog()\"],[\"package = \\\" \\\"\",\"package\",\"file = \\\" \\\"\",null,\"object\",\"object\",\"x\",\"x\",\"x\",\"x\",\"object\",\"data frame\",\"piped data (implicit), list of summary statistics\",\"object\",\"object\",\"object\",\"object\",null,\"object\",\"piped data (implicit), name-value pairs\\nex: mutate(newColumnName = oldColumn + 1)\",\"x, digits\",\"x\",\"conditional statement ~ result, …\",\"condition, result if true, result if false\",\"column of data frame\",\"column(s) of a data frame\",\"table object, margin\",\"data, aes(x = , y = )\",\"aes(x = ), binwidth = \",\"aes(x = )\",\"title, x, y, etc.\",\"filename = \\\" \\\"\",\"object, file = \\\" \\\"\",\"object\",\"Any numeric or character objects\",\"x\",\"piped data (implicit), conditional expression\",\"piped data (implicit), column names separated by commas\",\". ~ variable\",\"piped data (implicit), column names\",\"piped data (implicit)\",\"x, y (optional)\",\"x, y (optional)\",\"x, y (optional)\",\"x, y (optional)\",\"vars, strata (optional), data\",\"table object\",\"table object with exposure-outcome in top-left\",\"a = outcome+, b = outcome-, N1 = exposed+, N0 = exposed-\",\"a = outcome+, b = outcome-, N1 = exposed+, N0 = exposed-\",\"table object with exposure-outcome in top-left\",\"table object with exposure-outcome in top-left\"],[\"The name of the package you'd like to install, in quotes\",\"Loads a package and makes it available for use in R coding environment\",\"Give a file path that points to the .rds dataset you would like to load\",\"Used to assign a name to R output, thereby saving it to your R environment for later use\",\"Returns the top 6 items from an R object, like a data frame. Specify the exact number with n = \",\"Returns the length of an R object\",\"Returns the number of columns present in x\",\"Returns the number of rows present in x\",\"Returns the dimensions of an object.\",\"Returns the names of an object (e.g. the column names of a dataset)\",\"Provides summary data of an object. For data frames, these are summary stats like mean, median, etc.\",\"Alternative to summary(). Provides a broad overview of a data frame.\",\"Can be used in combination with group_by(). Will output summary statistic for each group.\",\"Returns the mean for numeric/logical vectors. \",\"Returns the median for numeric vectors.\",\"Returns the standard deviation for numeric vectors.\",\"Returns the interquartile range for numeric vectors.\",\"The R equivalent of \\\"and then\\\". Relays a data frame from one function to the next.\",\"Returns the sum of all values present in its arguments/object\",\"Adds a new variable (name) to a dataframe according to a value, hence \\\"name-value pair\\\". \",\"Rounds the values in x to the specified number of decimal places in \\\"digits = \\\"\",\"Takes floating point numbers (those with decimals) and converts them to integers\",\"See vignette. Used within mutate() to evaluate columns on conditional (if/else) statements, and returns a result for each row.\",\"Takes three arguments: a conditional statement, the result if it's true, and the result if it's false. Can be used with mutate()\",\"Converts numberic and character variables to \\\"factor\\\" categorical variables\",\"Provides one-way, two-way, and multi-way tabulation of variables from a dataset\",\"Returns conditional proportions. margin = 1 gives row proportions. margin = 2 gives column proportions.\",\"Initializes a ggplot object.\",\"Visualize the distribution of a single continuous variable by dividing x-axis into bins. Set width of bins with binwidth = \",\"Displays boxplot of a continuous variable. Stratify boxplots by specifying a y aesthetic (\\\"aes(x = , y = )\\\"\",\"Modify title, axis, and other plot labels\",\"Takes the most recent graphic run using ggplot and saves it to a file path given to filename = . Remember to use quotes around the file path and provide the file extension (.png, .jpg, .eps, etc.)\",\"Takes a data object and saves it to the file path designated by file = \\\" \\\". Don't forget to include file extension .rds\",\"Tells you the object's type. For an entire dataframe, this means the data type in each column. \",\"Returns the minimum and maximum of all the given arguments (A-Z, lowest-highest)\",\"Used to check values as missing. Commonly used to filter() based on missing values in a given column (eg. filter(is.na(variable)) )\",\"Used to subset a data frame, where it keeps all rows that satisfies the conditional expression (returns TRUE)\",\"Used to select specific columns in a dataset. Use -variable to drop a variable from a data frame and keep the rest.\",\"Used to stratify a visualization by an extra variable.\",\"Groups a data frame by sub-groups in specified column names, at which point you can perform operations within those subgroups using summarize(n(), mean(), median(), etc.)\",\"When used along with group_by(), returns subgroup frequency counts.\",\"Takes x, a numeric vector of data values (a list of numbers, like a column in a data frame), and performs one and two-way t-tests on those vectors.\",\"Takes x, a numeric vector of data values (a list of numbers, like a column in a data frame), and performs a Kruskal-Wallis rank sum test on those vectors.\",\"Takes x, a numeric vector of data values (a list of numbers, like a column in a data frame), and performs chi-squared contingency table tests and goodness-of-fit tests\",\"Takes x, a numeric vector of data values (a list of numbers, like a column in a data frame), and performs Fisher's exact test for testing the null of independence of rows and columns in a contingency table.\",\"Takes a vector of variables, vars, from a dataset and generates summary statistics for them. If strata is given, stratifies by strata.\",\"Takes a table object generated by table() and diagonally inverts the position of its rows and columns so that the top-right is now the bottom-left, etc.\",\"Returns Measures of assocation. For 2x2 tables, returns RD, RR and OR according to referent variable. For stratified 2x2 tables, returns pooled and crude measures of association.\",\"Takes +/- outcomes by exposure and calculates risk difference and confidence intervals\",\"Takes +/- outcomes by exposure and calculates risk ratio and confidence intervals\",\"Computes summary measures of risk. When exposure variable has more than one level, use mAssoc() instead.\",\"Takes a stratified table and returns the result of a chi-square test of homogeneity on the various strata's risk differences\"],[\"utils\",\"base R\",\"base R\",\"base R\",\"utils\",\"base R\",\"base R\",\"base R\",\"base R\",\"base R\",\"base R\",\"skimr\",\"dplyr\",\"base R\",\"stats\",\"stats\",\"stats\",\"dplyr\",\"base R\",\"dplyr\",\"base R\",\"base R\",\"dplyr\",\"base R\",\"base R\",\"base R\",\"base R\",\"ggplot2\",\"ggplot2\",\"ggplot2\",\"ggplot2\",\"ggplot2\",\"base R\",\"base R\",\"base R\",\"base R\",\"dplyr\",\"dplyr\",\"ggplot2\",\"dplyr\",\"dplyr\",\"stats\",\"stats\",\"stats\",\"stats\",\"tableone\",\"epiAssist\",\"epiAssist\",\"fmsb\",\"fmsb\",\"epiR\",\"epiAssist\"]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th>Task<\\/th>\\n      <th>Function<\\/th>\\n      <th>Arguments<\\/th>\\n      <th>Description<\\/th>\\n      <th>Package<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"pagelength\":20,\"order\":[],\"autoWidth\":false,\"orderClasses\":false,\"orderCellsTop\":true}},\"evals\":[],\"jsHooks\":[]}\r\n\r\n\r\n\r\n",
      "last_modified": "2021-08-31T12:48:44-04:00"
    },
    {
      "path": "help_ggplot2.html",
      "title": "Visualizations with {ggplot2}",
      "description": "Histograms, boxplots, and pointalism using {ggplot2}.\n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\nGoals:\r\nExternal resources\r\nFor the visually inclined: RFun, Visualization with ggplot2 with John Little\r\nFor the linguistically inclined:\r\nFor the theoretically inclined:\r\nFor the obsessively inclined:\r\nFor the listically inclined:\r\n\r\nCore competencies for 705 lab\r\nCreate a new ggplot with ggplot()\r\nAdd a dataset to the plot\r\nAssign x and y aesthetics with aes():\r\nCreate a histogram with geom_histogram()\r\nCreate a boxplot with geom_boxplot()\r\nGenerate a stratified boxplot by adding a y aesthetic:\r\nCreate titles and axis labels with labs()\r\nBONUS: Generating multiple stratified plots with facet_wrap() or filter()\r\n\r\n\r\nGoals:\r\nBy following along with this document, you will know how to:\r\nRead and interpret basic code in {ggplot2}’s grammar of graphics\r\nInitialize a plotting field using ggplot()\r\nMap x and y aesthetics to a plot’s axes using aes()\r\nGenerate histograms with geom_histogram()\r\nGenerate boxplots and stratified boxplots with geom_boxplot()\r\nCreate titles and axis labels with labs()\r\nSave a plot with ggsave()\r\nBasically, this help document will provide you with the tools necessary to complete the labs for GLHLTH 705.\r\nExternal resources\r\nIf you’re interested in going one level deeper, we highly recommend you check out the following resources, which will give a better introduction to {ggplot2} than we ever could (also why we’re plugging these at the top).\r\nAs R rule of thumb, it’s good to have multiple mediums of exposure to the same idea. We recommend you pick the one that suits your learning style and come back for more later:\r\nFor the visually inclined: RFun, Visualization with ggplot2 with John Little\r\nJohn Little is nothing short of the world’s best librarian.\r\nFor the linguistically inclined:\r\nR for Data Science, Chapter 3, by Hadley Wickham\r\nAfter a preface and introduction, this is the first actual chapter in R4DS. The rationale is that {ggplot2} is actually pretty fun and satisfying to use. It’s pretty well guaranteed to have you hooked if you give it a chance.\r\nFor the theoretically inclined:\r\n“A Layered Grammar of Graphics” by Hadley Wickham\r\nPublished in the Journal of Computational and Graphical Statistics, 2010\r\nFor the obsessively inclined:\r\nggplot2: Elegant Graphics for Data Analysis, by Hadely Wickham\r\n**cough** Also by Hadley Wickham. (look it up using the Duke Library search engine and log in with your Duke credentials\r\nFor the listically inclined:\r\nReference page of ggplot2 commands\r\nCore competencies for 705 lab\r\nFor those of you who made it through that onslaught of links without clicking on a single one, welcome to the Core Competencies section. We hope that this section is somehow dry enough that you go and find your answers in one of the resources above. But for those of you who are still feeling stubborn:\r\nCreate a new ggplot with ggplot()\r\nTo initialize a plotting space, we first need to tell R that we want to use ggplot. If we just call ggplot() and run it without any data, we get a blank field. This is our sandbox:\r\n\r\n\r\nggplot()\r\n\r\n\r\n\r\n\r\nAdd a dataset to the plot\r\nFor this example, we’ll use the dataset available in base R called iris, which provides measurements and species data on a bunch of – you guessed it – irises.\r\n\r\n\r\n\r\n\r\nSepal.Length\r\nSepal.Width\r\nPetal.Length\r\nPetal.Width\r\nSpecies\r\n5.1\r\n3.5\r\n1.4\r\n0.2\r\nsetosa\r\n4.9\r\n3.0\r\n1.4\r\n0.2\r\nsetosa\r\n4.7\r\n3.2\r\n1.3\r\n0.2\r\nsetosa\r\n4.6\r\n3.1\r\n1.5\r\n0.2\r\nsetosa\r\n5.0\r\n3.6\r\n1.4\r\n0.2\r\nsetosa\r\n5.4\r\n3.9\r\n1.7\r\n0.4\r\nsetosa\r\n\r\n\r\n\r\n\r\nFigure 1: A bearded iris\r\n\r\n\r\n\r\nWe can add the dataframe to our plot by including it in the argument data =.\r\nSuperficially, this doesn’t change our output:\r\n\r\n\r\nggplot(data = iris)\r\n\r\n\r\n\r\n\r\nBut the data frame is now a part of the plot. One way to verify this is by assigning the two previous plots a name and inspecting their size with object.size(). The plot with the data should be bigger:\r\n\r\n\r\nwithout_data <- ggplot()\r\n\r\nobject.size(without_data)\r\n#> 3720 bytes\r\n\r\nwith_data <- ggplot(data = iris)\r\n\r\nobject.size(with_data)\r\n#> 10704 bytes\r\n\r\n\r\n\r\nAssign x and y aesthetics with aes():\r\nNext, we need to tell ggplot which variables we’re working with, and where to put them (their “aesthetic mapping”). We do this using the argument aes(x = variable1, y = variable2)\r\nIf we’re creating histograms and singular boxplots, we only require a single variable on the x-axis. We can initialize it as follows:\r\n\r\n\r\nggplot(data = iris, mapping = aes(x = Sepal.Length))\r\n\r\n\r\n\r\n\r\nSee how ggplot assigned Sepal.Length to the x axis?\r\nCreate a histogram with geom_histogram()\r\nOkay, let’s cut to the chase. We want a plot.\r\n{ggplot2} has a large number of plotting types and styles. Given the types of variables we’ve mapped to ggplot’s aesthetics, all we need to do is choose a type of plot appropriate for that type of variable, and add it as a new layer with +\r\n\r\n\r\nggplot(data = iris, mapping = aes(x = Sepal.Length)) +\r\n  geom_histogram()\r\n\r\n\r\n\r\n\r\nMany plots also allow us to add color with the argument aes(fill = \"colorname\") (colors are always written as strings, in quotes!).\r\nWe may also change the size of our bins with argument binwidth = x:\r\n\r\n\r\nggplot(data = iris, mapping = aes(x = Sepal.Length)) +\r\n  geom_histogram(fill = \"#12BBAC\", binwidth = .25)\r\n\r\n\r\n\r\n\r\n\r\nRefer here for a list of color options available in R. Along with default color names, R also accepts hexadecimal color codings. For fun, I’ve made our histogram the same color as this website’s navbar. Did it work?\r\nCreate a boxplot with geom_boxplot()\r\nA single boxplot functions in the same exact manner. Instead of geom_histogram(), we add a boxplot layer with geom_boxplot(). This time, I’ve used a default color name, goldenrod2, instead of a hexadecimal color code:\r\n\r\n\r\nggplot(data = iris, mapping = aes(x = Sepal.Length)) +\r\n  geom_boxplot(fill = 'goldenrod2')\r\n\r\n\r\n\r\n\r\nGenerate a stratified boxplot by adding a y aesthetic:\r\nWe can create multiple boxplots within a single plot by adding a categorical variable as a second aesthetic. The iris dataset contains a categorical variable, Species, which would be appropriate for this task:\r\n\r\n\r\nggplot(data = iris, mapping = aes(x = Sepal.Length, y = Species)) +\r\n  geom_boxplot(fill = '#AC12BB')\r\n\r\n\r\n\r\n\r\nWe can also display the boxplots vertically by assigning Sepal.Length to y = and Species to x = :\r\n\r\n\r\nggplot(data = iris, mapping = aes(y = Sepal.Length, x = Species)) +\r\n  geom_boxplot(fill = '#AC12BB')\r\n\r\n\r\n\r\n\r\nCreate titles and axis labels with labs()\r\nFinally, we need to make our plots fit for public use… it needs axis labels and a title. We can specify these by adding another layer to our plot, labs(). Make sure you write your labels as strings:\r\n\r\n\r\nggplot(data = iris, mapping = aes(y = Sepal.Length, x = Species)) +\r\n  geom_boxplot(fill =  \"#12BBAC\") + \r\n  labs(x = \"Species\", \r\n       y = \"Sepal Length\", \r\n       title = \"Boxplots of Sepal Length of Irises by Species\")\r\n\r\n\r\n\r\n\r\nBONUS: Generating multiple stratified plots with facet_wrap() or filter()\r\nYou might be wondering what this sort of stratification might look life if we tried the same thing with a histogram. Can a histogram accept a y aesthetic? When we try and assign a second aesthetic to a histogram, we get the following result:\r\n\r\n\r\nggplot(data = iris, mapping = aes(x = Species, y = Sepal.Length)) +\r\n  geom_histogram(fill = \"goldenrod2\")\r\n\r\n#> Error: stat_bin() can only have an x or y aesthetic.\r\n\r\n\r\n\r\nfacet_wrap()\r\nAn easy way to generate stratified histograms is with the additional layer, facet_wrap(), which takes a formula in the following syntax:\r\n. ~ stratifyingVariable\r\nThe period here represents our ggplot object. We put the stratifying variable on the right side of the formula as a way to designate it as the “independent variable” of sorts. The output, . , depends on whatever categorical we assign as our faceting variable. In this case, the histograms dependson the variable Species:\r\n\r\n\r\nggplot(data = iris, mapping = aes(x = Sepal.Length)) + \r\n  geom_histogram(fill = \"goldenrod2\") +\r\n  facet_wrap(. ~ Species) + \r\n    labs(x = \"Sepal Length\", \r\n         y = \"Count\",\r\n         title = \"Histograms of Sepal Length of Irises by Species\")\r\n\r\n\r\n\r\n\r\nWe might decide that we want the plots stacked vertically instead of horizontally to help us better compare their distributions by Sepal Length. We can do that too, with the argument nrow =:\r\n\r\n\r\nggplot(data = iris, mapping = aes(x = Sepal.Length)) + \r\n  geom_histogram(fill = \"#AC12BB\") +\r\n  facet_wrap(. ~ Species, nrow = 3) +\r\n  labs(x = \"Sepal Length\",\r\n       y = \"Count\",\r\n       title = \"Histograms of Sepal Length of Irises by Species\")\r\n\r\n\r\n\r\n\r\npipe and filter()\r\nWhat if we only want the Sepal Lengths for the species Iris virginica?\r\nOne way would be to use filter(), which we connect to our ggplot using a pipe:\r\n\r\n\r\niris %>%\r\n  filter(Species == \"virginica\") %>%\r\n  ggplot(mapping = aes(x = Sepal.Length)) +\r\n  geom_histogram(fill = \"#12BBAC\") + \r\n  labs(x = \"Sepal Length\", \r\n       title = \"Histograms of Sepal Length of Irises by Species\")\r\n\r\n\r\n\r\n\r\n\r\nNote that when we use a pipe for a ggplot, we’ve already specified the dataset at the top of the code, so we don’t need to type it again when we call ggplot(). It already knows what data we’re working with because iris is funneled along the pipeline!\r\n\r\n\r\n\r\n",
      "last_modified": "2021-08-31T12:49:34-04:00"
    },
    {
      "path": "help_jargon.html",
      "title": "R Jargon",
      "description": "How to talk like an R nerd. Here, we detail various R-related vocabulary, the familiarity of which will make your life a lot easier.  \n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\nR Vocabulary List\r\n$\r\n#\r\nargument\r\nassignment operator\r\nbase R\r\ncode chunk\r\nconsole\r\ndebugging\r\ndirectory\r\ndplyr\r\nfactor\r\nfunction\r\nggplot2\r\ngit\r\nHTML\r\nindex\r\nknit\r\nlibrary/packages\r\nmagrittr/pipe/%>%\r\nparameter\r\nreproducible expressions (reprex)\r\nstring/character\r\ntibble\r\ntidy data\r\ntidyverse\r\ntraceback\r\ntribble\r\nvector\r\nvignette\r\nWickham, Hadley\r\nYAML header\r\n\r\n\r\nDoes a pesky word keep popping up that’s given you the sneaking suspicion that you have no idea what you’re doing?\r\nI’ve got good news, but also some bad news.\r\nThe bad: The person writing this probably has no idea what they’re doing either. And unless you study computer engineering, there’s a good chance you will never really “know” what you are “doing”.\r\nThe good: A quick vocabulary lesson will help do away with that sneaking suspicion, at least temporarily.\r\nR Vocabulary List\r\nSearch this document using ctrl+f for a brief definition of that word or symbol and external links for further reading. Can’t find the word here? Try R Documentation or Google.\r\n$\r\nUsed to specify a specific variable within a list-like object (like a data.frame, a tibble, a model, an actual list created with list())\r\nExample:\r\n\r\n\r\n# this code uses head() to view the first six observations for the variable \"cyl\" in dataset \"mtcars\"\r\n\r\nhead(mtcars$cyl)\r\n#> [1] 6 6 4 6 8 6\r\n\r\n\r\n\r\n#\r\nIn a chunk of code, # is used to comment out text. This tells the R console to ignore that line. This is useful for making comments in your code.\r\nOutside of code chunks, a series of hashes on a new line, followed by a space and some text, indicate different levels for document subheadings, like this:\r\nIn R Markdown, type:\r\n# one hash\r\n## two hashes\r\n### three hashes\r\n#### four hashes\r\n##### five hashes\r\nKnitting in HTML renders as:\r\n one hash \r\n two hashes \r\n three hashes \r\n four hashes \r\n five hashes \r\nargument\r\nThe various pieces of data necessary for a function to run. Many functions have arguments with default values.\r\nFor example, for many statistical tests of significance, the significance level is set to a default of 0.95. If you don’t include this argument in your function, it will refer to the default and use it.\r\nIn a function’s documentation, under “Usage”, you can tell when an argument has a default because the argument’s name is equated to a value.\r\nAs another example, the function head() takes two main arguments, an object, x, and n, the number of observations you’d like for head() to print.\r\nGo to the help documentation for head() (?head) and find what its default is. Or maybe you already have noticed its default when you’ve run head() in your labs.\r\nassignment operator\r\nThe <- is called the Assignment Operator. We can use it to assign names to objects in our coding environment:\r\nWe can use our assignment operator for characters, numbers, logical operators, etc.:\r\n\r\n\r\nfruit <- c(\"oranges\", \"papayas\", \"apricots\")\r\n\r\nnumber <- 99\r\n\r\nlogical <- FALSE\r\n\r\n\r\n\r\nNow that the above values are stored in our environment, we can use them in other functions or operations:\r\n\r\n\r\npaste(fruit, \"are orange\", sep = \" \")\r\n#> [1] \"oranges are orange\"  \"papayas are orange\"  \"apricots are orange\"\r\n\r\nnumber + 1\r\n#> [1] 100\r\n\r\nisTRUE(logical)\r\n#> [1] FALSE\r\n\r\n\r\n\r\nbase R\r\nThese are functions that are a part of the original R programming language, and so do not require a call to a package using library().\r\nGo here to see a complete list of functions that come with the R Base Package\r\ncode chunk\r\nIn R Markdown, code chunks look like this:\r\n```{r}\r\n```\r\nAnything written between these two lines can be sent to the R Console and run as code. Anything not bound within these lines is interpreted as text, and is printed as-is in a rendered document.\r\nconsole\r\nIn lieu of running code in your R Markdown document, you can type it directly into the window that says “Console”.\r\nWhen you click “Run” on any R Markdown code, that code gets run in the Console.\r\ndebugging\r\nThe process of identifying and fixing problems in your code. A debugger is a program that walks through your code, line-by-line, allowing you to inspect elements within the environment as the code runs.\r\nThis becomes more useful when you’re writing your own functions and are confused as to why they’re behaving a certain way.\r\ndirectory\r\n“The working directory of a process is a directory of a hierarchical file system”\r\nA directory is any file folder on your computer.\r\nYour root directory is the top-most directory on your computer (in Windows, this is the folder calls “C:”, for Mac users, it’s usually labelled as “Macintosh HD”)\r\nIn R, your working directory is typically the folder that contains whatever R Project you have open.\r\nSay you have a dataset called “data.rds” in your main working directory. You can import it using any number of functions by referring to its file path as simply “data.rds”.\r\nBut say you have that dataset in a series of folders within your working directory. The folders are organized like this, which each subsequent folder inside the last, and the data file in the folder titled “data”:\r\nworking directory > try1 > fullAnalysis > data\r\nThe “file path” for referring to your data file from your working directory would be this:\r\n“try1/fullAnalysis/data/data.rds”\r\ndplyr\r\nA package that contains a set of functions that help solve “the most common data manipulation challenges”. Main functions include:\r\nmutate()\r\nselect()\r\nfilter()\r\nsummarize() (or summarise())\r\narrange()\r\nHere’s the chapter from R for Data Science\r\nYou may also find this vignette helpful\r\nfactor\r\nA data type that is the preferred way to store categorical variables in R.\r\nUsing factor(), you can convert:\r\na character to a factor: This takes all unique values of a character variable and assigns them an underlying number, or “levels”.\r\nFor example:\r\n\r\n\r\ndumplings <- c(\"momos\", \"pop tarts\", \"raviolis\", \"momos\", \"empanadas\", \"pierogis\")\r\n\r\nfactor(dumplings)\r\n#> [1] momos     pop tarts raviolis  momos     empanadas pierogis \r\n#> Levels: empanadas momos pierogis pop tarts raviolis\r\n\r\n\r\n\r\nan integer to a factor: This takes integers and makes the lowest number the base level, and the next highest 1, the next highest 2, etc. You can assign a “label” to each respective level with “labels = c()”, with a vector, c(), with the same length as the number of levels.\r\n\r\n\r\nintegers <- c(1, 2, 3, 2, 3, 2, 1)\r\n\r\nfactor(integers,\r\n       labels = c(\"chicken\", \"egg\", \"rooster\"))\r\n#> [1] chicken egg     rooster egg     rooster egg     chicken\r\n#> Levels: chicken egg rooster\r\n\r\n\r\n\r\nUnderstanding factors mostly takes time. If you want to speed that up, here’s the R for Data Science chapter on factors\r\nIn it, they reference a few articles for further reading:\r\nWrangling categorical data in R\r\nstringsAsFActors: An unauthorized biography\r\nstringsAsFactors =  – this one is kinda funny\r\nfunction\r\nA “self-contained” piece of code that takes a predefined type of input data (arguments), operates on it, and returns an output or result.\r\nggplot2\r\nThe “gg” in ggplot2 stands for “grammar of graphics”.\r\nTo get a high-level overview of ggplot2 basics, I highly recommend this introduction to data visualization.\r\nIf you’re really trying to nerd out, you can access the free full text of ggplot2: Elegant Graphics for Data Analysis by Hadley Wickham by looking it up using the Duke Library search engine and logging in with your Duke credentials\r\ngit\r\nYou may have heard of GitHub, a popular website for version control, collaboration, and code sharing.\r\nGit is the underlying software that GitHub runs on. It’s free and open source. You won’t be expected to use a Git repository for your projects in this class, but it’s nice to know what’s out there.\r\nThis massive book is available for those who would like to learn more. I think the first and second sections, “Getting Started” and “Git Basics”, are good places to start.\r\nHTML\r\nStands for HyperText Markup Language. It’s the standard language used for documents that are meant to be displayed in a web browser, and is highly customizable. When R Markdown renders to HTML, it does almost all of the heavy lifting for you.\r\nindex\r\nNot particularly important for Fall semester, but is an important concept to understand when working with data.\r\nIn programming, an index is a numerical representation of an item’s position in a sequence.\r\nIn R, indexes start at number 1 (as opposed to other languages that start at 0).\r\nYou can refer to an item’s index with [ ].\r\nLETTERS gives us a character vector of every letter of the alphabet\r\n\r\n\r\nLETTERS\r\n#> [1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\" \"G\" \"H\" \"I\" \"J\" \"K\" \"L\" \"M\" \"N\" \"O\"\r\n#> [16] \"P\" \"Q\" \"R\" \"S\" \"T\" \"U\" \"V\" \"W\" \"X\" \"Y\" \"Z\"\r\n\r\n\r\n\r\nWe can refer to individual letters by calling their index:\r\n\r\n\r\nLETTERS[1]\r\n#> [1] \"A\"\r\n\r\nLETTERS[5]\r\n#> [1] \"E\"\r\n\r\nLETTERS[26]\r\n#> [1] \"Z\"\r\n\r\n\r\n\r\nData Frames and Tibbles can be indexed using syntax [row, column]. So data[1,1] would call the value in the first row of the first column, data[2,1] would call the value in the second row of the first column, and so on.\r\nknit\r\nThe button at the top of your R Markdown document that instructs the document to render as its designated output. The standard outputs for R Markdown are HTML, Word, and PDF. But there are an ever expanding set of R Markdown outputs available to R users. This entire website was created using a document output type called “Distill”\r\nlibrary/packages\r\nI’ll quote from an answer in StackOverflow for this one:\r\n“In R, a package is a collection of R functions, data and compiled code. The location where the packages are stored is called the library.”\r\nmagrittr/pipe/%>%\r\nChapter from R for Data Science gives a nice intro.\r\nFor you nerds, here’s a history of the pipe operator in R\r\nparameter\r\nUsed interchangeably with “argument”\r\nreproducible expressions (reprex)\r\nWhen you encounter a problem in your code that you just can’t figure out, it’s often best to create a reproducible expression. This allows whoever is helping you to recreate your problem in their own R console.\r\nCreating a “reprex” often entails trimming your code to the bare essentials and isolating whatever step in your code is causing it to hit an error.\r\nIt might also be the case that you need to create toy data. To do this, you can use the following tools:\r\ntibble() to build a dataset from vectors of equal length\r\nrunif() to create a vector of n length of random floating point values\r\nseq() to create a vector of n length in a defined sequence\r\nsample() to draw n random samples, with replace = TRUE/FALSE, from a pre-existing vector\r\nx:y – use a colon to generate a series of integers from x to y\r\nc() with comma separated values to designate a vector\r\nstring/character\r\nStrings are created with either single quotes or double quotes. It indicates that a value is meant to be read as-is, rather than transformed according to R’s computational rules for numbers and logical values.\r\nFor example, writing the logical value, TRUE as a string makes it unrecognizable to R as logical:\r\n\r\n\r\na <- TRUE\r\nisTRUE(a)\r\n#> [1] TRUE\r\n\r\nb <- \"TRUE\"\r\nisTRUE(b)\r\n#> [1] FALSE\r\n\r\n\r\n\r\nAs simple as it sounds, strings are a topic of mind-numbing complexity, as the underlying encoding of text strings governs the way that code is able to interact with data as well as other code.\r\nR for Data Science gives a nice introduction to the mechanics of strings here, but hints at their wider implications in its chapter on data importation.\r\nRegular expressions (not to be confused with reproducible expressions), or regex, are their own beast, and may help you understand how databases and search engines work\r\ntibble\r\nLike a data.frame object, but with enhanced “printing”.\r\nRead this section on tibbles from R for Data Science to learn more.\r\ntidy data\r\n“Tidy datasets are all alike, but every messy dataset is messy in its own way.” - Hadley Wickham\r\nThe first few sections in this chapter from R for Data Science gives a nice introduction.\r\nThe basic ideas behind tidy data are defined by these three rules:\r\nEach variable must have its own column\r\nEach observation must have its own row\r\nEach value must have its own cell\r\nYou can read more about the underlying theory in this article that was published in the Journal of Statistical Software.\r\ntidyverse\r\n“The tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures.”\r\nFamiliar packages include:\r\ndplyr\r\nggplot2\r\ntibble\r\ntraceback\r\nIf you hit an error, use traceback() to print a summary of how your program/code arrived at that error. In simple terms, it’s tracing your steps prior to your code hitting an error.\r\ntribble\r\nShort for “transposed tibble”, it’s a function that allows you to create a tibble by hand, with the syntax and subsequent output:\r\n\r\n\r\ntribble(\r\n  ~colA, ~colB, ~colC, ~colD,\r\n  \"a\",   1, \"Square\", \"orange\",\r\n  \"b\",   2, \"Circle\", \"maracuya\",\r\n  \"c\",   3, \"Rhombus\", \"cashew\"\r\n)\r\n\r\n\r\n\r\n\r\ncolA\r\ncolB\r\ncolC\r\ncolD\r\na\r\n1\r\nSquare\r\norange\r\nb\r\n2\r\nCircle\r\nmaracuya\r\nc\r\n3\r\nRhombus\r\ncashew\r\n\r\n?tribble in the R Console for more details\r\nvector\r\nA vector is a list of values, all of the same type.\r\nWe use c() to create a vector, separating items with commas when we specify them individually.\r\nThe following are all valid vectors:\r\n\r\n\r\n# a vector of numbers 1, 2, 3:\r\nc(1, 2, 3)\r\n#> [1] 1 2 3\r\n\r\n# a vector of numbers 1 through 12, and then 20:\r\nc(1:12, 20)\r\n#> [1]  1  2  3  4  5  6  7  8  9 10 11 12 20\r\n\r\n# a vector of logical values:\r\nc(TRUE, FALSE, NA, TRUE)\r\n#> [1] \"TRUE\"  \"FALSE\" \"TRUE\" \r\n\r\n# a vector of 5 values randomly drawn from a uniform distribution with min 0 and max 1:\r\nrunif(5)\r\n#> [1] 0.97420369 0.04387835 0.68818071 0.13420150 0.30322080\r\n\r\n\r\n\r\nMost operations on vectors apply to each value individually:\r\n\r\n\r\nx <- c(1:5)\r\n\r\nx + 2\r\n#> [1] 3 4 5 6 7\r\n\r\nx * 2\r\n#> [1]  2  4  6  8  10\r\n\r\n\r\n\r\nAs such, a data frame is just a list of vectors of all the same length. That list is what’s known formally as a “recursive vector”. Lists can contain other lists.\r\nThis is a somewhat complex topic. If you’re really hungry for more info, this chapter in R for Data Science is highly informative, but may be confusing at first for those without any programming background.\r\nvignette\r\nA vignette is a long-form guide to a package. It highlights a package’s main functions and their usage. Learning from vignettes is one of the best ways to self-teach yourself a skill in R.\r\nWickham, Hadley\r\nA kiwi and a statistician who probably authored 80% of the links on this page. He is known for the tidyverse, the book R for Data Science, and his twitter.\r\nYAML header\r\nA short blob of text at the top of your R Markdown document specifying things like the document’s title, time and date stamp, and the document’s output type.\r\nThe YAML header is part of what makes R Markdown such a flexible document. There are many ways to customize your R Markdown output. We won’t get into those.\r\nJust know that at the top of your R Markdown document that says output: html_document is what instructs your it to automatically knit as an HTML file.\r\n\r\n\r\n\r\n",
      "last_modified": "2021-08-31T12:49:36-04:00"
    },
    {
      "path": "help_packages.html",
      "title": "Packages",
      "description": "Help with installing and understanding packages, at home and in the wild.\n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\nTerminology\r\nInstalling packages\r\nLoading libraries\r\n\r\nTerminology\r\nPackages and libraries\r\nPackages are a collection of R functions, data, and compiled code.\r\nA library is the location where the packages are stored on your computer.\r\nUsage: “Install the {tidyverse} package from CRAN. It will automatically save to your R library.”\r\nCRAN\r\nCRAN stands for the Comprehensive R Archive Network. Along with providing downloads and updates to the base R computing software, CRAN is the main source for stable, published versions of packages. The function install.packages() automatically looks in CRAN for whatever package you name inside the function.\r\n\r\nUsing other installation functions, it’s also possible to download packages from various other sources, like GitHub. But most of the time, those packages are still in development, which means various features could become inoperable over time (as opposed to making defunct code backwards-compatible). Getting packages from CRAN is the best way to ensure that your code will still run months and sometimes years into the future\r\nObjects, functions, and arguments\r\nObject refers to any data structure in R, with special attributes and methods according to the object type. Most objects are open to exploration and transformation.\r\nExamples of objects: Data frames, linear models, vectors, tables, variables, ggplot objects, matrices, etc.\r\nFunctions are the verbs of the R language. They operate on objects in your R environment, and behave according to their arguments. On this website, functions are written in monospace font and followed by a pair of parentheses, ()\r\nExamples of functions: mean(), summary(), factor(), exp()\r\nArguments are the settings and information supplied to a given function. Each new argument should be separated from the rest with a comma.\r\nSome functions don’t require any arguments, like R.Version() and sessionInfo(). These functions, when run, supply information about the R environment, but don’t operate on an object.\r\nOther functions only require a single argument: an object. For example, the function head() takes many different types of objects, and supplies the first 6 observations within a given object.\r\nBut say you want a different number of observations. head() allows you to manually set those observations with n =.\r\nTyping head(object, n = 10) will return the first 10 observations in a given object.\r\nMost functions come with default arguments that are submitted to the function implicitly. In the case of head(), the default method for n = is 6.\r\nALL arguments can be specified with the syntax argumentName = value. But most functions are written to recognize certain types of values as belonging to a specific argument, and operate on them accordingly.\r\nSo head(object, 10) will also give the first 10 observations of object, since head() knows to recognize an object of type integer as belonging to n =.\r\nYou can view any function’s arguments by going to its documentation. In the R console, run ?functionName, and it will appear in the help window. Under the “Usage” section, you will find information on the function’s syntax and arguments. If an argument is alone, without an equals sign, =, that argument doesn’t have default and must be submitted by the user to run. However, if a given argument contains an equals sign and a value to the right of the equals sign, that value is the argument’s default.\r\nDocumentation\r\nAny function or package’s documentation is the primary authority on the appropriate usage of that function. Typically, a function’s documentation will contain:\r\nA brief description of the function\r\nAn overview of the functin’s usage\r\nA detailed description of each argument needed for the function to run\r\nFurther function details\r\nValue, which indicates what gets returned when the function is run\r\nExamples of the function in use\r\nThe documentation for any function is available via the R console by running ?functionName.\r\nSometimes, you can find documentation that goes into greater detail by searching the function’s documentation online.\r\nWhen you are having problems with your code, it’s smart to check the function’s documentation before looking elsewhere. With practice and a little persistence, you will become more comfortable with reading and interpreting help documentation.\r\nInstalling packages\r\nPackages are collections of functions. As we’ll see shortly, we use functions as code to view, manipulate, and analyze our data.\r\nThere are packages that come built-in with R. These have names like {base}, {utils}, and {stats}.\r\nSince R is open-source, users are able to create their own packages so that other R users can use them. These packages are available in places like the Comprehensive R Archive Network (CRAN for short) and GitHub.\r\nLucky for us, packages are easily retrieved from the R Console. If you haven’t already, run the following code from your console to download the packages that we’ll be needing for Fall Semester. You only need to do this once:\r\n\r\n\r\ninstall.packages(\"tidyverse\")\r\ninstall.packages(\"skimr\")\r\ninstall.packages(\"epiR\")\r\ninstall.packages(\"devtools\")\r\ndevtools::install_github(\"potato-nathan/epiAssist\")\r\n\r\n\r\n\r\nLoading libraries\r\nTo enable R to use a package’s functions in our current project environment, we need to load the packages using the library() function.\r\nIn a fresh code chunk, call in the {tidyverse} and {skimr} packages using the following code:\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(skimr)\r\n\r\n\r\n\r\nNotice that when we install packages, we need to specify their names using quotes, but when we load them into R using library(), R recognizes them as package objects automatically, and so don’t need quotes. You can surround them with quotes and it will load all the same.\r\n\r\n\r\n\r\n",
      "last_modified": "2021-08-31T12:49:37-04:00"
    },
    {
      "path": "help_projects.html",
      "title": "Creating a New Project",
      "description": "This page provides advice on establishing a workflow and creating new projects with RStudio\n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\nCreate folders to store your work\r\nCreate a New Project in RStudio\r\n\r\nIn R, we organize our work by projects. It is best practice to keep no more than one project in any single folder on your computer. First, we should establish the file folders from which we’ll be working.\r\nCreate folders to store your work\r\nIf you haven’t already, now would be a good time to:\r\nCreate a new folder on your computer called “705 Lab”.\r\nWithin that folder, create a folder for whatever lab you’re working on, “Lab X”.\r\nFinally, within that folder, create a folder for your data, called “data”\r\nSave your dataset into the folder called “data”. This helps keep things organized as the project grows.\r\nPlease make this a habit. We will expect you to do this for every lab, as it will keep your work organized and will keep you happy.\r\nCreate a New Project in RStudio\r\nNow open RStudio and take a deep breath. Don’t panic. This will all be very familiar in a few short months.\r\nInitiate a new project by going to File and clicking New Project.\r\nSelect Existing Directory:\r\nInitiate a new project in an existing directoryIn the following window, hit Browse. Navigate to the folder where you’d like to house your new project (or create a new one if you haven’t already. Open it, then hit Open.\r\n\r\n\r\n\r\n",
      "last_modified": "2021-08-31T12:49:38-04:00"
    },
    {
      "path": "help_rstudio.html",
      "title": "Help with RStudio",
      "description": "This page provides links to introduce you to the RStudio environment.\n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\nRStudio Basics\r\nThe R Environment\r\nManaging window panes\r\nChange RStudio color theme\r\nRMarkdown keyboard shortcuts\r\nSession information and help documentation\r\n\r\nExtended resource guide to RStudio\r\n\r\nRStudio Basics\r\nThe R Environment\r\n\r\n\r\n\r\nA: File (New project, New file > R Markdown)B: Pop-out window (also works in R Markdown)C: Manage window panesD: New fileE: Find packages, check for updates, manage environment optionsF: Find cheatsheets (dplyr, ggplot2, R Markdown)G: Current R ProjectH: Console historyI: Plot viewerJ: File pathK: Browse help documentationL: Click to return to working directory homeM: Click to expand or shrink window pane or…N: Drag to manage window size\r\nManaging window panes\r\nDid you lose track of one of your window panes? Use the View tab in the top bar to return to a view with all panes\r\n\r\nChange RStudio color theme\r\nIs the white RStudio background burning your retnas? Need a change of scenery? No need to go outside!\r\nIn the top bar, go to Tools > Global Options, and then select “Appearance”:\r\nChange appearanceRMarkdown keyboard shortcuts\r\nYou can find me mashing the following keys in Windows:\r\nShortcut\r\nKeys (Windows)\r\nKeys (MacOS)\r\nRun a line of code\r\n(With typing cursor on line) ctrl + enter\r\n(With typing cursor on line) cmd + enter\r\nRun a chunk of code\r\n(With typing cursor in chunk) ctrl + shift + enter\r\n(With typing cursor in chunk) cmd + shift + enter\r\nCreate a new chunk\r\nctrl + alt + i\r\ncmd + option + i\r\nType a pipe\r\nctrl + shift + m\r\ncmd + shift + m\r\nType an assignment operator\r\nalt + -\r\noption + -\r\nStop running\r\n(In Console) esc\r\n(In Console) esc\r\nSession information and help documentation\r\nIf for some reason your code is hitting an error, here are some useful commands you might run in your RStudio Console to help troubleshoot:\r\nFunction Documentation\r\nCommand: ?functionName\r\nWhat it does: Pulls up the documentation for whatever you type in place of functionName, and displays it in the Help window. A function’s documentation is often (but not always) the final word on how a function is meant to behave in R. It will provide information on the function’s arguments (and their default values), the resulting output (“Values”), and will give you examples of how to implement that function.\r\nSession information\r\nCommand: sessionInfo()\r\nWhat it does: If you’re experiencing issues with a certain package, maybe it’s time for an update. You can check your current version of R using this command. Find information on specific packages by using the argument ‘package = packageName’.\r\nYou can also find which version of RStudio you have by running RStudio.Version()\r\nView Traceback\r\nCommand: traceback()\r\nWhat it does: If your code keeps hanging up on an error, you can often find clues as to why it’s happening by viewing the underlying code that’s causing your code to halt. Traceback will print the list of functions that were called before the error occurred. It’s literally “tracing your steps” right before the error happened. The language itself in the traceback is usually pretty cryptic and hard to read at first. Don’t let that stop you from looking at it when you encounter problems.\r\nMemory storage information\r\nCommand: object_size()\r\nWhat it does: For the purposes of this lab, you most likely won’t need to worry about your computer’s RAM. Nevertheless, the nerds among us might find it interesting to inspect the size of an object that we’ve saved to our R Environment. Use object_size(objectName) to inspect a given object. Alternatively, inspect overall RAM usage and limitations with memory.size() and memory.limit().\r\nExtended resource guide to RStudio\r\nR Studio will be the launching pad for all of our lab assignments in this course.\r\nThe RStudio environment itself can feel overwhelming at first. Luckily, there are excellent resources already available to help get you acquainted:\r\nFor those of you who prefer to learn by reading, this website’s first tutorial, “Getting started with R/RStudio”, is a good place to start.\r\nDr. Eric Green used to run a summer workshop called “I Eat Data Science for Breakfast”. He gives a stellar tutorial to the RStudio environment starting at 10:49 in his Week 1 video.\r\n\r\n\r\n\r\nFigure 1: I Eat Data Science for Breakfast\r\n\r\n\r\n\r\nIf you were checking your email at all over the summer, you may also already be familiar with Duke Librarian, John Little, and his series called RFun. Feel free to review his video introducing the RStudio environment\r\n\r\n\r\n\r\nFigure 2: RFun with John Little\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2021-08-31T12:49:39-04:00"
    },
    {
      "path": "index.html",
      "title": "Biostat Lab",
      "description": "The hub for DGHI Biostat & Epi lab assignments, help pages, and whirly gigs\n",
      "author": [],
      "contents": "\r\n\r\n\r\n\r\nFigure 1: xkcd.com - ‘Base Rate’\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2021-08-31T12:49:40-04:00"
    },
    {
      "path": "lab_0.html",
      "title": "GLHLTH 705 Lab 0",
      "description": "__Due:__ September 17, 2021 by 14:00 ET\n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\nLAB MATERIALS:\r\nLab 0 Goals:\r\nTask 1: Establish a workflow\r\nCreate folders (directories!) and projects to store your work\r\nCreate a New Project in RStudio\r\n\r\nTask 2: Familiarize yourself with R Markdown\r\nTask 3: Install packages and load libraries\r\nInstalling packages\r\nLoad libraries\r\n\r\nTask 4: Load data\r\nObjects\r\nThe assignment operator (<-)\r\n\r\nTask 5: Explore the data\r\nTask 6: Create variable mage\r\nPipes (%>%)\r\n\r\nTask 7: Frequency distributions of mage\r\nTask 8: Create variable magec\r\ncase_when()\r\nConvert a character variable to a factor\r\n\r\nTask 9: Cross-tab of mage and magec\r\nMethod 1\r\nMethod 2\r\n\r\nTask 10: Save new dataset\r\n\r\n\r\n\r\n\r\nNote: This lab is not graded (i.e. does not contribute to your course evaluation, so don’t fret)\r\nLAB MATERIALS:\r\nR Markdown file for Lab 0. Click link to download. Fill it in with your answers to the following lab tasks. Once you’re finished, rename it as firsInitial_YourLastName_Lab0.Rmd, and submit it using the Sakai dropbox.\r\n“Lab_0_kenya.rds” - data file available in the Resources folder of the Sakai course webpage\r\nLab 0 Goals:\r\nBy the end of this lab, you will have demonstrated a basic familiarity with the R coding environment and R Markdown files. You will be able to:\r\nStart a new project and import data\r\nUnderstand useful R terminology\r\nUse summary statistics to describe the Kenya dataset\r\nDerive an ordinal categorical variable from a continuous one\r\nGenerate histograms and boxplots using ggplot2\r\nCreate a simple cross-tabulation of two variables\r\nTask 1: Establish a workflow\r\nIn R, we organize our work by projects. It is best practice to keep no more than one project in any single folder on your computer. First, we should establish the file folders from which we’ll be working. This should always be your first step when beginning an analysis.\r\nCreate folders (directories!) and projects to store your work\r\nIf you haven’t already, now would be a good time to:\r\nCreate a new folder on your computer, calling it “705 Lab”.\r\nWithin that folder, create a folder for this lab, called “Lab 0”.\r\nFinally, within that folder, create a folder for your data, called “data”\r\nSave the dataset titled Lab_0_kenya.rds (downloaded from Sakai) into the folder called “data”\r\nThe resulting file path for your data file should look like this:\r\n“/705 Lab/Lab 0/data/Lab_0_kenya.rds”\r\nPlease make this a habit. We will expect you to do this for every lab, as it will keep your work organized and will keep you happy.\r\nCreate a New Project in RStudio\r\nNow open RStudio and take a deep breath. Don’t panic. This will all be very familiar in a few short months.\r\nInitiate a new project by going to File and clicking New Project.\r\n Then select Existing Directory and hit Browse. Navigate to the folder titled Lab 0. Open it, then hit Open.\r\nIf you’ve done this correctly, your new folder, “Lab 0” should appear in the bottom-right R Studio window pane, under the tab “Files” Likewise, the folder name, “Lab 0” should appear in the upper-right hand corner, indicating the current project’s name.\r\n\r\nCurrent project indicator is marked as “G” in this helpful screenshot of the R Studio environment\r\nTask 2: Familiarize yourself with R Markdown\r\nNow, you will open a special kind of document known as “R Markdown”. This is a text editor (like Word or Google Docs), but with a twist. You can implement the instructions you have written in the R programming language directly within the document. That is, you can “run code” directly within the document. This makes data analysis an interactive, iterative (and therefore fun?) process that usually proceeds as follows:\r\nThe word “run” can have so many meanings. So exactly what do we mean when we say “run code”?\r\nIt’s a term meant to describe the process of allowing a computing program or software to operate upon something you’ve created in your computing environment. In R, you can \"run code directly in the R console by typing your code and hitting enter. R Markdown can also run chunks of code, which we’ll see in a moment.\r\nWrite some code to generate new variables and perform statistical analysis\r\nRun the code\r\nObserve how your dataset behaved\r\nTake a few notes (for Science)\r\nTweak code\r\nRepeat\r\nFor each lab, we will provide you with a skeleton Markdown file. If you haven’t already, download that file (“lab0_705_fall2021.Rmd”, available at the top of this page) and save it to your folder named “Lab 0”.\r\nNow open the file in RStudio. You can just double-click on it from the file folder. It will appear in the RStudio window pane, but might feel a little pinched. Luckily, RStudio allows Markdowns to pop-out.\r\nClick the white square at the top of the Lab 0 document (circled in red in the photo below) to do just that:\r\nPop-outTask 3: Install packages and load libraries\r\nInstalling packages\r\nPackages are collections of functions. As we’ll see shortly, we use functions as code to inspect, manipulate, and analyze our data. They are the verbs of the R language.\r\nThere are packages that come built-in with R. These have names like {base}, {utils}, and {stats}.\r\nSince R is open-source, users are able to create their own packages so that other R users can use them. These packages are available in places like the Comprehensive R Archive Network (CRAN for short) and GitHub.\r\nLucky for us, packages are easily retrieved from the R Console. If you haven’t already, run the following code from the R Console to download the packages that we’ll be needing for this semester. Unless you uninstall R, __*you should only ever have to do this once__*:\r\n\r\n\r\ninstall.packages(\"tidyverse\")\r\ninstall.packages(\"skimr\")\r\ninstall.packages(\"epiR\")\r\ninstall.packages(\"devtools\")\r\ndevtools::install_github(\"potato-nathan/epiAssist\")\r\n\r\n\r\n\r\nLoad libraries\r\nUnlike installing packages, every time we open a new R session, we need to enable a package’s use in the R environment. To enable R to use a specific package and its functions, we can load them using the library() function.\r\nIn a fresh code chunk, call in the {tidyverse} and {skimr} packages using the following code:\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(skimr)\r\n\r\n\r\n\r\nNotice that when we install packages, we need to specify their names using quotes because the package name is not yet known to your own copy of RStudio. On the other hand, when we load them into the R environment using library(), R automatically recognizes them as the names of packages, so they don’t require quotations.\r\nTask 4: Load data\r\nYou will use the dataset Lab_0_kenya.rds for this lab. You’ve hopefully already saved it to the folder Lab 0 > data. You will use the function readRDS() to import the data file from your computer’s folder.\r\nSince our project has been created within the “Lab 0” folder, it is thus our “Working Directory”, and RStudio will automatically start from that folder when we give it a function that asks it to look in our file directory.\r\nWithin our function, all we need to do is specify the file name, and that it’s in the folder called “data”.\r\nUse the following code to load your data into R and give it the name kenya.\r\n\r\n\r\nkenya <- readRDS('data/Lab_0_kenya.rds')\r\n\r\n\r\n\r\nWhen we load our data into R, it becomes what, in R, is called a data frame, which is the R term used for a dataset object. Without going into too much detail, it’s like having a spreadsheet of data with rows (i.e. different individual records) and columns (i.e. variables). For those of you familiar with mathematical terminology, it’s like a matrix.\"\r\nObjects\r\nIf functions are the verbs of the R language, objects are the nouns. Just like nouns, there are many different types of objects, which we will learn about throughout the semester. For now, you just need to understand that an object is anything in your R environment that is able to be explored, transformed, or analyzed by R functions. Objects also have unique names. In the code above, kenya is a data frame object.\r\nTo further illustrate, in the chunk of code below, fruit becomes a character vector object of length 3, number becomes a numeric vector object of length 1, and logical becomes a logical vector object of length 1.\r\nYou can use class() to inspect an object’s type, and object.size() to inspect an object’s size. We will learn more about vectors in a later assignment.\r\nMost functions require specific types of objects.\r\nThe assignment operator (<-)\r\nThe <- is called the Assignment Operator. We use it to assign names to objects in our coding environment:\r\nWe can use our assignment operator for characters, numbers, logical operators, etc.:\r\n\r\n\r\nfruit <- c(\"oranges\", \"papayas\", \"apricots\")\r\n\r\nnumber <- 99\r\n\r\nlogical <- FALSE\r\n\r\n\r\n\r\nNow that the above values are stored in our environment, we can use them in other functions or operations as predefined variables:\r\n\r\n\r\npaste(fruit, \"are orange\", sep = \" \")\r\n#> [1] \"oranges are orange\"  \"papayas are orange\"  \"apricots are orange\"\r\n\r\nnumber + 1\r\n#> [1] 100\r\n\r\nisTRUE(logical)\r\n#> [1] FALSE\r\n\r\n\r\n\r\nWe’ve done the same thing with our dataset, giving it the name kenya. We might use the function head() to view the first six rows in the dataset. This is a quick and easy way to glance at our dataset and its accompanying variables:\r\n\r\n\r\nhead(kenya)\r\n\r\n\r\n\r\nTask 5: Explore the data\r\nFamiliarize yourself with the data by using the commands ncol(), nrow(), class(), names() and skim().\r\nAre there any string/character variables?\r\nAre there any variable or value labels?\r\nDo any variables have notes?\r\nSimilar to head(), we can feed our kenya data frame to various functions that tell us other useful information about it. As a tip, you can use $ in the format [datasetName]$[variableName] to refer to a specific variable/column within a dataset.\r\nUse ncol() to print the number of columns in our data frame\r\nUse nrow() to print the number of rows\r\nUse names() to view each variable’s name.\r\nUse class() to view each variable’s “type”\r\nUse skim() to print summary statistics for each variable in the data frame\r\nTask 6: Create variable mage\r\nUsing a pipe (%>%) and the mutate() function, create a new variable, mage for mother’s age (as an integer) at the time of each child’s birth (note – some of these mothers have had multiple children).\r\nThis is calculated from variables b3 (month code of child’s birth) and v011 (month code of mother’s birth). The difference between the values of these variables is in months, so divide by 12 to get years. See the data dictionary for a more detailed description of month codes and how to use them. Use as.integer() around your calculation to truncate the calculated values for mage to integers.\r\n\r\nMost datasets are accompanied by documentation or data dictionaries, which are a description of the variables in the data set and other relevant pieces of information. Be sure to read through these guidelines if you haven’t already.\r\nFor this task, the mutate() function will work with the following syntax.\r\n\r\n\r\n# don't forget to write over your old dataframe using `kenya <-`\r\n\r\ndata <- data %>%\r\n  mutate(newVariableName = (oldVariable1 - oldVariable2))\r\n\r\n\r\n\r\nPipes (%>%)\r\nOne of the most useful tools in the tidyverse package is a little thing called a pipe. It’s represented with the symbol %>%, and allows us to express a series of operations in a continuous string of code, rather than using the assignment operator over and over.\r\nAs an example, let’s pretend we have a dataset that is a record of birds struck by aircraft in the United Sates over the past few decades. We want to know the frequency distribution of sky conditions for birds struck over 5,000 feet in the air in planes flying over North Carolina.\r\nIn base R, we might code it like this:\r\n\r\n\r\n\r\nWith a pipe, %>%, both our code and output becomes more tidy and readable:\r\n\r\n\r\n\r\nThey pipe can be interpeted as signifying “and then”. In the above example, the pipe tells the R console to take the dataset birds, and then filter by state and height, and then group by sky, and then count the observations in each group.\r\nTask 7: Frequency distributions of mage\r\nSuppose you want to break down mage into an ordinal categorical variable with three categories. First, we might inspect the frequency distribution (in one-way frequency table) for mage. Do this using table().\r\ntable() works by identifying unique values within a variable, and then counts their occurence.\r\nIt works on character variables, categorical (factor) variables, and even numbers.\r\nAs was mentioned earlier, we can tell R to look at specific variables inside our dataframe with the $ sign. The syntax looks like this:\r\n\r\n\r\ndataframeName$variableName\r\n\r\n\r\n\r\nIf we want to include a count of NA values in our table, we can also use the argument useNA = 'always' within table().\r\nUse table() to look at kenya$mage, then consider the following questions:\r\nAre there any missing values for mage? If so, how many?\r\nWhich range of ages appear the most frequently in mage?\r\nTask 8: Create variable magec\r\nUsing mage, generate a new variable with three categories: “<18”, “18-39”, and “≥ 40”, naming the new variable magec\r\nmagec stands for: {m}other’s\r\n{age}\r\n{c}ategorical\r\nSet the values for magec to be 0,1,2, where 0 corresponds to the youngest age group (<18).\r\nWe recommend you do this in the following steps:\r\nUse a pipe (%>%) and then mutate() to create a new variable, magec,\r\nWithin your mutate() command, use case_when() to create a series of conditional statements that assign numbers 0, 1, and 2 to each category\r\nOn a new line of code, use factor() to assign labels to each level of your new variable\r\nFinally, once you get it to work, don’t forget to use the assignment operator to save your changes to the kenya data frame.\r\n\r\nValues:\r\n0: <18\r\n1: 18-39\r\n2: ≥ 40\r\ncase_when()\r\nThis function is used to create conditional rules when creating new variables with mutate(). It allows you to create a series of if-then (conditional) statements based on variables within your data. At first, the syntax for case_when() might strike you as a little overly complicated, especially for coding binary variables. But as your variables become more complex, case_when() really shines as a highly efficient way to create new variables on a series of complex conditions.\r\nAn example of the syntax is as follows:\r\n\r\n\r\ncase_when(size == 'small' ~ 0, \r\n          size == 'medium' ~ 1,\r\n          size == 'large' ~ 2,\r\n          TRUE ~ NA)\r\n\r\n\r\n\r\nHere, the tildes represents a formula. To the left of the formula is a logical operation that can evaluate to either TRUE or FALSE\r\n\r\nClick here for a list of logical operators available in the R language\r\nTo the right of the tilde, we put the value that we want to return if the logical operation evaluates to TRUE. If it is FALSE or NULL, case_when() behaves by moving on and testing the next conditional statement.\r\nIf it were run on a dataset containing a variable called size, the literal translation of the above code would go something like this:\r\n\r\nFor each row in the dataset,  \r\n    * if `size` equals 'small', then return 0\r\n    * if `size` equals 'medium', then return 1\r\n    * if `size` equals 'large', then return 2\r\n    * if `size` is any other real value, then return `NA`\r\n\r\n\r\nThis sort of literal translation of a programming language into readable English is what’s known as “pseudo-code”\r\nWhen we do this within a mutate() function, the returned values get assigned to the new variable for each row of the dataset as they’re evaluated.\r\nThe right side of the formula needs to always produce a value of the same variable type, but otherwise you have a high degree of freedom in what can be returned when the conditional statement is TRUE, including mathematical operations on other variables.\r\nSee the documentation for case_when() for more examples of this function’s capabilities\r\nConvert a character variable to a factor\r\nConvert a variable to a factor with the following syntax:\r\n\r\n\r\ndata$variableYouWantToFactor <- factor(data$variableYouWantToFactor,\r\n                                       labels = c(\"Label for 0\", \"Label for 1\", \"Label for 2\"))\r\n\r\n\r\n\r\nTask 9: Cross-tab of mage and magec\r\nLook at a cross-tabulation (two-way table) of mage and magec to ensure that magec was created correctly. Be sure missing values were handled properly (all observations that have a missing value for mage should be assigned the R missing value “NA” for magec). Try the two separate methods for cross-tabulation, as we will be using both for separate purposes later in the semester:\r\nMethod 1\r\nType “?table” in the console for help with how to create a 2x2 table. Note: the order of the variables in the command controls which one is in the rows and which is in the columns. Experiment to make your table readable.\r\n\r\n\r\n# example code:\r\n\r\ntable(data$x, data$y, useNA = 'always')\r\n\r\n\r\n\r\nMethod 2\r\nWe can also use tidyverse functions to accomplish a two-way tabulation of our variables of interest. These functions will become increasingly relevant and useful, and are a big reason why R is such a popular platform for data science. We will use a pipe (%>%), group_by(), another pipe, and count() to get the same output given by table().\r\n\r\n\r\n# example code:\r\n\r\n# notice that we don't want to assign this operation to a name\r\n# we just want to view the output, hence the lack of \"data <- \"\r\ndata %>%\r\n  group_by(x, y) %>%\r\n  count()\r\n\r\n\r\n\r\nA translation of the above code to written instructions would go as follows, where and then represents the grammatical equivalent of our pipe, %>%:\r\n“Take dataset, data, and then group_by variable x, and within those groups, group_by variable y, and then count the values in each of our groups.”\r\nHere’s a link if you’re interested in learning more about pipes. Or just take a look at this tweet:\r\n\r\n\r\n\r\nTask 10: Save new dataset\r\nUsing function saveRDS(), save the new dataset in the same directory as our original data, using the following format: “firstInitial_YourLastName_lab0.rds”\r\nsaveRDS() takes two primary arguments:\r\nThe dataframe object you want to save\r\nThe location in which you’d like it saved as a .rds file\r\nDon’t forget the following:\r\nThe file locations should be in quotes, so that R knows to read it as a character string\r\nYour file should be saved in your local Lab 0 folder, data/\r\n\r\n\r\n\r\n",
      "last_modified": "2021-08-31T12:50:31-04:00"
    },
    {
      "path": "lab_1.html",
      "title": "GLHLTH 705 Lab 1",
      "description": "__Due:__ October 1, 2021 by 14:00 ET\n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\nLAB MATERIALS:\r\nLab 1 Goals:\r\nTask 1: Load libraries & packages\r\nTask 2: Recode variables\r\nTask2a: bord5\r\nTask 2b: male\r\nTask 2c: mweight\r\nTask 2d: mheight\r\nTask 2e: mbmi\r\n\r\nTask 3: Frequency histograms\r\nTask 4: Boxplots\r\nTask 5: Frequency table (Table 1)\r\nTask 6: Data dictionary (Table 2)\r\nTask 7: Short answer\r\n\r\nLAB MATERIALS:\r\nR Markdown file for Lab 1.__ Click link to download. Fill it in with your answers to the following lab tasks. When you’re ready to submit, name it as Lab1_FirstinitialYourlastname.Rmd, and submit it using the Sakai dropbox.\r\nExcel document for Tables 1 & 2 TODO: Set up answer entry on Sakai?\r\nLab_1_kenya.rds - data file available on Sakai\r\nLab 1 Goals:\r\nGenerate derived variables\r\nIdentify and recode special values\r\nRun descriptive statistics for continuous and categorical variables\r\nGenerate graphics for continuous variables\r\nGenerate a complete data dictionary\r\nTask 1: Load libraries & packages\r\nUse the dataset Lab_1_kenya.rds for this assignment.\r\nYou will also need packages {skimr} and {tidyverse}\r\n** Reminder: To run a line of code, use the shortcut ctrl + enter (Windows)/cmd + enter (Mac)\r\nTask 2: Recode variables\r\nConstruct the following derived variables using a pipe and mutate(). For each, examine the component variables for coded special values (i.e. “NA”) and be sure to set derived variable values appropriately. Label all variables and the coded values for the categorical ones using the factor() command.\r\nHelp with pipes\r\nHelp with factors\r\nTask2a: bord5\r\nbord5: Dichotomous variable indicating birth order of the child. [0=current child was 1st through 4th in the birth order, 1=current child was later than 4th in the birth order]. Based on variable bord (birth order).\r\nTask 2b: male\r\nmale: Dichotomous categorical variable indicating that the child is male [0=female, 1=male]. Based on variable b4.\r\nTask 2c: mweight\r\nmweight: Continuous variable for maternal weight at time of interview (in kilograms). Based on variable v437. Note that v437 contains 1 implied decimal place. Divide by 10 to get kilograms.\r\nTask 2d: mheight\r\nmheight: Continuous maternal height at time of interview (in meters). Based on variable v438. Note that this variable is in centimeters and also contains 1 implied decimal place.\r\nTask 2e: mbmi\r\nmbmi: maternal body mass index (BMI). Weight (kilograms) / height^2 (meters).\r\nTask 3: Frequency histograms\r\nGenerate frequency histograms of mweight, mheight and mbmi. Put meaningful axis labels and a title on each figure.\r\nHelp with histograms in ggplot2\r\nTask 4: Boxplots\r\nGenerate boxplots of mweight, mheight, and mbmi for the levels of magec. Put meaningful axis labels and a title on each figure.\r\nHelp with boxplots in ggplot2\r\nTask 5: Frequency table (Table 1)\r\nFill in Table 1 with the frequency counts and percentages for the levels of the 3 categorical variables you have generated. Calculate percentages only for the non-missing values. Round percentages to 1 decimal place.\r\nWe would like you to use a pipe (%>%) with group_by() and summarize() to generate summary statistics.\r\ngroup_by() works by grouping rows by categorical variables. It then allows a function like summarize() to calculate summary statistics on those sub-groups, including percentages and frequency counts.\r\nAs an example, we can use the mtcars dataset available within RStudio:\r\n\r\n\r\nhead(mtcars)\r\n\r\n\r\n\r\n\r\n\r\nmpg\r\ncyl\r\ndisp\r\nhp\r\ndrat\r\nwt\r\nqsec\r\nvs\r\nam\r\ngear\r\ncarb\r\nMazda RX4\r\n21.0\r\n6\r\n160\r\n110\r\n3.90\r\n2.620\r\n16.46\r\n0\r\n1\r\n4\r\n4\r\nMazda RX4 Wag\r\n21.0\r\n6\r\n160\r\n110\r\n3.90\r\n2.875\r\n17.02\r\n0\r\n1\r\n4\r\n4\r\nDatsun 710\r\n22.8\r\n4\r\n108\r\n93\r\n3.85\r\n2.320\r\n18.61\r\n1\r\n1\r\n4\r\n1\r\nHornet 4 Drive\r\n21.4\r\n6\r\n258\r\n110\r\n3.08\r\n3.215\r\n19.44\r\n1\r\n0\r\n3\r\n1\r\nHornet Sportabout\r\n18.7\r\n8\r\n360\r\n175\r\n3.15\r\n3.440\r\n17.02\r\n0\r\n0\r\n3\r\n2\r\nValiant\r\n18.1\r\n6\r\n225\r\n105\r\n2.76\r\n3.460\r\n20.22\r\n1\r\n0\r\n3\r\n1\r\n\r\nSay we want to know the frequency counts of cars with 4, 6, and 8 cylinders (variable: cyl) based on whether or not a car has an automatic or manual transmission (variable am, 0 = automatic, 1 = manual).\r\nWe can use group_by() to create subgroups according to am and cyl. Using group_by() by itself doesn’t alter the appearance of our data frame, but our observations are now implicitly grouped according to transmission type and number of cylinders.\r\n\r\n\r\nmtcars %>%\r\n  group_by(am, cyl)\r\n\r\n\r\n\r\n\r\nmpg\r\ncyl\r\ndisp\r\nhp\r\ndrat\r\nwt\r\nqsec\r\nvs\r\nam\r\ngear\r\ncarb\r\n21.0\r\n6\r\n160\r\n110\r\n3.90\r\n2.620\r\n16.46\r\n0\r\n1\r\n4\r\n4\r\n21.0\r\n6\r\n160\r\n110\r\n3.90\r\n2.875\r\n17.02\r\n0\r\n1\r\n4\r\n4\r\n22.8\r\n4\r\n108\r\n93\r\n3.85\r\n2.320\r\n18.61\r\n1\r\n1\r\n4\r\n1\r\n21.4\r\n6\r\n258\r\n110\r\n3.08\r\n3.215\r\n19.44\r\n1\r\n0\r\n3\r\n1\r\n18.7\r\n8\r\n360\r\n175\r\n3.15\r\n3.440\r\n17.02\r\n0\r\n0\r\n3\r\n2\r\n18.1\r\n6\r\n225\r\n105\r\n2.76\r\n3.460\r\n20.22\r\n1\r\n0\r\n3\r\n1\r\n\r\nWe might use summarize() to operate on these sub-groups. summarize() works with the following syntax:\r\n\r\n\r\nmtcars %>%\r\n  group_by(am, cyl) %>%\r\n  summarize(columnName = Function(variable), \r\n            anotherColumnName = anotherFunction(variable),\r\n            etc., etc....)\r\n\r\n\r\n\r\nDepending on whether our variable is categorical or continuous, we can use summary statistic functions like n() (for raw counts), mean(), sd(), median(), IQR(), min(), and max() within our summarize() function. Except for n(), which doesn’t take any arguments, the rest of these functions take a variable name.\r\nWe can start by using n() for sub-group frequency counts.\r\n\r\n\r\nmtcars %>%\r\n  group_by(am, cyl) %>%\r\n  summarize(Counts = n())\r\n\r\n\r\n\r\n\r\nam\r\ncyl\r\nCounts\r\n0\r\n4\r\n3\r\n0\r\n6\r\n4\r\n0\r\n8\r\n12\r\n1\r\n4\r\n8\r\n1\r\n6\r\n3\r\n1\r\n8\r\n2\r\n\r\nThen we can use mutate() to operate on our summary table as if it were its own data frame.\r\nWithin mutate(), we create a new variable with NewVariable =, and equate it to the following operation: Counts / sum(Counts, na.rm = TRUE) (sub-group counts divided by the sum of sub-group counts).\r\n__**Note__: na.rm = TRUE instructs the function to remove NA values from the sum of values in our sub-group.\r\n\r\n\r\nmtcars %>%\r\n  group_by(am, cyl) %>%\r\n  summarize(Counts = n()) %>%\r\n  mutate(Proportions = Counts / sum(Counts, na.rm = TRUE))\r\n\r\n\r\n\r\n\r\nam\r\ncyl\r\nCounts\r\nProportions\r\n0\r\n4\r\n3\r\n0.1578947\r\n0\r\n6\r\n4\r\n0.2105263\r\n0\r\n8\r\n12\r\n0.6315789\r\n1\r\n4\r\n8\r\n0.6153846\r\n1\r\n6\r\n3\r\n0.2307692\r\n1\r\n8\r\n2\r\n0.1538462\r\n\r\nAlso notice that we’re able to do this within one continuous pipe.\r\nTask 6: Data dictionary (Table 2)\r\nTable 2 is the data dictionary for the Kenya dataset, with columns added to annotate the variables and provide summary statistics. Using output from skim():\r\nFill in the columns in this table for the variables originally in the dataset.\r\nAdd rows for the 7 new variables that you have created in both Labs 0 and 1.\r\nTask 7: Short answer\r\nEnter your response into your own RMarkdown file\r\nExamine the range and proportion of missing values for each of the 7 variables you have created in Labs 0 and 1. Are there characteristics of any of these variables that are concerning (e.g., missing, suspicious or impossible values)? In contemplating analysis of these data, what do you think should be done with anomalous information? What effect would missing values have on the validity of your analyses (e.g., how might missing or extreme values affect inferences)? (Response no longer than 250 words, please)\r\n\r\n\r\n\r\n",
      "last_modified": "2021-08-31T12:50:33-04:00"
    },
    {
      "path": "lab_2.html",
      "title": "GLHLTH 705 Lab 2",
      "description": "__Due:__ October 22, 2021 by 14:00 ET\n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\nLAB MATERIALS:\r\nLab 2 Goals:\r\nTask 1: Load libraries and data\r\nTask 2: Code derived variables\r\nTask 2a: size\r\nTask 2b: belowavg\r\nTask 2c: pnc\r\nTask 2d: rural\r\nTask 2e: education\r\nTask 2f: death\r\nTask 2g: time\r\n\r\nTask 3: Close the cohort\r\nTask 3a: Exclude irrelevant observations\r\nTask 3b:\r\nTask 3c: Recode time to (time + 1)\r\nTask 3d: Save new dataset\r\n\r\nTask 4: Frequency histograms\r\nTask 5: Boxplots of time\r\nTask 6: Frequency counts and percentages (Table 1)\r\nTask 7: Update data dictionary (Table 2)\r\nTask 8: Short Answer\r\n\r\nLAB MATERIALS:\r\nR Markdown file for Lab 2. Click link to download. Fill it in with your answers to the following lab tasks. When you’re ready to submit, name it as Lab2_FirstinitialYourlastname.Rmd, and submit it using the Sakai dropbox.\r\nExcel document for Tables 1 & 2 TODO: Set up answer entry on Sakai?\r\nLab_2_kenya.rds - data file available on Sakai\r\nLab 2 Goals:\r\nGenerate derived variables\r\nIdentify and recode special values\r\nRun descriptive statistics for continuous and categorical variables\r\nGenerate graphics for continuous variables\r\nGenerate a complete data dictionary\r\nTask 1: Load libraries and data\r\nFor this assignment, use the dataset that I have supplied you – lab_2_kenya.rds. You will need the {tidyverse} package as well.\r\nTask 2: Code derived variables\r\nConstruct the following derived variables. For each, examine the component variables for missing values (e.g. ‘missing’, ‘unknown’) and be sure to set derived variable values appropriately (e.g., check to make sure that your NA values have not been mistakenly added to another category). Variables should be of the correct type. Convert all categorical variables to factors, labeling them appropriately.\r\nTask 2a: size\r\nsize: Categorical variable describing size of child at birth (subjectively described by mother). Note that greater values indicate smaller size: [1=very large, 2=larger than average, 3=average, 4=smaller than average, 5=very small]. Based on variable m18.\r\nTask 2b: belowavg\r\nbelowavg: Dichotomous variable indicating if the child’s size was below average [0= average, larger than average or very large, 1=smaller than average, very small]\r\nTask 2c: pnc\r\npnc: Dichotomous categorical variable for any prenatal care [0 = the referent category; no prenatal care; 1 = the index category; received prenatal care]. Based on variable m2n.\r\nTask 2d: rural\r\nrural: Dichotomous indicator of rural residence [0=urban, 1=rural]. Based on variable v025.\r\nTask 2e: education\r\neducation: Categorical educational level obtained by mother [create values for 0=did not attend school, 1=primary school only, 2=post-primary education]. Based on variable s109.\r\nTask 2f: death\r\ndeath: Dichotomous categorical variable for death within the first 5 years of life [0 = child alive at 5th birthday (the referent category); 1 = child died before 5th birthday (the index category)]. Based on the variables b5 (death indicator) and b7.\r\nNote: When creating this new variable, we want our conditional statement to account for two factors: Whether a child died, and if they did, whether or not that death occurred before their 5th birthday.\r\nTask 2g: time\r\ntime: Continuous variable for the age at death OR age at interview for children still alive. Based on b7 (age at death for children dead at interview); for children alive at time of interview, calculate their age using v008 (date of interview) and b3 (date of birth). Note that times are given in months.\r\nTask 3: Close the cohort\r\nCreate the equivalent of a closed cohort for the analysis of 5-year childhood mortality from this dataset . In a closed cohort, everyone must be at risk of the outcome at entry into the cohort, and all members of the cohort must remain at risk until they experience the outcome or complete the entire risk period for the outcome.\r\nA closed cohort allows for estimation of absolute (unconditional) risks and associated effect measures. For this study, the risk period for child mortality begins at birth and ends when a child dies, or when 60 months of life have been completed. I recommend completing the steps below in order to generate what we want. There are other orders in which to do this, but sometimes you will get incorrect results. If you’re adventurous, I recommend trying other ways to code this; you’ll learn a lot from that exercise.\r\nNOTE: Complete task 3 before completing the remaining tasks.\r\nTask 3a: Exclude irrelevant observations\r\nExclude those observations from the dataset where the child was alive (death=0) and follow-up time (time) was less than 60 months (think about which Boolean operator you’ll need to exclude those children). You should end up with 16,828 subjects in the dataset. In this step, you should also use an assignment operator (<-) to assign this closed cohort to a different object name. That way, you will be able to work with this dataset separately in the coming steps.\r\nTask 3b:\r\nAlso, if a child was still alive by their 5th birthday, we need to recode their time to 60 (we are censoring these observations). These children were alive at 5 years of age and so should not be counted as deaths in our analysis.\r\nTask 3c: Recode time to (time + 1)\r\nFinally, recode the time variable to indicate the month of life during which death or interview occurred (add 1 month to the current value). The range on time should now be 1 month – 61 months.\r\nTask 3d: Save new dataset\r\nSave this new dataset containing 16828 subjects (make sure to give it a new name).\r\nTask 4: Frequency histograms\r\nGenerate frequency histograms of time, overall and separately for levels of death. Put meaningful axis labels and a title on each figure.\r\nTODO: Help with histograms in ggplot2\r\nTask 5: Boxplots of time\r\nGenerate boxplots of time, overall and separately for levels of death. Put meaningful axis labels and a title on each figure.\r\nTODO: Help with boxplots in ggplot2\r\nTask 6: Frequency counts and percentages (Table 1)\r\nFill in Table 1 with the frequency counts and percentages for the levels of the new categorical variables you have generated. Calculate percentages only for the non-missing values. Round percentages to 1 decimal place.\r\nRefer to the previous lab for help on frequency counts and percentages.\r\nVariables:\r\nSize at birth (size)\r\nSize at birth categorical (belowavg)\r\nPrenatal care (pnc)\r\nResidence type (rural)\r\nMother’s education (education)\r\nDeath by 5 years (death)\r\nTask 7: Update data dictionary (Table 2)\r\nUpdate your data dictionary for the Kenya dataset, adding the newly created variables.\r\nTask 8: Short Answer\r\nExamine the range and proportion of missing for each of the variables 7 you have created in this lab. Are there characteristics of any of these variables that are concerning (e.g., missing, suspicious or impossible values)? In contemplating analysis of these data, what do you think should be done with anomalous information? What effect would missing values have on the validity of your analyses (e.g., how might missing or extreme values affect inferences)?\r\nProvide your answer to this question within your own RMarkdown file\r\n\r\n\r\n\r\n",
      "last_modified": "2021-08-31T12:50:34-04:00"
    },
    {
      "path": "lab_3.html",
      "title": "GLHLTH 705 Lab 3",
      "description": "__Due:__ November 12, 2021 by 14:00 ET\n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\nLAB MATERIALS:\r\nLab 3 Goals:\r\nTask 1: Load libraries and dataset\r\nTask 2: BIG PICTURE\r\nTask 3: Table 1 - Tabular analysis of continuous variables by mortality status\r\nSummary statistics by variable\r\nStratified summary statistics\r\nT-tests and Kruskal-Wallis tests\r\n\r\nTask 4: Table 2 - Tabular analysis of cat. variable by death\r\n{tableone} package\r\nCreate a vector of variables\r\nConstruct {tableone} objects\r\nPrint table\r\nFisher’s exact test of significance\r\n\r\nTask 5: Table 3 - RDs, RRs, and ORs\r\nTake a deep breath.\r\nFrequency counts\r\nRisk\r\nRisk difference (RD), Risk Ratio (RR), and Odds Ratio (OR)\r\nmAssoc() for Measures of Association\r\n\r\nTask 6: Side-by-side boxplots\r\n\r\nLAB MATERIALS:\r\nR Markdown file for Lab 3. Click link to download. Fill it in with your answers to the following lab tasks. When you’re ready to submit, name it as Lab3_FirstinitialYourlastname.Rmd, and submit it using the Sakai dropbox.\r\nExcel document for Tables 1 & 2 TODO: Set up answer entry on Sakai?\r\nLab_3_kenya.rds - data file available on Sakai\r\nLab 3 Goals:\r\nAssess the bivariate relationships between outcome and covariables and the statistical association between the two variables\r\nEstimate the epidemiologic measures of association (risk difference, risk ratio and odds ratio) between risk factors and outcome\r\nTask 1: Load libraries and dataset\r\nFor this assignment, we’ll need packages {tidyverse}, {skimr}, {epiR}, {tableone}, and {epiAssist}\r\nFor your data, you will use the dataset ‘Lab_3_kenya.rds’. This has all of the variables accumulated through Lab 2.\r\nTask 2: BIG PICTURE\r\nJust think on this for a moment. Internalize it. Come back to it:\r\nFor this and subsequent labs, we are considering 1) Child mortality within 5 years as our outcome (response) variable\r\n2) Birth order >= 5 as the main risk factor variable and\r\n3) Other variables as potential confounders or modifiers of the relationship between mortality and birth order.\r\nTask 3: Table 1 - Tabular analysis of continuous variables by mortality status\r\nFill in Table 1 with the descriptive statistics for maternal BMI and maternal age, overall and by levels of child mortality within 5 years. This table also includes statistical measures (T-test and Kruskal-Wallis) of the association between outcome and maternal BMI and age (i.e., comparing the distribution of maternal BMI for the children who died and those who were alive). Round p-values to 3 decimal places.\r\nSummary statistics by variable\r\nThe skim() function provided by {skimr} makes this task a simple one. However, unlike previous labs where we ran a summary on the entire dataset, in this task we want individual variables.\r\nWe can use a pipe with skimr() to call specific variables, like this:\r\n\r\n\r\ndata %>%\r\n  skim(variable)\r\n\r\n\r\n\r\nStratified summary statistics\r\nSince we’re already doing things in a pipe, we can use the filter function to retrieve summary statistics based on mortality status. To stratify by the variable death, we should add the group_by() function to our pipeline, specifying that we want to “group by death, then skim the variable mbmi”.\r\nAs a final note, you’ll notice that skim() doesn’t provide raw counts when we stratify by death. To get raw counts, run the same code, except replace the entire skim() function with count(). No arguments necessary.\r\nT-tests and Kruskal-Wallis tests\r\nT-tests and their nonparametric cousin, Kruskal-Wallis, can be used to compare means between sub-populations, with the null hypothesis (H0) being that all means are equal.\r\nThere are many functions in R that perform a specific task, while computing a p-value as a matter of course by applying the appropriate statistical test of significance (see Task 4).\r\nHowever, there are a few simple functions from the {stats} package that we can use to manually run these tests. To implement, we use functions t.test() and kruskal.test(). Both take the same arguments:\r\nA formula: depVar ~ indepVar\r\nA data frame\r\nIn this case, we are interested in knowing whether or not the means for mbmi and mage differ by strata of child mortality status, death.\r\nWhat this means is that when we’re building our formula, the “y”/dependent variable (the resulting means), should be on the left, while the “x”/independent variable (a categorical stratifying variable), should be on the right.\r\n\r\nFormulas: In R, formulas, indicated with ~ in place of an equals sign (=), play numerous roles that play by different rules depending on the function using them. Most of the time, they take the form y ~ x. Click here for an in-depth reference article on formulas in R.\r\nExample of T-tests and Kruskal-Wallis\r\nAs an example, we might use the starwars dataset, available in the {dplyr} package, to inspect the body mass of different characters by species:\r\n\r\nname\r\nheight\r\nmass\r\nhair_color\r\nskin_color\r\neye_color\r\nbirth_year\r\nsex\r\ngender\r\nhomeworld\r\nspecies\r\nLuke Skywalker\r\n172\r\n77\r\nblond\r\nfair\r\nblue\r\n19.0\r\nmale\r\nmasculine\r\nTatooine\r\nHuman\r\nC-3PO\r\n167\r\n75\r\nNA\r\ngold\r\nyellow\r\n112.0\r\nnone\r\nmasculine\r\nTatooine\r\nDroid\r\nR2-D2\r\n96\r\n32\r\nNA\r\nwhite, blue\r\nred\r\n33.0\r\nnone\r\nmasculine\r\nNaboo\r\nDroid\r\nDarth Vader\r\n202\r\n136\r\nnone\r\nwhite\r\nyellow\r\n41.9\r\nmale\r\nmasculine\r\nTatooine\r\nHuman\r\nLeia Organa\r\n150\r\n49\r\nbrown\r\nlight\r\nbrown\r\n19.0\r\nfemale\r\nfeminine\r\nAlderaan\r\nHuman\r\nOwen Lars\r\n178\r\n120\r\nbrown, grey\r\nlight\r\nblue\r\n52.0\r\nmale\r\nmasculine\r\nTatooine\r\nHuman\r\n\r\nOur formal question we’d like to answer might be: “Does mean character height differ by a character’s gender?”\r\nWe can calculate summary statistics for sub-group frequency counts and means:\r\n\r\n\r\nstarwars %>%\r\n  filter(!is.na(gender)) %>%\r\n  group_by(gender) %>%\r\n  summarize(n = n(), mean = mean(height, na.rm = TRUE))\r\n\r\n\r\n\r\n\r\ngender\r\nn\r\nmean\r\nfeminine\r\n17\r\n164.6875\r\nmasculine\r\n66\r\n176.5161\r\n\r\nOur T test would effectively tell us whether or not those means are statistically different from each other given the available data:\r\n\r\n\r\nt.test(height ~ gender, data = starwars)\r\n\r\n#>  Welch Two Sample t-test\r\n#>\r\n#> data:  height by gender\r\n#> t = -1.5596, df = 37.315, p-value = 0.1273\r\n#> alternative hypothesis: true difference in means between group feminine and group masculine #> is not equal to 0\r\n#> 95 percent confidence interval:\r\n#>  -27.191682   3.534423\r\n#> sample estimates:\r\n#>  mean in group feminine mean in group masculine \r\n#>                164.6875                176.5161 \r\n\r\n\r\n\r\nHowever, since our group for gender == feminine only contains 17 observations, so we might decide that a nonparametric test (one that doesn’t assume a normal distribution), might be more appropriate for the small sample size. In this case we can use the Kruskal-Wallis test:\r\n\r\n\r\nkruskal.test(height ~ gender, data = starwars)\r\n\r\n#> Kruskal-Wallis rank sum test\r\n#> \r\n#> data:  height by gender\r\n#> Kruskal-Wallis chi-squared = 8.6845, df = 1, p-value = 0.003209\r\n\r\n\r\n\r\nNotice the difference in the conclusion you would draw from either test. For the T-test, we should conclude that we cannot reject the null hypothesis, whereas the nonparametric test tells us that there is a statistically significant difference in the mean height of the two groups.\r\nTask 4: Table 2 - Tabular analysis of cat. variable by death\r\nFill in Table 2 with the frequency counts and COLUMN percentages for All Subjects and ROW percentages for the proportions alive and dead for the levels of the categorical variables listed in the table, again overall and by levels of child mortality within 5 years. Calculate percentages only for the non-missing values. Round percentages to 1 decimal place. This table also includes statistical measures (Chi-Sq and Fisher’s exact test) of the association between child dead/alive and the variables in the table. Round p-values to 3 decimal places.\r\n{tableone} package\r\nWe will complete this task by first using the package called {tableone}, a powerful tool for tabular analysis, especially for a stratified analyses like this one.\r\nMore on {tableone}: I’ve used this package a LOT for generating data reports. It is a super convenient tool for compiling baseline data. It’s also easy to copy/paste into excel using the print() argument quote = TRUE, which creates character-delineation between values, that excel recognizes as columns. Find a tutorial on all of that here\r\nA {tableone} object will generate mean and standard deviation for continuous variables, and frequency counts and percentages for variables that we designate as “factorVars” (categorical).\r\nThe primary function in this package is called CreateTableOne(), which has several arguments of central importance for it to run:\r\n1.) vars = - a vector of variable names, written as strings\r\n2.) strata = - a categorical variable for stratifying (if desired)\r\n3.) data = - your data frame\r\n4.) factorVars = - used to convert character variables to factors on the fly\r\nfactorVars: You shouldn’t need to use factorVars = for your kenya data, since we have already converted categorical variables to factors in the previous labs\r\nCreate a vector of variables\r\nFor starters, we’ll need a vectorized list of our variables of interest, written as strings (in quotes).\r\nAs an example, we might use the derived variables in our kenya dataset for size and pnc. To make our code more readable, we will create a vector object that contains both variable names. To do this, we put both variable names in quotes, separating them by commas within a vector, c(). Then we need to assign the vector a name so that we can call it later. Here, we’ve named the vector variables.\r\n\r\n\r\nvariables <- c('size', 'pnc')\r\n\r\n\r\n\r\nConstruct {tableone} objects\r\nSimple table\r\nFor a simple table of summary statistics, we only need to use arguments vars = and data =. It’s best practice to assign the {tableone} object to a name, so that we can use it in a print function in a moment.\r\n\r\n\r\nsimple_tab <- CreateTableOne(vars = variables, \r\n                             data = kenya)\r\n\r\n\r\n\r\nStratified table\r\nWe may also want to stratify our results by a second variable, like bord5.\r\n\r\n\r\nstratified_tab <- CreateTableOne(vars = variables, \r\n                                 data = kenya, \r\n                                 strata = 'bord5')\r\n\r\n\r\n\r\nPrint table\r\nWe can then use a print() function, with arguments specific to our {tableone} object, to view our simple and stratified tables.\r\nWith categorical variables, it’s typically best to use the argument showAllLevels = TRUE so that we see every level of our categorical variables. Setting showAllLevels to FALSE will leave out the referent level for each variable.\r\nSimple table\r\n\r\n\r\nprint(simple_tab, showAllLevels = TRUE)\r\n\r\n#>              level                  Overall      \r\n#>  n                                   16828        \r\n#>  size (%)     Very large                26 ( 7.4) \r\n#>               Larger than average       76 (21.7) \r\n#>               Average                  158 (45.1) \r\n#>               Smaller than average      65 (18.6) \r\n#>               Very small                25 ( 7.1) \r\n#>  pnc (%)      No prenatal care          15 ( 9.1) \r\n#>               Received prenatal care   149 (90.9) \r\n\r\n\r\n\r\nStratified table with Chi-squared test\r\nWhereas the simple table prints overall frequency counts and percentages, our stratified table will take the stratifying variable, and perform a chi-squared test to for differences between the various groups:\r\n\r\n\r\nprint(stratified_tab, showAllLevels = TRUE)\r\n\r\n#>              Stratified by bord5\r\n#>               level                  1-4 in Birth Order 5+ in Birth Order  p     test\r\n#>  n                                   13263              3565                         \r\n#>  size (%)     Very large                23 ( 9.3)          3 ( 2.9)        0.157     \r\n#>               Larger than average       51 (20.6)         25 (24.5)                  \r\n#>               Average                  108 (43.5)         50 (49.0)                  \r\n#>               Smaller than average      50 (20.2)         15 (14.7)                  \r\n#>               Very small                16 ( 6.5)          9 ( 8.8)                  \r\n#>  pnc (%)      No prenatal care           4 ( 3.7)         11 (20.0)        0.002     \r\n#>               Received prenatal care   105 (96.3)         44 (80.0)       \r\n\r\n\r\n\r\nNote that each row of the stratified results sum to the same result as given by the column for “Overall” in our simple_tab.\r\nAs the default for categorical variables, the {tableone} print() function performs a chi-squared test on the difference of proportions between the different strata for bord5.\r\nThere’s one glaring issue here, however: neither of our stratified variables meet the assumption of normality! Each contains at least one cross-tabulation with counts that are less than 5.\r\nFisher’s exact test of significance\r\nWhen we look at our decision tree for hypothesis testing, we notice that we need to use a Fisher’s Exact test when our data doesn’t meet the assumption of normality necessary for a Chi-squared test of significance.\r\nTo do this in a {tableone} object, we just need to include exact = TRUE in our print() function to calculate a Fisher’s Exact test, the nonparametric equivalent of the Chi-square test.\r\n\r\n\r\nprint(stratified_tab, showAllLevels = TRUE, exact = TRUE)\r\n\r\n#>               Stratified by bord5\r\n#>               level                  1-4 in Birth Order 5+ in Birth Order p      test \r\n#>  n                                   13263              3565                          \r\n#>  size (%)     Very large                23 ( 9.3)          3 ( 2.9)        0.141 exact\r\n#>               Larger than average       51 (20.6)         25 (24.5)                   \r\n#>               Average                  108 (43.5)         50 (49.0)                   \r\n#>               Smaller than average      50 (20.2)         15 (14.7)                   \r\n#>               Very small                16 ( 6.5)          9 ( 8.8)                   \r\n#>  pnc (%)      No prenatal care           4 ( 3.7)         11 (20.0)        0.001 exact\r\n#>               Received prenatal care   105 (96.3)         44 (80.0)                   \r\n\r\n\r\n\r\nAlthough in this particualr interest we draw the same conclusions from the Fisher’s exact test as we do from the Chi-square test, assuming normality when the evidence doesn’t exist in the data can lead to misleading results further down the line.\r\nTask 5: Table 3 - RDs, RRs, and ORs\r\nFill in Table 3 with frequency counts overall and by levels of child mortality within 5 years and with the stratum-specific risk of death, the risk difference, risk ratio and the odds ratio of death comparing levels for each of the 5 variables to the reference group. The reference group for each variable is noted. For the two 3-level variables, calculate risk differences and ratios for each non-referent level relative to the reference group. Round risks to 3 decimal places and ratios to 2 decimal places. The STATA cs command provides a convenient way to produce the data for this table.\r\nTake a deep breath.\r\nDon’t let this table overwhelm you. We’re going to break it down step-by-step\r\nFrequency counts\r\nThe frequency counts in this table align with those generated for Table 2.\r\nRisk\r\nThe value for risk also aligns with that generated in the previous task, except that in Table 2 you reported it as a percentage. Risk should be reported as a proportion, rounding to the third decimal.\r\nRisk difference (RD), Risk Ratio (RR), and Odds Ratio (OR)\r\nRD, RR, and OR are what are known as “measures of association” because they are used to compare outcomes between levels of a categorical variable. In epidemiology, we refer to the baseline variable as the “Referent variable” and the comparator variable as the “Index variable”.\r\nIn practical terms, for things like RRs and ORs, the index variable goes in the numerator and the referent variable goes in the denominator.\r\n\\[\\frac{index\\ \\ variable}{referent\\ \\ variable}\\]\r\nWe can use our underlying knowledge of these measures of association to calculate our answers by hand and confirm the results that we’ll obtain using the functions that follow.\r\nmAssoc() for Measures of Association\r\nFor this task, we will use mAssoc(), which generates all three measures of association in a singular output.\r\nTo use mAssoc(), we need to take several steps to ensure that our data is in the proper shape for our measures to be calculated appropriately:\r\n1.) Create a table object using table()\r\n2.) Rearrange the table with flipTable() to put it in the appropriate alignment for use by mAssoc()\r\n3.) Submit our rearranged table to mAssoc().\r\nAs a way of demonstrating, we will use the variable bord5, stratified by death to calculate our measures of association.\r\nCreate a table object with table()\r\nThe first step is fairly straightforward. We need to generate a table object and assign it a name. Keep in mind that we want our exposure variable (in this case, bord5) along the left side of the table and our outcome variable (death) at the top of the table.\r\n\r\n\r\ntab_bord5 <- table(kenya$bord5, kenya$death)\r\ntab_bord5\r\n\r\n#>                      Alive  Dead\r\n#>  1-4 in Birth Order 11809  1454\r\n#>  5+ in Birth Order   3002   563\r\n\r\n\r\n\r\nRearrange table with flipTable()\r\nmAssoc() comes from the package {epiAssist}. The main argument it takes is a 2x2 table object (or a stratified 2x2 table object) in the following format, with the cross-tabulation of outcome of interest (Outcome +) and exposure of interest (Expose +) in the top-left cell:\r\n\r\nThe function mAssoc() is a wrapper function for the epi.2by2() function from package {epiR}.\r\n\r\n\r\n#   -----------  ----------  ----------\r\n#                Outcome +   Outcome -\r\n#   -----------  ----------  ----------\r\n#     Expose +   cell 1      cell 3          \r\n#     Expose -   cell 2      cell 4\r\n#   -----------  ----------  ----------\r\n\r\n\r\n\r\nIf we refer back to our table for bord5 and death, we’ll notice that it currently displays the inverse:\r\n\r\n\r\ntab_bord5\r\n\r\n#>                       Alive  Dead\r\n#>  1-4 in Birth Order  11809   1454\r\n#>  5+ in Birth Order    3002    563\r\n\r\n\r\n\r\nTo fix this, we can use flipTable(), and assign it back to the same name for the table object:\r\n\r\n\r\ntab_bord5 <- flipTable(tab_bord5)\r\n\r\ntab_bord5\r\n\r\n#>                      Dead  Alive\r\n#>  5+ in Birth Order    563   3002\r\n#>  1-4 in Birth Order  1454  11809\r\n\r\n\r\n\r\nGenerate measures of association with mAssoc()\r\nTo generate our measures of association and their accompanying confidence intervals, all we need to do is submit our rearranged table to mAssoc(), specifying that our data is cohort data with method = \"cohort.count\", and setting our confidence level to 0.95 with conf.level = .95:\r\n\r\n\r\nmAssoc(tab_bord5, method = \"cohort.count\", conf.level = 0.95)\r\n\r\n#> MEASURES OF ASSOCIATION FOR:  5+ in Birth Order (index) vs.  1-4 in Birth Order (referent)\r\n#> Point estimates and 95% CIs:\r\n#> -------------------------------------------------------------------\r\n#> Inc risk ratio                                 1.44 (1.32, 1.58)\r\n#> Odds ratio                                     1.52 (1.37, 1.69)\r\n#> Attrib risk in the exposed *                   0.05 (0.04, 0.06)\r\n#> Attrib fraction in the exposed (%)            30.58 (24.05, 36.56)\r\n#> Attrib risk in the population *                0.01 (0.00, 0.02)\r\n#> Attrib fraction in the population (%)          8.54 (6.20, 10.82)\r\n#> -------------------------------------------------------------------\r\n#> Uncorrected chi2 test that OR = 1: chi2(1) = 62.125 Pr>chi2 = <0.001\r\n#> Fisher exact test that OR = 1: Pr>chi2 = <0.001\r\n#>  Wald confidence limits\r\n#>  CI: confidence interval\r\n#>  * Outcomes per population unit \r\n\r\n\r\n\r\nReading the output:\r\nHere is a quick guide to the components of the output relevant to this activity:\r\nInc risk ratio: Our basic risk ratioOdds ratio: Our basic odds ratioAttrib risk in the exposed: The risk difference. mAssoc() takes another argument, units =, which by default is set to 1. This regulates our output for the measure of risk difference.\r\n\r\nIf we set units = 100, our attributable risk in the expose (risk difference) would be interpretable as a difference of x number of outcomes per 100 people in the index group when compared to the referent group. Since our population unit is set to 1, this number is simply interpreted as an individual’s risk difference of experiencing the outcome if they are in the index level for the exposure.\r\nmAssoc() when exposure variable has 3+ levels\r\nNow that you’ve seen the code for bord5 as an example, if you’re paying close attention, you may have started to wonder, “But what about for variables like education, when there’s more than one index variable?”\r\nGood news: flipTable() and mAssoc() are designed to handle exposure variables with any number of levels. This means that the steps you take for bord5, male, and rural will also apply to magec and education, simple as that.\r\nRunning mAssoc() when your exposure variable has 2+ levels will render multiple outputs for each respective index variable. The person who wrote this function was even nice enough to label each of those levels and whether or not they are the index or the referent level.\r\nTask 6: Side-by-side boxplots\r\nGenerate side-by-side boxplots of maternal BMI (mbmi) and maternal age (magec) by levels of child mortality (death).\r\nClick here for help with boxplots using {ggplot2}\r\nFor this activity, we need boxplots that plot continuous variables based on child mortality. In ggplot2, we can stratify boxplots by using a second categorical variable when we specify our aesthetics (aes()). In other words, if you put mbmi on the y-axis, you can put death as the variable on the x-axis.\r\nAdditionally, don’t forget to label your axes, give the plot a title, and save it using ggsave()\r\n\r\nWithin your call to geom_boxplot(), you can also change the color with fill = \"colorname\". Find a list of color names here\r\nHere’s an example of what we are looking for, using the starwars dataset. We’ve adjusted the dimensions a bit to accommodate our lone outlier. Care to guess who it is?\r\n\r\n\r\n\r\n\r\n\r\n\r\nFigure 1: A heavy slug\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2021-08-31T12:50:36-04:00"
    },
    {
      "path": "lab_4.html",
      "title": "GLHLTH 705 Lab 4",
      "description": "__Due:__ November 23, 2021 by 22:00 ET\n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\nLAB MATERIALS:\r\nLab 2 Goals:\r\nLab 4\r\nTask 1: Load libraries & data\r\nTask 2: Tabular analysis of stratifying variables\r\nTask 2.1: Frequency counts and risk\r\nTask 2.2: Create epi.2by2() (TODO) object with table() and flipTable()\r\nTask 2.3: Risk differences, tests of homogeneity, and pooled estimates\r\nTask 2.4: Risk ratios, tests of homogeneity, and pooled estimates\r\n\r\n\r\n\r\nLAB MATERIALS:\r\nR Markdown file for Lab 4. Click link to download. Fill it in with your answers to the following lab tasks. When you’re ready to submit, name it as Lab4_FirstinitialYourlastname.Rmd, and submit it using the Sakai dropbox.\r\nExcel document with Table for Task 2 TODO: Set up answer entry on Sakai?\r\nLab_4_kenya.rds - data file available on Sakai\r\nLab 2 Goals:\r\nCalculate measures of association between birth order and child mortality by 5 years within strata of other variables\r\nAssess potential for confounding and effect modification (mediation) by those other variables.\r\nLab 4\r\nTask 1: Load libraries & data\r\nFor this assignment, use the dataset ‘Lab_4_kenya.rds’.\r\nHopefully at this point in the semester, you have a pretty good grasp on how to load libraries and data, but if not:\r\nHelp loading libraries Help loading data\r\nTask 2: Tabular analysis of stratifying variables\r\nThe table in Task 2 is an example of a stratified analysis of exposure and outcome variables. Essentially, we want to know if the four variables listed in the first part of Task 2 are confounders or effect measure modifiers. Is the difference in mortality status of children of various birth orders at all different between boys and girls? What about rural and urban families? Mothers of different age categories? And so on…\r\nBelow, we present the full set of steps needed to complete the tabular analysis. We then break down each step with instruction on how to complete it in R.\r\nSo, to complete the table, you must take the following steps:\r\n1.) Fill in the table with the numbers of deaths (death, the outcome) and the 60-month risk of death according to birth order (bord5, the exposure), stratified by:\r\nChild’s gender (variable male)\r\nRural/urban residence (variable rural)\r\nMaternal age (variable magec)\r\nMaternal education (variable education)\r\n2.) Create an epi.2by2()/mAssoc() (TODO: decide which one to use) for each of the coviarables in part 1 of task 2 by:\r\nCreate a table object with table(), similar to the one you created in lab 3, where your outcome of interest is mortality (death) and your exposure of interest is child birth order (bord5). Except this time, include a third stratifying variable.\r\nFlip the table using flipTable() so that the index level of the exposure and outcome of interest is in the top-left corner of the table.\r\nCreate your epi.2by2() object by using the flipped table object.\r\n3.) Calculate the Risk Difference for mortality in association with bord5 within each covariable stratum\r\nExtract risk differences and confidence intervals from epi.2by2 (TODO) object\r\nExtract the homogeneity test statistic and p-value for each covariate listed in part 1a-1d To get the stratified and pooled estimates and p-values, run the epi.2by2() command. For pooled and crude estimates for risk difference, you will need to access the following values within your epi.2by2 object:\r\nvalue 1\r\nvalue 2\r\nvalue 3\r\nFor each covariable listed above, report the standardized pooled estimate for the risk difference and the Mantel-Haenszel pooled estimate (M-H combined) for the RR\r\n4.) Calculate the Risk Ratio for mortality in association with bord5 within each covariable stratum\r\nExtract the risk ratios and confidence intervals from epi.2by2() (TODO) object\r\nExtract the homogeneity test statistic and p-value for each covariate listed in part 1a-1d To get the stratified and pooled estimates and p-values, run the epi.2by2() command. For pooled and crude estimates for risk difference, you will need to access the following values within your epi.2by2 object:\r\nvalue 1\r\nvalue 2\r\nvalue 3\r\nFor each covariable listed above, report the Mantel-Haenszel pooled estimate (M-H combined) for the RR\r\nTask 2.1: Frequency counts and risk\r\nWe’ve been using dplyr commands like filter(), group_by(), summarize(), and count() to generate strata-specific frequency counts since Lab 1.\r\nSince this lab has us dealing with 3 different stratifying variables at a time, group_by() is especially useful, since we can list all stratifying variables in group_by(), then count the variables with count().\r\nAs a quick example, we could use the mtcars dataset to see how many cars in the dataset have each individual combination of number of cylinders (cyl), gears (gear), and transmission types (am):\r\n\r\n\r\nmtcars %>%\r\n  group_by(am, cyl, gear) %>%\r\n  count()\r\n\r\n\r\n\r\n\r\nam\r\ncyl\r\ngear\r\nn\r\n0\r\n4\r\n3\r\n1\r\n0\r\n4\r\n4\r\n2\r\n0\r\n6\r\n3\r\n2\r\n0\r\n6\r\n4\r\n2\r\n0\r\n8\r\n3\r\n12\r\n1\r\n4\r\n4\r\n6\r\n1\r\n4\r\n5\r\n2\r\n1\r\n6\r\n4\r\n2\r\n1\r\n6\r\n5\r\n1\r\n1\r\n8\r\n5\r\n2\r\n\r\nTo take within-group proportions (read: RISK), we can add a pipe (%>%) and mutate() function to the end of the last chunk of code. Since our observations are still implicitly in groups from our call to group_by(), we need to re-group, this time only by the first two variable levels, am and cyl. Taking the sum, sum() of n will then give us the within-group sum. Dividing n by sum(n) thus gives us within group proportions. For example, this will tell us the proportion of automatic (am == 1), 4-cylinder (cyl == 4) vehicles that have either 3 gears and 4 gears. When we run that code below, we see that a third of the vehicles in that particular grouping have 3 gears, and two-thirds have 4 gears:\r\n\r\n\r\nmtcars %>%\r\n  group_by(am, cyl, gear) %>%\r\n  count() %>% \r\n  group_by(am, cyl) %>%\r\n  mutate(proportion = n/sum(n))\r\n\r\n\r\n\r\n\r\nam\r\ncyl\r\ngear\r\nn\r\nproportion\r\n0\r\n4\r\n3\r\n1\r\n0.3333333\r\n0\r\n4\r\n4\r\n2\r\n0.6666667\r\n0\r\n6\r\n3\r\n2\r\n0.5000000\r\n0\r\n6\r\n4\r\n2\r\n0.5000000\r\n0\r\n8\r\n3\r\n12\r\n1.0000000\r\n1\r\n4\r\n4\r\n6\r\n0.7500000\r\n1\r\n4\r\n5\r\n2\r\n0.2500000\r\n1\r\n6\r\n4\r\n2\r\n0.6666667\r\n1\r\n6\r\n5\r\n1\r\n0.3333333\r\n1\r\n8\r\n5\r\n2\r\n1.0000000\r\n\r\nTask 2.2: Create epi.2by2() (TODO) object with table() and flipTable()\r\nRemember mAssoc() from lab 3? Well, surprise! mAssoc() is what we call a “wrapper” function for epi.2by2(). It means exactly what it sounds like– mAssoc() wraps around epi.2by2(), using its functionality for a different purpose.\r\nIn the previous lab(TODO: Add link), we used mAssoc() to generate measures of association for a single two-way tabulation with multiple index levels in the exposure of interest.\r\nFor this lab, epi.2by2() satisfies most of our needs: when given a two-way table() object that is stratified by a third variable, it returns crude and adjusted pooled measures of association. What does that mean?\r\nYou might think of a “pooled estimate” as essentially a weighted average of the individual strata’s measures of assocation: a crude pooled risk difference is an average of risk differences across strata, weighted according to the n of each subgroup. (TODO: is it?)\r\nFor this lab, we are only interested in the adjusted estimates (“M-H”).\r\nJust like mAssoc(), epi.2by2() requires a table object with the cross-tabulation of the exposure and outcome of interest in the top left corner of the table. That being the case, you need to create a table object with table(), flip it with flipTable(), and then feed that flipped table object to the function epi.2by2().\r\nepi.2by2() requires a few more arguments. We should specify the following:\r\n\r\n\r\nepi.2by2(tableObject, # our table object\r\n         \r\n         method = \"cohort.count\", # specifies the type of study we're evaluating -- impacts the weighting of subgroups in the pooled estimates (TODO: does it?)\r\n         \r\n         unit = 1, # this allows our measures of risk (including risk difference) to be reported as \"per person\" rather than \"per 100\", which would be the default\r\n         \r\n         conf.level = .95 # this is the default, but it's always good to remind ourselves of our confidence level)\r\n         )\r\n\r\n\r\n\r\nTask 2.3: Risk differences, tests of homogeneity, and pooled estimates\r\nOnce you’ve created your epi.2by2 object, you can call it from the environment to print it. The printed output will give you M-H pooled estimates of risk difference with accompanying confidence intervals.\r\nBut we are interested in reporting risk differences to three decimals. We can dig into the object with $ to locate the exact measures of risk difference.\r\n\r\nSee this link if you are still uncertain about the functionality of $\r\nRisk difference\r\nTo access the risk differences of individual strata, use the following path to refer to that exact component of your epi.2by2 object:\r\n\r\n\r\nobject$massoc.detail$ARisk.strata.wald\r\n\r\n\r\n\r\nYou’ll notice that this returns an estimate for each strata. Our table, however, asks for two estimates for each strata. What gives??\r\nSince we have pre-defined “referent” and “index” levels, with a risk difference defined as “the level of interest minus the referent level”, what’s the result when our referent level is also our level of interest?\r\nTest of homogeneity\r\nInconveniently, to test for homogeneity between the risk differences within our various substrata, we need to run a separate command. If your original flipTable object is properly oriented, it should suffice. Use your flipTable as an argument in the function epiHomog() to return the test statistic and p-value of for a test of homogeneity at a significance level of 0.05:\r\n\r\n\r\nepiHomog(flippedTable)\r\n\r\n\r\n\r\nMantel-Haenszel pooled estimate of RD\r\nFinally, we want the pooled estimate of risk difference. Since we need to report it to three decimal points, as per lab submission guidelines on rounding, we need to reach into our epi.2by2 object and find that Mantel-Haenszel pooled estimate. You can access it by calling the following path:\r\n\r\n\r\nobject$massoc.detail$ARisk.mh.wald\r\n\r\n\r\n\r\nTask 2.4: Risk ratios, tests of homogeneity, and pooled estimates\r\nThe process for obtaining risk ratios, test statistic for homogeneity between strata, and pooled estimates is very similar to that of obtaining risk differences. Except this time, we don’t need a separate function for finding the test of homogeneity – epi.2by2 generates the test statistic for homogeneity of risk ratios itself. Perhaps it’s safe to understand this as meaning that testing for homogeneity on stratified risk ratios is a more common practice within the fields of epi and biostatistics. Nonetheless, it’s helpful to see how significance tests might differ according to which measure of association is being used.\r\n\r\n(TODO: add link to article on weighted least squares equation for homogeneity test on risk differences)\r\n(TODO: is there also a reason pertaining to confounding/EMM for why we’d want to test for homogeneity across stratified risk differences?)\r\nRisk ratios\r\nTo extract risk ratios from an epi.2by2 object, you can use the following path:\r\n\r\n\r\nobject$massoc.detail$RR.strata.wald\r\n\r\n\r\n\r\nAgain, when you’re finding these risk ratios, consider what we’re asking for in the table when we request the risk ratio of the referent group as it pertains to the referent group. What does it amount to when we take the ratio of something on itself?\r\nTest of homogeneity\r\nThe test of homogeneity for risk ratios is available in the following location within our epi.2by2 object\r\n\r\n\r\nobject$massoc.detail$wRR.homog\r\n\r\n\r\n\r\nMantel-Haenszel pooled estimate of RR\r\nThe the Mantel-Haenszel pooled estimate appears in the printed output when you call your epi.2by2 object from the environment, and is titled “Inc risk ratio (M-H)”. However, this is only reported to two decimals. To access the actual value instead of the rounded one, you can look for it in the following place within your epi.2by2 object:\r\n\r\n\r\nobject$massoc.detail$RR.mh.wald\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2021-08-31T12:50:38-04:00"
    },
    {
      "path": "mind_map.html",
      "title": "Mapping Our Steps",
      "description": "This page gives a birds-eye view of our analysis, where we've come from and where we're headed.\n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\nThe data science life cycle\r\n\r\nThe data science life cycle\r\nIn Hadley Wickam and Garrett Grolemund’s book, R for Data Science, they introduce a model of tools found in the typical data analysis project:\r\nDS Life CycleThis process consists of:\r\nimporting your data into R;\r\ntidying your data by shaping and storing it in a consistent format (luckily, this has largely already been done for us).\r\nOnce the data is tidy and coherent, we will need to:\r\ntransform the data by narrowing in on observations of interest, creating new variables, and calculating summary statistics;\r\nvisualize it to gain new perspective and insights;\r\nmodel the data with a well-defined research question.\r\nAnd this will all be for naught if without effectively communicating our findings.\r\nIn the lab section for Biostat 705 and 707, we’ll be working in a similar loop. To this toolbox, we might also add Workflow Management, which encompasses the bulk of what we do.\r\nThe below visualization aims to provide a visualization of this as we encounter it in-action.\r\n\r\n\r\n{\"x\":{\"diagram\":\"\\n\\n      digraph boxes_and_circles {\\n      \\n      graph[rankdir = LR, fontname = Helvetica]\\n      \\n      subgraph lab_1 {\\n      \\n      # adding node statements\\n      \\n      node [shape = circle]\\n      \\n      lab0 [label = \\\"Lab 0\\\", penwidth = 2.0]\\n      \\n      #Workflow\\n      #Explore\\n      #Transform\\n      \\n      node [shape = box]\\n      \\n      aa [label = \\\"Create workspace\\\"]\\n      ab [label = \\\"Inspect dimensions\\\"]\\n      ac [label = \\\"Generate summary statistics\\\"]\\n      ad [label = \\\"Create derived variable for mothers age\\\"]\\n      ae [label = \\\"Create categorical variable for mothers age\\\"]\\n      \\n      \\n      \\n      # add definition using node IDs\\n      \\n      lab0 -> aa #[label = \\\"Workflow\\\"]\\n      lab0 -> ab #[label = \\\"Explore\\\"]\\n      lab0 -> ac #[label = \\\"Explore\\\"]\\n      lab0 -> ad #[label = \\\"Transform\\\"]\\n      lab0 -> ae #[label = \\\"Transform\\\")]\\n      \\n      #aa -> Workflow\\n      #ab -> Explore\\n      #ac -> Explore\\n      #ad -> Transform\\n      #ae -> Transform\\n      \\n      }\\n      \\n      subgraph lab_1 {\\n    \\n      # adding node statements\\n      \\n      node [shape = circle]\\n      \\n      lab1 [label = \\\"Lab 1\\\", penwidth = 2.0]\\n      \\n      #Explore\\n      #Transform\\n      #Visualize\\n      #Workflow\\n      #Communicate\\n\\n      node [shape = box]\\n\\n      ba [label = \\\"Construct derived variables\\\\nbord5, male, mweight, mbmi\\\"]\\n      bb [label = \\\"Visualize with\\\\nhistograms and boxplots\\\"]\\n      bc [label = \\\"Calculate frequency counts\\\\nand percentages\\\"]\\n      bd [label = \\\"Create data dictionary\\\"]\\n      be [label = \\\"Consider implications of\\\\nmissing values\\\"]\\n      \\n      # edge definitions\\n      \\n      lab1 -> {ba, bb, bc, bd, be}\\n      \\n      #ba -> Transform\\n      #bb -> Visualize\\n      #bc -> Explore\\n      #bd -> Workflow\\n      #be -> Communicate\\n      \\n      \\n      }\\n      \\n      {rank = same; lab0 -> lab1}\\n      \\n      subgraph lab_2 {\\n      \\n      # adding node statements\\n      \\n      node [shape = circle]\\n      \\n      lab2 [label = \\\"Lab 2\\\", penwidth = 2.0]\\n      \\n      #Explore\\n      #Transform\\n      #Workflow\\n      #Visualize\\n      #Communicate\\n\\n      node [shape = box]\\n      \\n      ca [label = \\\"Construct derived variables\\\\nsize, belowavg, pnc, rural\\\\neducation, death, and time\\\"]\\n      cb [label = \\\"Update data dictionary\\\"]\\n      cc [label = \\\"Exclude observations where child was alive\\\\nbut less than 60 months of age\\\"]\\n      cd [label = \\\"Generate frequency histograms and\\\\nboxplo1ts for death and time\\\"]\\n      ce [label = \\\"Calculate frequency counts\\\\nandpercentages\\\"]\\n      cf [label = \\\"Consider implications of\\\\nanomalous and missing data\\\"]\\n      \\n      # define edges\\n      \\n      lab2 -> {ca, cb, cc, cd, ce, cf}\\n      \\n      #ca -> Transform\\n      #cb -> Workflow\\n      #cc -> Visualize\\n      #cd -> Explore\\n      #ce -> Explore\\n      #cf -> Communicate\\n      \\n      }\\n      \\n      {rank = same; lab1 -> lab2}\\n      \\n      subgraph lab_3 {\\n      \\n      # adding node statements\\n      \\n      node [shape = circle]\\n      \\n      lab3 [label = \\\"Lab 3\\\", penwidth = 2.0]\\n      \\n      #Explore\\n      #Workflow\\n      #Communicate\\n      #Model\\n\\n      node [shape = box]\\n      \\n      da [label = \\\"Define research question:\\\\nWhat are the factors that influence\\\\nthe outcome of child mortality by 5 years of age?\\\"]\\n      db [label = \\\"Generate statistics\\\\nof continuous variables\\\\nmbmi and mage, by mortality status\\\"]\\n      dc [label = \\\"Generate stratified frequency counts and\\\\npercentages of categorical variables, by mortality status\\\"]\\n      dd [label = \\\"Calculate T-tests (continuous vars)\\\\nand Chi-square tests (categorical vars) across stratified levels\\\"]\\n      de [label = \\\"Generate measures of association\\\\n(RD, RR, OR) of exposures and outcomes\\\"]\\n      \\n      df [label = \\\"Interpret results\\\"]\\n      \\n      # define edges\\n      \\n      lab3 -> {da, db, dc, dd, de, df}\\n      \\n      #da -> Workflow\\n      #db -> Explore\\n      #dc -> Explore\\n      #dd -> Model\\n      #de -> Model\\n      #df -> Communicate\\n      \\n      }\\n      \\n      {rank = same; lab2 -> lab3}\\n      \\n      \\n\\n      subgraph lab_4 {\\n    \\n      \\n      # adding node statements\\n      \\n      node [shape = circle]\\n      \\n      lab4 [label = \\\"Lab 4\\\", penwidth = 2.0]\\n      \\n            \\n      #Explore\\n      #Communicate\\n      #Model\\n\\n      node [shape = box]\\n      \\n      ea [label = \\\"Calculate measures of association (RD, RR) between\\\\nbirth order and child mortality by 5 years\\\\nwithin strata of other variables\\\"]\\n      eb [label = \\\"Test for confounding and effect measure modification\\\\ndue to stratifying variables\\\"]\\n      ec [label = \\\"Interpret results\\\"]\\n\\n      \\n      # define edges\\n      \\n      lab4 -> {ea, eb, ec}\\n      \\n      #ea -> Explore\\n      #eb -> Model\\n      #ec -> Communicate\\n      \\n      }\\n      \\n      {rank = same; lab3 -> lab4}\\n      \\n      }\\n    \",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}\r\n\r\n\r\n\r\n",
      "last_modified": "2021-08-31T12:51:25-04:00"
    },
    {
      "path": "mind_map_first.html",
      "title": "Mapping Our Steps",
      "description": "This page gives a birds-eye view of our analysis, where we've come from and where we're headed.\n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\n\r\n{\"x\":{\"diagram\":\"\\n\\n      digraph lab_0 {\\n      \\n      graph [rankdir = LR]\\n      \\n      # adding node statements\\n      \\n      node [shape = circle]\\n      \\n      lab0 [label = \\\"Lab 0\\\"]\\n      \\n      Workflow\\n      Explore\\n      Transform\\n      \\n      node [shape = box]\\n      \\n      a [label = \\\"Create workspace\\\"]\\n      b [label = \\\"Inspect dimensions\\\"]\\n      c [label = \\\"Generate summary statistics\\\"]\\n      d [label = \\\"Create derived variable for mothers age\\\"]\\n      e [label = \\\"Create categorical variable for mothers age\\\"]\\n      \\n      \\n      \\n      # add definition using node IDs\\n      \\n      lab0 -> {a, b, c, d, e}\\n      \\n      a -> Workflow\\n      b -> Explore\\n      c -> Explore\\n      d -> Transform\\n      e -> Transform\\n      \\n      }\\n\\n    \\n      \\n      \",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}\r\n\r\n\r\n{\"x\":{\"diagram\":\"\\n\\n      digraph lab_1 {\\n      \\n      graph [rankdir = LR]\\n      \\n      # adding node statements\\n      \\n      node [shape = circle]\\n      \\n      lab1 [label = \\\"Lab 1\\\"]\\n      \\n      Explore\\n      Transform\\n      Visualize\\n      Workflow\\n      Communicate\\n\\n      node [shape = box]\\n\\n      a [label = \\\"Construct derived variables\\\\nbord5, male, mweight, mbmi\\\"]\\n      b [label = \\\"Visualize with\\\\nhistograms and boxplots\\\"]\\n      c [label = \\\"Calculate frequency counts\\\\nand percentages\\\"]\\n      d [label = \\\"Create data dictionary\\\"]\\n      e [label = \\\"Consider implications of\\\\nmissing values\\\"]\\n      \\n      # edge definitions\\n      \\n      lab1 -> {a, b, c, d, e}\\n      \\n      a -> Transform\\n      b -> Visualize\\n      c -> Explore\\n      d -> Workflow\\n      e -> Communicate\\n      \\n      \\n      }\\n      \\n      \",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}\r\n\r\n\r\n{\"x\":{\"diagram\":\"\\n\\n      digraph lab_2 {\\n      \\n      graph [rankdir = LR]\\n      \\n      # adding node statements\\n      \\n      node [shape = circle]\\n      \\n      lab2 [label = \\\"Lab 2\\\"]\\n      \\n      Explore\\n      Transform\\n      Workflow\\n      Visualize\\n      Communicate\\n\\n      node [shape = box]\\n      \\n      a [label = \\\"Construct derived variables\\\\nsize, belowavg, pnc, rural\\\\neducation, death, and time\\\"]\\n      b [label = \\\"Update data dictionary\\\"]\\n      c [label = \\\"Exclude observations where child was alive\\\\nbut less than 60 months of age\\\"]\\n      d [label = \\\"Generate frequency histograms and\\\\nboxplo1ts for death and time\\\"]\\n      e [label = \\\"Calculate frequency counts\\\\nandpercentages\\\"]\\n      f [label = \\\"Consider implications of\\\\nanomalous and missing data\\\"]\\n      \\n      # define edges\\n      \\n      lab2 -> {a, b, c, d, e, f}\\n      \\n      a -> Transform\\n      b -> Workflow\\n      c -> Visualize\\n      d -> Explore\\n      e -> Explore\\n      f -> Communicate\\n      \\n      }\\n      \\n      \",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}\r\n\r\n\r\n{\"x\":{\"diagram\":\"\\n\\n      digraph lab_3 {\\n      \\n      graph [rankdir = LR]\\n      \\n      # adding node statements\\n      \\n      node [shape = circle]\\n      \\n      lab3 [label = \\\"Lab 3\\\"]\\n      \\n      Explore\\n      Workflow\\n      Communicate\\n      Model\\n\\n      node [shape = box]\\n      \\n      a [label = \\\"Define research question:\\\\nWhat are the factors that influence\\\\nthe outcome of child mortality by 5 years of age?\\\"]\\n      b [label = \\\"Generate statistics\\\\nof continuous variables\\\\nmbmi and mage, by mortality status\\\"]\\n      c [label = \\\"Generate stratified frequency counts and\\\\npercentages of categorical variables, by mortality status\\\"]\\n      d [label = \\\"Calculate T-tests (continuous vars)\\\\nand Chi-square tests (categorical vars) across stratified levels\\\"]\\n      e [label = \\\"Generate measures of association\\\\n(RD, RR, OR) of exposures and outcomes\\\"]\\n      \\n      f [label = \\\"Interpret results\\\"]\\n      \\n      # define edges\\n      \\n      lab3 -> {a, b, c, d, e, f}\\n      \\n      a -> Workflow\\n      b -> Explore\\n      c -> Explore\\n      d -> Model\\n      e -> Model\\n      f -> Communicate\\n      \\n      }\\n      \\n      \",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}\r\n\r\n\r\n{\"x\":{\"diagram\":\"\\n\\n      digraph lab_4 {\\n      \\n      graph [rankdir = LR]\\n      \\n      # adding node statements\\n      \\n      node [shape = circle]\\n      \\n      lab4 [label = \\\"Lab 4\\\"]\\n      \\n            \\n      Explore\\n      Communicate\\n      Model\\n\\n      node [shape = box]\\n      \\n      a [label = \\\"Calculate measures of association (RD, RR) between\\\\nbirth order and child mortality by 5 years\\\\nwithin strata of other variables\\\"]\\n      b [label = \\\"Test for confounding and effect measure modification\\\\ndue to stratifying variables\\\"]\\n      c [label = \\\"Interpret results\\\"]\\n\\n      \\n      # define edges\\n      \\n      lab4 -> {a, b, c}\\n      \\n      a -> Explore\\n      b -> Model\\n      c -> Communicate\\n      \\n      }\\n      \\n      \",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2021-08-31T12:51:26-04:00"
    },
    {
      "path": "submit.html",
      "title": "Lab Submission Guidelines",
      "description": "Submit your lab according to these guidelines to ensure you receive full points.\n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\nMaterials to submit\r\nNotes:\r\nSubmitting\r\nRounding\r\n\r\nGrading:\r\n\r\nSubmit your documents electronically via the SAKAI DropBox by 14:00 ET on the date given for each respective lab (SAKAI time-stamps DropBox submissions).\r\nMaterials to submit\r\nR Markdown (.Rmd) file with:\r\nyour name in the YAML header\r\nrequisite code under each task subheading\r\n\r\n.html file - must have run without error\r\nExcel (.xls/.xlsx) file with tables (labs 2, 3, and 4)\r\nNotes:\r\nSubmitting\r\nThe Rmarkdown file should run from start to finish with no errors, including calling in the analysis dataset, and should produce all of the results and graphs to complete this assignment.\r\nPlease name your files with the paradigm LabX_FirstinitialYourlastname.* (e.g. Lab1_LPark.Rmd), so that your lab instructor can identify each Rmarkdown-file.\r\nLate policy: 5 point deduction per 24 hour period past due date and time.\r\nRounding\r\nWhen completing tables, follow the rounding patterns specified below. This pattern will help in grading this assignment.\r\nThis should be obvious, but do not round intermediate values. Only round final answers for submission.\r\nIf rounding for a given table is not specified or not clear, ask your lab instructor for clarification.\r\nUnless specified within the task instructions, use 3 decimal places for risks and differences and 2 decimal places for rates and ratios.\r\nTo aid in our grading of these assignments, the last figure kept should be rounded UP when the first figure dropped is >= 5. For example, in rounding to 2 decimal places, 0.235 should be reported as 0.24 and 0.245 should be reported as 0.25. Note that this method will introduce a small bias to higher numbers, but another method, “rounding-to-even”, confuses many users.\r\nGrading:\r\n(TODO - change this grading guideline?) RMarkdown files will be graded on an all or nothing. If your markdown file runs from start to finish with no errors, you will receive full credit. If it hits an error message, you will receive NO credit (for the purposes of grading, if any “chunk” of code in your file contains special options, we will ask you to resubmit a file omitting them).\r\nR MARKDOWN FILE: Your markdown file MUST have the following components:\r\nan informative YAML header (see guide to Lab 0[TODO])\r\na set of commands calling in all relevant packages\r\na line of code calling in the analysis dataset\r\neach task’s code performed under the appropriate subheading\r\nif a task requires plotting with ggplot2, a line of code saving said plot using ggsave()\r\n\r\nTABLES: a certain number of points are allocated for each item. Errors include but are not limited to: incorrect values, omissions, and inappropriate rounding. For ease of grading, please do not alter the dimensions of any table. If you are confused on which values go where, please reach out to us for clarification.\r\nSHORT ANSWER QUESTIONS: will generally be graded as follows: 20% of allocated points for some sign of sentience; 40% of allocated points for getting at least part of the answer; 60% of allocated points for a pretty good answer; 80% of allocated points for an excellent, solid answer; 100% of allocated points for an out-of-the-ballpark answer.\r\n\r\n\r\n\r\n",
      "last_modified": "2021-08-31T12:51:27-04:00"
    }
  ],
  "collections": []
}

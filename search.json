{
  "articles": [
    {
      "path": "about.html",
      "title": "About",
      "description": "Reach out to your instructors for assistance. Learn more about them here.",
      "author": [],
      "contents": "\r\n\r\nContents\r\nLarry\r\nNour\r\nNathan\r\nThe Catscot\r\n\r\nLarry\r\n\r\n\r\n\r\nLarry has been growing out his hair.\r\nOH:\r\nNour\r\n\r\n\r\n\r\nNour, in cosplay as a butterfly. Pretty spot on, eh?\r\nOH: Thursday, 9am - 10am\r\nNathan\r\n\r\n\r\n\r\nNathan is a professional potato.\r\nOH: Tuesday, 11:30am - 12:30pm\r\nThe Catscot\r\n\r\n\r\n\r\nOur little mascot in the upper-left hand corner was Designed courtesy of 699pic at pngtree.com\r\n\r\n\r\n\r\n",
      "last_modified": "2021-11-16T11:08:22-05:00"
    },
    {
      "path": "broken.html",
      "title": "Broken Code",
      "author": [],
      "contents": "\r\nSome tips for code that won’t run\r\nDon’t hesitate to ask for help if you are having problems with software, code, etc. But be warned, your question should be specific and detailed, not just “Why won’t this line of code run?”\r\nIn my experience, unsent emails are sometimes better (and faster) than a TA. What looks like a hairy knot in your code is often quick to untangle when you start putting things in writing. This is why we ask that any email you send be both detailed and specific.\r\nHere are some other things that may help in getting your code to work:\r\nCheck that you haven’t mixed up == and =\r\nGo for a walk (seriously, maybe you’ve been sitting for too long)\r\nGoogle your error message – there’s almost always someone else who has encountered a similar problem\r\nDon’t delete and make changes to the same line of code. Copy/paste into a new chunk. Formulate a hypothesis. Make changes. Take notes on what changed. Revise your hypothesis. Repeat.\r\nLearn the R Debugger (The code for 705’s lab probably won’t need the debugger, but if you’re getting deep into things, ask Nathan about it)\r\nYou might also flip through this article on commmon problems in R Markdown, which might provide some answers.\r\n\r\n\r\n\r\n",
      "last_modified": "2021-11-16T10:59:47-05:00"
    },
    {
      "path": "data_dict.html",
      "title": "Data Dictionary",
      "description": "Data Dictionary for Kenya's 2008 Demographic and Health Survey.\n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\nData Dictionary Help\r\nKenya DHS Data Dictionary (file)\r\nCentury Month Code\r\n\r\n\r\n\r\n\r\n\r\nData Dictionary Help\r\nGuidelines for Data Collection & Data Entry - Theresa A Scott, MS (Vanderbilt University)\r\nKenya DHS Data Dictionary (file)\r\n\r\n\r\n{\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"caseid\",\"bord\",\"b3\",\"b4\",\"b5\",\"b7\",\"m2n\",\"m18\",\"m19\",\"v008\",\"v011\",\"v025\",\"v437\",\"v438\",\"v463z\",\"s109\"],[\"Used to uniquely identify each mother.\",\"Birth order\",\"Century month code for the date of birth of the child (see note below on century month codes).\",\"Sex of child.\",\"Whether child was alive or dead at the time of interview.\",\"Age at death of the child in completed months gives a calculated age at death from the reported information. This variable occupies three digits. BASE: Dead children (B5 = 0).\",\"Indicator of no prenatal care.\",\"Size of child as reported subjectively by the mother.\",\"Weight of child at birth given in kilograms with three implied decimal places. Divide by 1000 to produce weight in kilograms. Children who were not weighed are coded 9996\",\"Century month code of date of interview (see note on century month codes).\",\"Century month code of date of birth of the mother (see note on century month codes).\",\"Type of place of residence. \",\"Weight of the mother in kilograms at time of interview. There is one implied decimal place in the weight (decimal points are not included in the data file). To produce the weight in kilograms divide by 10.\",\"Height of the mother in centimeters at time of interview. There is one implied decimal place in the height (decimal points are not included in the data file). To produce the height in centimeters divide by 10.\",\"Indicator of no smoking.\",\"Education of the mother.\"],[\"Character\",\"Integer\",\"Integer\",\"1.   male; \\n2. female\",\"0.   dead; 1. alive\",\"Integer\",\"0.   some prenatal care; \\n1. no prenatal care;\\n9. missing\",\"1.   very large; \\n2. large; \\n3. average; \\n4. smaller than average; \\n5. very small; \\n8. don't know; \\n9. missing\",\"Integer (note implied decimal places)\",\"Integer\",\"Integer\",\"1.   urban; \\n2. rural\",\"Integer (note implied decimal places)\",\"Integer (note implied decimal places)\",\"0.   smokes something; \\n1. smokes nothing; \\n9. missing\",\"0.   did not attend school; \\n1. primary; \\n2. post-primary/vocational; \\n3. seoncdary / a-level; \\n4. college (middle level); \\n5. university\"]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th>Variable<\\/th>\\n      <th>Description<\\/th>\\n      <th>Coding<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"pagelength\":20,\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}\r\nCentury Month Code\r\nAll dates in the data file are expressed in terms of months and years and also as century month codes. A century month code (CMC) is the number of the month since the start of the century.\r\nFor example, January 1900 is CMC 1, January 1901 is CMC 13, January 1980 is CMC 961, September 1994 is CMC 1137.\r\nThe CMC for a date is calculated from the month and year as follows: CMC = (YY * 12) + MM for month MM in year 19YY.\r\nTo calculate the month and year from the CMC use the following formulae: YY = int((CMC - 1) / 12) MM = CMC - (YY * 12)\r\nFor Dates in 2000 and after the CMC is calculated as follows: CMC = ((YYYY-1900) * 12) + MM for month MM in year YYYY.\r\nTo calculate the month and year from the CMC use the following formulae: YYYY = int((CMC - 1) / 12)+1900 MM = CMC - ((YYYY-1900) * 12)\r\n\r\n\r\n\r\n",
      "last_modified": "2021-11-16T10:59:50-05:00"
    },
    {
      "path": "fun_dict.html",
      "title": "Function Dictionary",
      "description": "A searchable table of all relevant functions for 705 and 707 labs\n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\n\r\n\r\n\r\n\r\n{\"x\":{\"filter\":\"top\",\"vertical\":false,\"filterHTML\":\"<tr>\\n  <td data-type=\\\"character\\\" style=\\\"vertical-align: top;\\\">\\n    <div class=\\\"form-group has-feedback\\\" style=\\\"margin-bottom: auto;\\\">\\n      <input type=\\\"search\\\" placeholder=\\\"All\\\" class=\\\"form-control\\\" style=\\\"width: 100%;\\\"/>\\n      <span class=\\\"glyphicon glyphicon-remove-circle form-control-feedback\\\"><\\/span>\\n    <\\/div>\\n  <\\/td>\\n  <td data-type=\\\"character\\\" style=\\\"vertical-align: top;\\\">\\n    <div class=\\\"form-group has-feedback\\\" style=\\\"margin-bottom: auto;\\\">\\n      <input type=\\\"search\\\" placeholder=\\\"All\\\" class=\\\"form-control\\\" style=\\\"width: 100%;\\\"/>\\n      <span class=\\\"glyphicon glyphicon-remove-circle form-control-feedback\\\"><\\/span>\\n    <\\/div>\\n  <\\/td>\\n  <td data-type=\\\"character\\\" style=\\\"vertical-align: top;\\\">\\n    <div class=\\\"form-group has-feedback\\\" style=\\\"margin-bottom: auto;\\\">\\n      <input type=\\\"search\\\" placeholder=\\\"All\\\" class=\\\"form-control\\\" style=\\\"width: 100%;\\\"/>\\n      <span class=\\\"glyphicon glyphicon-remove-circle form-control-feedback\\\"><\\/span>\\n    <\\/div>\\n  <\\/td>\\n  <td data-type=\\\"character\\\" style=\\\"vertical-align: top;\\\">\\n    <div class=\\\"form-group has-feedback\\\" style=\\\"margin-bottom: auto;\\\">\\n      <input type=\\\"search\\\" placeholder=\\\"All\\\" class=\\\"form-control\\\" style=\\\"width: 100%;\\\"/>\\n      <span class=\\\"glyphicon glyphicon-remove-circle form-control-feedback\\\"><\\/span>\\n    <\\/div>\\n  <\\/td>\\n  <td data-type=\\\"character\\\" style=\\\"vertical-align: top;\\\">\\n    <div class=\\\"form-group has-feedback\\\" style=\\\"margin-bottom: auto;\\\">\\n      <input type=\\\"search\\\" placeholder=\\\"All\\\" class=\\\"form-control\\\" style=\\\"width: 100%;\\\"/>\\n      <span class=\\\"glyphicon glyphicon-remove-circle form-control-feedback\\\"><\\/span>\\n    <\\/div>\\n  <\\/td>\\n<\\/tr>\",\"data\":[[\"Import\",\"Import\",\"Import\",\"Operator\",\"Explore\",\"Explore\",\"Explore\",\"Explore\",\"Explore\",\"Explore\",\"Explore\",\"Explore\",\"Explore\",\"Stats\",\"Stats\",\"Stats\",\"Stats\",\"Operator\",\"Transform\",\"Transform\",\"Transform\",\"Transform\",\"Conditional\",\"Conditional\",\"Transform\",\"Tabular analysis\",\"Transform\",\"Plot\",\"Plot\",\"Plot\",\"Plot\",\"Export\",\"Export\",\"Explore\",\"Stats\",\"Explore\",\"Transform\",\"Transform\",\"Plot\",\"Transform\",\"Explore\",\"Stats\",\"Stats\",\"Stats\",\"Stats\",\"Tabular analysis\",\"Measures of Association\",\"Measures of Association\",\"Measures of Association\",\"Measures of Association\",\"Measures of Association\",\"Measures of Association\"],[\"install.packages()\",\"library()\",\"readRDS()\",\"&lt;-\",\"head()\",\"length()\",\"ncol()\",\"nrow()\",\"dim()\",\"names()\",\"summary()\",\"skim()\",\"summarize()\",\"mean()\",\"median()\",\"sd()\",\"IQR()\",\"%&gt;%\",\"sum()\",\"mutate()\",\"round()\",\"as.integer()\",\"case_when()\",\"ifelse()\",\"factor()\",\"table()\",\"proportions()\",\"ggplot()\",\"geom_histogram()\",\"geom_boxplot()\",\"labs()\",\"ggsave()\",\"saveRDS()\",\"class()\",\"range()\",\"is.na()\",\"filter()\",\"select()\",\"facet_wrap()\",\"group_by()\",\"count()\",\"t.test()\",\"kruskal.test()\",\"chisq.test()\",\"fisher.test()\",\"CreateTableOne()\",\"flipTable()\",\"mAssoc()\",\"riskdifference()\",\"riskratio()\",\"epi.2by2()\",\"epiHomog()\"],[\"package = \\\" \\\"\",\"package\",\"file = \\\" \\\"\",null,\"object\",\"object\",\"x\",\"x\",\"x\",\"x\",\"object\",\"data frame\",\"piped data (implicit), list of summary statistics\",\"object\",\"object\",\"object\",\"object\",null,\"object\",\"piped data (implicit), name-value pairs\\nex: mutate(newColumnName = oldColumn + 1)\",\"x, digits\",\"x\",\"conditional statement ~ result, …\",\"condition, result if true, result if false\",\"column of data frame\",\"column(s) of a data frame\",\"table object, margin\",\"data, aes(x = , y = )\",\"aes(x = ), binwidth = \",\"aes(x = )\",\"title, x, y, etc.\",\"filename = \\\" \\\"\",\"object, file = \\\" \\\"\",\"object\",\"Any numeric or character objects\",\"x\",\"piped data (implicit), conditional expression\",\"piped data (implicit), column names separated by commas\",\". ~ variable\",\"piped data (implicit), column names\",\"piped data (implicit)\",\"x, y (optional)\",\"x, y (optional)\",\"x, y (optional)\",\"x, y (optional)\",\"vars, strata (optional), data\",\"table object\",\"table object with exposure-outcome in top-left\",\"a = outcome+, b = outcome-, N1 = exposed+, N0 = exposed-\",\"a = outcome+, b = outcome-, N1 = exposed+, N0 = exposed-\",\"table object with exposure-outcome in top-left\",\"table object with exposure-outcome in top-left\"],[\"The name of the package you'd like to install, in quotes\",\"Loads a package and makes it available for use in R coding environment\",\"Give a file path that points to the .rds dataset you would like to load\",\"Used to assign a name to R output, thereby saving it to your R environment for later use\",\"Returns the top 6 items from an R object, like a data frame. Specify the exact number with n = \",\"Returns the length of an R object\",\"Returns the number of columns present in x\",\"Returns the number of rows present in x\",\"Returns the dimensions of an object.\",\"Returns the names of an object (e.g. the column names of a dataset)\",\"Provides summary data of an object. For data frames, these are summary stats like mean, median, etc.\",\"Alternative to summary(). Provides a broad overview of a data frame.\",\"Can be used in combination with group_by(). Will output summary statistic for each group.\",\"Returns the mean for numeric/logical vectors. \",\"Returns the median for numeric vectors.\",\"Returns the standard deviation for numeric vectors.\",\"Returns the interquartile range for numeric vectors.\",\"The R equivalent of \\\"and then\\\". Relays a data frame from one function to the next.\",\"Returns the sum of all values present in its arguments/object\",\"Adds a new variable (name) to a dataframe according to a value, hence \\\"name-value pair\\\". \",\"Rounds the values in x to the specified number of decimal places in \\\"digits = \\\"\",\"Takes floating point numbers (those with decimals) and converts them to integers\",\"See vignette. Used within mutate() to evaluate columns on conditional (if/else) statements, and returns a result for each row.\",\"Takes three arguments: a conditional statement, the result if it's true, and the result if it's false. Can be used with mutate()\",\"Converts numberic and character variables to \\\"factor\\\" categorical variables\",\"Provides one-way, two-way, and multi-way tabulation of variables from a dataset\",\"Returns conditional proportions. margin = 1 gives row proportions. margin = 2 gives column proportions.\",\"Initializes a ggplot object.\",\"Visualize the distribution of a single continuous variable by dividing x-axis into bins. Set width of bins with binwidth = \",\"Displays boxplot of a continuous variable. Stratify boxplots by specifying a y aesthetic (\\\"aes(x = , y = )\\\"\",\"Modify title, axis, and other plot labels\",\"Takes the most recent graphic run using ggplot and saves it to a file path given to filename = . Remember to use quotes around the file path and provide the file extension (.png, .jpg, .eps, etc.)\",\"Takes a data object and saves it to the file path designated by file = \\\" \\\". Don't forget to include file extension .rds\",\"Tells you the object's type. For an entire dataframe, this means the data type in each column. \",\"Returns the minimum and maximum of all the given arguments (A-Z, lowest-highest)\",\"Used to check values as missing. Commonly used to filter() based on missing values in a given column (eg. filter(is.na(variable)) )\",\"Used to subset a data frame, where it keeps all rows that satisfies the conditional expression (returns TRUE)\",\"Used to select specific columns in a dataset. Use -variable to drop a variable from a data frame and keep the rest.\",\"Used to stratify a visualization by an extra variable.\",\"Groups a data frame by sub-groups in specified column names, at which point you can perform operations within those subgroups using summarize(n(), mean(), median(), etc.)\",\"When used along with group_by(), returns subgroup frequency counts.\",\"Takes x, a numeric vector of data values (a list of numbers, like a column in a data frame), and performs one and two-way t-tests on those vectors.\",\"Takes x, a numeric vector of data values (a list of numbers, like a column in a data frame), and performs a Kruskal-Wallis rank sum test on those vectors.\",\"Takes x, a numeric vector of data values (a list of numbers, like a column in a data frame), and performs chi-squared contingency table tests and goodness-of-fit tests\",\"Takes x, a numeric vector of data values (a list of numbers, like a column in a data frame), and performs Fisher's exact test for testing the null of independence of rows and columns in a contingency table.\",\"Takes a vector of variables, vars, from a dataset and generates summary statistics for them. If strata is given, stratifies by strata.\",\"Takes a table object generated by table() and diagonally inverts the position of its rows and columns so that the top-right is now the bottom-left, etc.\",\"Returns Measures of assocation. For 2x2 tables, returns RD, RR and OR according to referent variable. For stratified 2x2 tables, returns pooled and crude measures of association.\",\"Takes +/- outcomes by exposure and calculates risk difference and confidence intervals\",\"Takes +/- outcomes by exposure and calculates risk ratio and confidence intervals\",\"Computes summary measures of risk. When exposure variable has more than one level, use mAssoc() instead.\",\"Takes a stratified table and returns the result of a chi-square test of homogeneity on the various strata's risk differences\"],[\"utils\",\"base R\",\"base R\",\"base R\",\"utils\",\"base R\",\"base R\",\"base R\",\"base R\",\"base R\",\"base R\",\"skimr\",\"dplyr\",\"base R\",\"stats\",\"stats\",\"stats\",\"dplyr\",\"base R\",\"dplyr\",\"base R\",\"base R\",\"dplyr\",\"base R\",\"base R\",\"base R\",\"base R\",\"ggplot2\",\"ggplot2\",\"ggplot2\",\"ggplot2\",\"ggplot2\",\"base R\",\"base R\",\"base R\",\"base R\",\"dplyr\",\"dplyr\",\"ggplot2\",\"dplyr\",\"dplyr\",\"stats\",\"stats\",\"stats\",\"stats\",\"tableone\",\"epiAssist\",\"epiAssist\",\"fmsb\",\"fmsb\",\"epiR\",\"epiAssist\"]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th>Task<\\/th>\\n      <th>Function<\\/th>\\n      <th>Arguments<\\/th>\\n      <th>Description<\\/th>\\n      <th>Package<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"pagelength\":20,\"order\":[],\"autoWidth\":false,\"orderClasses\":false,\"orderCellsTop\":true}},\"evals\":[],\"jsHooks\":[]}\r\n\r\n\r\n\r\n",
      "last_modified": "2021-11-16T11:00:36-05:00"
    },
    {
      "path": "help_ggplot2.html",
      "title": "Visualizations with {ggplot2}",
      "description": "Histograms, boxplots, and pointalism using {ggplot2}.\n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\nGoals:\r\nExternal resources\r\nFor the visually inclined: RFun, Visualization with ggplot2 with John Little\r\nFor the linguistically inclined:\r\nFor the theoretically inclined:\r\nFor the obsessively inclined:\r\nFor the listically inclined:\r\n\r\nCore competencies for 705 lab\r\nCreate a new ggplot with ggplot()\r\nAdd a dataset to the plot\r\nAssign x and y aesthetics with aes():\r\nCreate a histogram with geom_histogram()\r\nCreate a boxplot with geom_boxplot()\r\nGenerate a stratified boxplot by adding a y aesthetic:\r\nCreate titles and axis labels with labs()\r\nBONUS: Generating multiple stratified plots with facet_wrap() or filter()\r\n\r\n\r\nGoals:\r\nBy following along with this document, you will know how to:\r\nRead and interpret basic code in {ggplot2}’s grammar of graphics\r\nInitialize a plotting field using ggplot()\r\nMap x and y aesthetics to a plot’s axes using aes()\r\nGenerate histograms with geom_histogram()\r\nGenerate boxplots and stratified boxplots with geom_boxplot()\r\nCreate titles and axis labels with labs()\r\nSave a plot with ggsave()\r\nBasically, this help document will provide you with the tools necessary to complete the labs for GLHLTH 705.\r\nExternal resources\r\nIf you’re interested in going one level deeper, we highly recommend you check out the following resources, which will give a better introduction to {ggplot2} than we ever could (also why we’re plugging these at the top).\r\nAs R rule of thumb, it’s good to have multiple mediums of exposure to the same idea. We recommend you pick the one that suits your learning style and come back for more later:\r\nFor the visually inclined: RFun, Visualization with ggplot2 with John Little\r\nJohn Little is nothing short of the world’s best librarian.\r\nFor the linguistically inclined:\r\nR for Data Science, Chapter 3, by Hadley Wickham\r\nAfter a preface and introduction, this is the first actual chapter in R4DS. The rationale is that {ggplot2} is actually pretty fun and satisfying to use. It’s pretty well guaranteed to have you hooked if you give it a chance.\r\nFor the theoretically inclined:\r\n“A Layered Grammar of Graphics” by Hadley Wickham\r\nPublished in the Journal of Computational and Graphical Statistics, 2010\r\nFor the obsessively inclined:\r\nggplot2: Elegant Graphics for Data Analysis, by Hadely Wickham\r\n**cough** Also by Hadley Wickham. (look it up using the Duke Library search engine and log in with your Duke credentials\r\nFor the listically inclined:\r\nReference page of ggplot2 commands\r\nCore competencies for 705 lab\r\nFor those of you who made it through that onslaught of links without clicking on a single one, welcome to the Core Competencies section. We hope that this section is somehow dry enough that you go and find your answers in one of the resources above. But for those of you who are still feeling stubborn:\r\nCreate a new ggplot with ggplot()\r\nTo initialize a plotting space, we first need to tell R that we want to use ggplot. If we just call ggplot() and run it without any data, we get a blank field. This is our sandbox:\r\n\r\n\r\nggplot()\r\n\r\n\r\n\r\n\r\nAdd a dataset to the plot\r\nFor this example, we’ll use the dataset available in base R called iris, which provides measurements and species data on a bunch of – you guessed it – irises.\r\n\r\n\r\n\r\n\r\nSepal.Length\r\nSepal.Width\r\nPetal.Length\r\nPetal.Width\r\nSpecies\r\n5.1\r\n3.5\r\n1.4\r\n0.2\r\nsetosa\r\n4.9\r\n3.0\r\n1.4\r\n0.2\r\nsetosa\r\n4.7\r\n3.2\r\n1.3\r\n0.2\r\nsetosa\r\n4.6\r\n3.1\r\n1.5\r\n0.2\r\nsetosa\r\n5.0\r\n3.6\r\n1.4\r\n0.2\r\nsetosa\r\n5.4\r\n3.9\r\n1.7\r\n0.4\r\nsetosa\r\n\r\n\r\n\r\n\r\nFigure 1: A bearded iris\r\n\r\n\r\n\r\nWe can add the dataframe to our plot by including it in the argument data =.\r\nSuperficially, this doesn’t change our output:\r\n\r\n\r\nggplot(data = iris)\r\n\r\n\r\n\r\n\r\nBut the data frame is now a part of the plot. One way to verify this is by assigning the two previous plots a name and inspecting their size with object.size(). The plot with the data should be bigger:\r\n\r\n\r\nwithout_data <- ggplot()\r\n\r\nobject.size(without_data)\r\n#> 3720 bytes\r\n\r\nwith_data <- ggplot(data = iris)\r\n\r\nobject.size(with_data)\r\n#> 10704 bytes\r\n\r\n\r\n\r\nAssign x and y aesthetics with aes():\r\nNext, we need to tell ggplot which variables we’re working with, and where to put them (their “aesthetic mapping”). We do this using the argument aes(x = variable1, y = variable2)\r\nIf we’re creating histograms and singular boxplots, we only require a single variable on the x-axis. We can initialize it as follows:\r\n\r\n\r\nggplot(data = iris, mapping = aes(x = Sepal.Length))\r\n\r\n\r\n\r\n\r\nSee how ggplot assigned Sepal.Length to the x axis?\r\nCreate a histogram with geom_histogram()\r\nOkay, let’s cut to the chase. We want a plot.\r\n{ggplot2} has a large number of plotting types and styles. Given the types of variables we’ve mapped to ggplot’s aesthetics, all we need to do is choose a type of plot appropriate for that type of variable, and add it as a new layer with +\r\n\r\n\r\nggplot(data = iris, mapping = aes(x = Sepal.Length)) +\r\n  geom_histogram()\r\n\r\n\r\n\r\n\r\nMany plots also allow us to add color with the argument aes(fill = \"colorname\") (colors are always written as strings, in quotes!).\r\nWe may also change the size of our bins with argument binwidth = x:\r\n\r\n\r\nggplot(data = iris, mapping = aes(x = Sepal.Length)) +\r\n  geom_histogram(fill = \"#12BBAC\", binwidth = .25)\r\n\r\n\r\n\r\n\r\n\r\nRefer here for a list of color options available in R. Along with default color names, R also accepts hexadecimal color codings. For fun, I’ve made our histogram the same color as this website’s navbar. Did it work?\r\nCreate a boxplot with geom_boxplot()\r\nA single boxplot functions in the same exact manner. Instead of geom_histogram(), we add a boxplot layer with geom_boxplot(). This time, I’ve used a default color name, goldenrod2, instead of a hexadecimal color code:\r\n\r\n\r\nggplot(data = iris, mapping = aes(x = Sepal.Length)) +\r\n  geom_boxplot(fill = 'goldenrod2')\r\n\r\n\r\n\r\n\r\nGenerate a stratified boxplot by adding a y aesthetic:\r\nWe can create multiple boxplots within a single plot by adding a categorical variable as a second aesthetic. The iris dataset contains a categorical variable, Species, which would be appropriate for this task:\r\n\r\n\r\nggplot(data = iris, mapping = aes(x = Sepal.Length, y = Species)) +\r\n  geom_boxplot(fill = '#AC12BB')\r\n\r\n\r\n\r\n\r\nWe can also display the boxplots vertically by assigning Sepal.Length to y = and Species to x = :\r\n\r\n\r\nggplot(data = iris, mapping = aes(y = Sepal.Length, x = Species)) +\r\n  geom_boxplot(fill = '#AC12BB')\r\n\r\n\r\n\r\n\r\nCreate titles and axis labels with labs()\r\nFinally, we need to make our plots fit for public use… it needs axis labels and a title. We can specify these by adding another layer to our plot, labs(). Make sure you write your labels as strings:\r\n\r\n\r\nggplot(data = iris, mapping = aes(y = Sepal.Length, x = Species)) +\r\n  geom_boxplot(fill =  \"#12BBAC\") + \r\n  labs(x = \"Species\", \r\n       y = \"Sepal Length\", \r\n       title = \"Boxplots of Sepal Length of Irises by Species\")\r\n\r\n\r\n\r\n\r\nBONUS: Generating multiple stratified plots with facet_wrap() or filter()\r\nYou might be wondering what this sort of stratification might look life if we tried the same thing with a histogram. Can a histogram accept a y aesthetic? When we try and assign a second aesthetic to a histogram, we get the following result:\r\n\r\n\r\nggplot(data = iris, mapping = aes(x = Species, y = Sepal.Length)) +\r\n  geom_histogram(fill = \"goldenrod2\")\r\n\r\n#> Error: stat_bin() can only have an x or y aesthetic.\r\n\r\n\r\n\r\nfacet_wrap()\r\nAn easy way to generate stratified histograms is with the additional layer, facet_wrap(), which takes a formula in the following syntax:\r\n. ~ stratifyingVariable\r\nThe period here represents our ggplot object. We put the stratifying variable on the right side of the formula as a way to designate it as the “independent variable” of sorts. The output, . , depends on whatever categorical we assign as our faceting variable. In this case, the histograms dependson the variable Species:\r\n\r\n\r\nggplot(data = iris, mapping = aes(x = Sepal.Length)) + \r\n  geom_histogram(fill = \"goldenrod2\") +\r\n  facet_wrap(. ~ Species) + \r\n    labs(x = \"Sepal Length\", \r\n         y = \"Count\",\r\n         title = \"Histograms of Sepal Length of Irises by Species\")\r\n\r\n\r\n\r\n\r\nWe might decide that we want the plots stacked vertically instead of horizontally to help us better compare their distributions by Sepal Length. We can do that too, with the argument nrow =:\r\n\r\n\r\nggplot(data = iris, mapping = aes(x = Sepal.Length)) + \r\n  geom_histogram(fill = \"#AC12BB\") +\r\n  facet_wrap(. ~ Species, nrow = 3) +\r\n  labs(x = \"Sepal Length\",\r\n       y = \"Count\",\r\n       title = \"Histograms of Sepal Length of Irises by Species\")\r\n\r\n\r\n\r\n\r\npipe and filter()\r\nWhat if we only want the Sepal Lengths for the species Iris virginica?\r\nOne way would be to use filter(), which we connect to our ggplot using a pipe:\r\n\r\n\r\niris %>%\r\n  filter(Species == \"virginica\") %>%\r\n  ggplot(mapping = aes(x = Sepal.Length)) +\r\n  geom_histogram(fill = \"#12BBAC\") + \r\n  labs(x = \"Sepal Length\", \r\n       title = \"Histograms of Sepal Length of Irises by Species\")\r\n\r\n\r\n\r\n\r\n\r\nNote that when we use a pipe for a ggplot, we’ve already specified the dataset at the top of the code, so we don’t need to type it again when we call ggplot(). It already knows what data we’re working with because iris is funneled along the pipeline!\r\n\r\n\r\n\r\n",
      "last_modified": "2021-11-16T11:00:41-05:00"
    },
    {
      "path": "help_jargon.html",
      "title": "R Jargon",
      "description": "How to talk like an R nerd. Here, we detail various R-related vocabulary, the familiarity of which will make your life a lot easier.  \n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\nR Vocabulary List\r\n$\r\n#\r\nargument\r\nassignment operator\r\nbase R\r\ncode chunk\r\nconsole\r\ndebugging\r\ndirectory\r\ndplyr\r\nfactor\r\nfunction\r\nggplot2\r\ngit\r\nHTML\r\nindex\r\nknit\r\nlibrary/packages\r\nmagrittr/pipe/%>%\r\nparameter\r\nreproducible expressions (reprex)\r\nstring/character\r\ntibble\r\ntidy data\r\ntidyverse\r\ntraceback\r\ntribble\r\nvector\r\nvignette\r\nWickham, Hadley\r\nYAML header\r\n\r\n\r\nDoes a pesky word keep popping up that’s given you the sneaking suspicion that you have no idea what you’re doing?\r\nI’ve got good news, but also some bad news.\r\nThe bad: The person writing this probably has no idea what they’re doing either. And unless you study computer engineering, there’s a good chance you will never really “know” what you are “doing”.\r\nThe good: A quick vocabulary lesson will help do away with that sneaking suspicion, at least temporarily.\r\nR Vocabulary List\r\nSearch this document using ctrl+f for a brief definition of that word or symbol and external links for further reading. Can’t find the word here? Try R Documentation or Google.\r\n$\r\nUsed to specify a specific variable within a list-like object (like a data.frame, a tibble, a model, an actual list created with list())\r\nExample:\r\n\r\n\r\n# this code uses head() to view the first six observations for the variable \"cyl\" in dataset \"mtcars\"\r\n\r\nhead(mtcars$cyl)\r\n#> [1] 6 6 4 6 8 6\r\n\r\n\r\n\r\n#\r\nIn a chunk of code, # is used to comment out text. This tells the R console to ignore that line. This is useful for making comments in your code.\r\nOutside of code chunks, a series of hashes on a new line, followed by a space and some text, indicate different levels for document subheadings, like this:\r\nIn R Markdown, type:\r\n# one hash\r\n## two hashes\r\n### three hashes\r\n#### four hashes\r\n##### five hashes\r\nKnitting in HTML renders as:\r\n one hash \r\n two hashes \r\n three hashes \r\n four hashes \r\n five hashes \r\nargument\r\nThe various pieces of data necessary for a function to run. Many functions have arguments with default values.\r\nFor example, for many statistical tests of significance, the significance level is set to a default of 0.95. If you don’t include this argument in your function, it will refer to the default and use it.\r\nIn a function’s documentation, under “Usage”, you can tell when an argument has a default because the argument’s name is equated to a value.\r\nAs another example, the function head() takes two main arguments, an object, x, and n, the number of observations you’d like for head() to print.\r\nGo to the help documentation for head() (?head) and find what its default is. Or maybe you already have noticed its default when you’ve run head() in your labs.\r\nassignment operator\r\nThe <- is called the Assignment Operator. We can use it to assign names to objects in our coding environment:\r\nWe can use our assignment operator for characters, numbers, logical operators, etc.:\r\n\r\n\r\nfruit <- c(\"oranges\", \"papayas\", \"apricots\")\r\n\r\nnumber <- 99\r\n\r\nlogical <- FALSE\r\n\r\n\r\n\r\nNow that the above values are stored in our environment, we can use them in other functions or operations:\r\n\r\n\r\npaste(fruit, \"are orange\", sep = \" \")\r\n#> [1] \"oranges are orange\"  \"papayas are orange\"  \"apricots are orange\"\r\n\r\nnumber + 1\r\n#> [1] 100\r\n\r\nisTRUE(logical)\r\n#> [1] FALSE\r\n\r\n\r\n\r\nbase R\r\nThese are functions that are a part of the original R programming language, and so do not require a call to a package using library().\r\nGo here to see a complete list of functions that come with the R Base Package\r\ncode chunk\r\nIn R Markdown, code chunks look like this:\r\n```{r}\r\n```\r\nAnything written between these two lines can be sent to the R Console and run as code. Anything not bound within these lines is interpreted as text, and is printed as-is in a rendered document.\r\nconsole\r\nIn lieu of running code in your R Markdown document, you can type it directly into the window that says “Console”.\r\nWhen you click “Run” on any R Markdown code, that code gets run in the Console.\r\ndebugging\r\nThe process of identifying and fixing problems in your code. A debugger is a program that walks through your code, line-by-line, allowing you to inspect elements within the environment as the code runs.\r\nThis becomes more useful when you’re writing your own functions and are confused as to why they’re behaving a certain way.\r\ndirectory\r\n“The working directory of a process is a directory of a hierarchical file system”\r\nA directory is any file folder on your computer.\r\nYour root directory is the top-most directory on your computer (in Windows, this is the folder calls “C:”, for Mac users, it’s usually labelled as “Macintosh HD”)\r\nIn R, your working directory is typically the folder that contains whatever R Project you have open.\r\nSay you have a dataset called “data.rds” in your main working directory. You can import it using any number of functions by referring to its file path as simply “data.rds”.\r\nBut say you have that dataset in a series of folders within your working directory. The folders are organized like this, which each subsequent folder inside the last, and the data file in the folder titled “data”:\r\nworking directory > try1 > fullAnalysis > data\r\nThe “file path” for referring to your data file from your working directory would be this:\r\n“try1/fullAnalysis/data/data.rds”\r\ndplyr\r\nA package that contains a set of functions that help solve “the most common data manipulation challenges”. Main functions include:\r\nmutate()\r\nselect()\r\nfilter()\r\nsummarize() (or summarise())\r\narrange()\r\nHere’s the chapter from R for Data Science\r\nYou may also find this vignette helpful\r\nfactor\r\nA data type that is the preferred way to store categorical variables in R.\r\nUsing factor(), you can convert:\r\na character to a factor: This takes all unique values of a character variable and assigns them an underlying number, or “levels”.\r\nFor example:\r\n\r\n\r\ndumplings <- c(\"momos\", \"pop tarts\", \"raviolis\", \"momos\", \"empanadas\", \"pierogis\")\r\n\r\nfactor(dumplings)\r\n#> [1] momos     pop tarts raviolis  momos     empanadas pierogis \r\n#> Levels: empanadas momos pierogis pop tarts raviolis\r\n\r\n\r\n\r\nan integer to a factor: This takes integers and makes the lowest number the base level, and the next highest 1, the next highest 2, etc. You can assign a “label” to each respective level with “labels = c()”, with a vector, c(), with the same length as the number of levels.\r\n\r\n\r\nintegers <- c(1, 2, 3, 2, 3, 2, 1)\r\n\r\nfactor(integers,\r\n       labels = c(\"chicken\", \"egg\", \"rooster\"))\r\n#> [1] chicken egg     rooster egg     rooster egg     chicken\r\n#> Levels: chicken egg rooster\r\n\r\n\r\n\r\nUnderstanding factors mostly takes time. If you want to speed that up, here’s the R for Data Science chapter on factors\r\nIn it, they reference a few articles for further reading:\r\nWrangling categorical data in R\r\nstringsAsFActors: An unauthorized biography\r\nstringsAsFactors =  – this one is kinda funny\r\nfunction\r\nA “self-contained” piece of code that takes a predefined type of input data (arguments), operates on it, and returns an output or result.\r\nggplot2\r\nThe “gg” in ggplot2 stands for “grammar of graphics”.\r\nTo get a high-level overview of ggplot2 basics, I highly recommend this introduction to data visualization.\r\nIf you’re really trying to nerd out, you can access the free full text of ggplot2: Elegant Graphics for Data Analysis by Hadley Wickham by looking it up using the Duke Library search engine and logging in with your Duke credentials\r\ngit\r\nYou may have heard of GitHub, a popular website for version control, collaboration, and code sharing.\r\nGit is the underlying software that GitHub runs on. It’s free and open source. You won’t be expected to use a Git repository for your projects in this class, but it’s nice to know what’s out there.\r\nThis massive book is available for those who would like to learn more. I think the first and second sections, “Getting Started” and “Git Basics”, are good places to start.\r\nHTML\r\nStands for HyperText Markup Language. It’s the standard language used for documents that are meant to be displayed in a web browser, and is highly customizable. When R Markdown renders to HTML, it does almost all of the heavy lifting for you.\r\nindex\r\nNot particularly important for Fall semester, but is an important concept to understand when working with data.\r\nIn programming, an index is a numerical representation of an item’s position in a sequence.\r\nIn R, indexes start at number 1 (as opposed to other languages that start at 0).\r\nYou can refer to an item’s index with [ ].\r\nLETTERS gives us a character vector of every letter of the alphabet\r\n\r\n\r\nLETTERS\r\n#> [1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\" \"G\" \"H\" \"I\" \"J\" \"K\" \"L\" \"M\" \"N\" \"O\"\r\n#> [16] \"P\" \"Q\" \"R\" \"S\" \"T\" \"U\" \"V\" \"W\" \"X\" \"Y\" \"Z\"\r\n\r\n\r\n\r\nWe can refer to individual letters by calling their index:\r\n\r\n\r\nLETTERS[1]\r\n#> [1] \"A\"\r\n\r\nLETTERS[5]\r\n#> [1] \"E\"\r\n\r\nLETTERS[26]\r\n#> [1] \"Z\"\r\n\r\n\r\n\r\nData Frames and Tibbles can be indexed using syntax [row, column]. So data[1,1] would call the value in the first row of the first column, data[2,1] would call the value in the second row of the first column, and so on.\r\nknit\r\nThe button at the top of your R Markdown document that instructs the document to render as its designated output. The standard outputs for R Markdown are HTML, Word, and PDF. But there are an ever expanding set of R Markdown outputs available to R users. This entire website was created using a document output type called “Distill”\r\nlibrary/packages\r\nI’ll quote from an answer in StackOverflow for this one:\r\n“In R, a package is a collection of R functions, data and compiled code. The location where the packages are stored is called the library.”\r\nmagrittr/pipe/%>%\r\nChapter from R for Data Science gives a nice intro.\r\nFor you nerds, here’s a history of the pipe operator in R\r\nparameter\r\nUsed interchangeably with “argument”\r\nreproducible expressions (reprex)\r\nWhen you encounter a problem in your code that you just can’t figure out, it’s often best to create a reproducible expression. This allows whoever is helping you to recreate your problem in their own R console.\r\nCreating a “reprex” often entails trimming your code to the bare essentials and isolating whatever step in your code is causing it to hit an error.\r\nIt might also be the case that you need to create toy data. To do this, you can use the following tools:\r\ntibble() to build a dataset from vectors of equal length\r\nrunif() to create a vector of n length of random floating point values\r\nseq() to create a vector of n length in a defined sequence\r\nsample() to draw n random samples, with replace = TRUE/FALSE, from a pre-existing vector\r\nx:y – use a colon to generate a series of integers from x to y\r\nc() with comma separated values to designate a vector\r\nstring/character\r\nStrings are created with either single quotes or double quotes. It indicates that a value is meant to be read as-is, rather than transformed according to R’s computational rules for numbers and logical values.\r\nFor example, writing the logical value, TRUE as a string makes it unrecognizable to R as logical:\r\n\r\n\r\na <- TRUE\r\nisTRUE(a)\r\n#> [1] TRUE\r\n\r\nb <- \"TRUE\"\r\nisTRUE(b)\r\n#> [1] FALSE\r\n\r\n\r\n\r\nAs simple as it sounds, strings are a topic of mind-numbing complexity, as the underlying encoding of text strings governs the way that code is able to interact with data as well as other code.\r\nR for Data Science gives a nice introduction to the mechanics of strings here, but hints at their wider implications in its chapter on data importation.\r\nRegular expressions (not to be confused with reproducible expressions), or regex, are their own beast, and may help you understand how databases and search engines work\r\ntibble\r\nLike a data.frame object, but with enhanced “printing”.\r\nRead this section on tibbles from R for Data Science to learn more.\r\ntidy data\r\n“Tidy datasets are all alike, but every messy dataset is messy in its own way.” - Hadley Wickham\r\nThe first few sections in this chapter from R for Data Science gives a nice introduction.\r\nThe basic ideas behind tidy data are defined by these three rules:\r\nEach variable must have its own column\r\nEach observation must have its own row\r\nEach value must have its own cell\r\nYou can read more about the underlying theory in this article that was published in the Journal of Statistical Software.\r\ntidyverse\r\n“The tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures.”\r\nFamiliar packages include:\r\ndplyr\r\nggplot2\r\ntibble\r\ntraceback\r\nIf you hit an error, use traceback() to print a summary of how your program/code arrived at that error. In simple terms, it’s tracing your steps prior to your code hitting an error.\r\ntribble\r\nShort for “transposed tibble”, it’s a function that allows you to create a tibble by hand, with the syntax and subsequent output:\r\n\r\n\r\ntribble(\r\n  ~colA, ~colB, ~colC, ~colD,\r\n  \"a\",   1, \"Square\", \"orange\",\r\n  \"b\",   2, \"Circle\", \"maracuya\",\r\n  \"c\",   3, \"Rhombus\", \"cashew\"\r\n)\r\n\r\n\r\n\r\n\r\ncolA\r\ncolB\r\ncolC\r\ncolD\r\na\r\n1\r\nSquare\r\norange\r\nb\r\n2\r\nCircle\r\nmaracuya\r\nc\r\n3\r\nRhombus\r\ncashew\r\n\r\n?tribble in the R Console for more details\r\nvector\r\nA vector is a list of values, all of the same type.\r\nWe use c() to create a vector, separating items with commas when we specify them individually.\r\nThe following are all valid vectors:\r\n\r\n\r\n# a vector of numbers 1, 2, 3:\r\nc(1, 2, 3)\r\n#> [1] 1 2 3\r\n\r\n# a vector of numbers 1 through 12, and then 20:\r\nc(1:12, 20)\r\n#> [1]  1  2  3  4  5  6  7  8  9 10 11 12 20\r\n\r\n# a vector of logical values:\r\nc(TRUE, FALSE, NA, TRUE)\r\n#> [1] \"TRUE\"  \"FALSE\" \"TRUE\" \r\n\r\n# a vector of 5 values randomly drawn from a uniform distribution with min 0 and max 1:\r\nrunif(5)\r\n#> [1] 0.97420369 0.04387835 0.68818071 0.13420150 0.30322080\r\n\r\n\r\n\r\nMost operations on vectors apply to each value individually:\r\n\r\n\r\nx <- c(1:5)\r\n\r\nx + 2\r\n#> [1] 3 4 5 6 7\r\n\r\nx * 2\r\n#> [1]  2  4  6  8  10\r\n\r\n\r\n\r\nAs such, a data frame is just a list of vectors of all the same length. That list is what’s known formally as a “recursive vector”. Lists can contain other lists.\r\nThis is a somewhat complex topic. If you’re really hungry for more info, this chapter in R for Data Science is highly informative, but may be confusing at first for those without any programming background.\r\nvignette\r\nA vignette is a long-form guide to a package. It highlights a package’s main functions and their usage. Learning from vignettes is one of the best ways to self-teach yourself a skill in R.\r\nWickham, Hadley\r\nA kiwi and a statistician who probably authored 80% of the links on this page. He is known for the tidyverse, the book R for Data Science, and his twitter.\r\nYAML header\r\nA short blob of text at the top of your R Markdown document specifying things like the document’s title, time and date stamp, and the document’s output type.\r\nThe YAML header is part of what makes R Markdown such a flexible document. There are many ways to customize your R Markdown output. We won’t get into those.\r\nJust know that at the top of your R Markdown document that says output: html_document is what instructs your it to automatically knit as an HTML file.\r\n\r\n\r\n\r\n",
      "last_modified": "2021-11-16T11:00:43-05:00"
    },
    {
      "path": "help_packages.html",
      "title": "Packages",
      "description": "Help with installing and understanding packages, at home and in the wild.\n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\nTerminology\r\nInstalling packages\r\nLoading libraries\r\n\r\nTerminology\r\nPackages and libraries\r\nPackages are a collection of R functions, data, and compiled code.\r\nA library is the location where the packages are stored on your computer.\r\nUsage: “Install the {tidyverse} package from CRAN. It will automatically save to your R library.”\r\nCRAN\r\nCRAN stands for the Comprehensive R Archive Network. Along with providing downloads and updates to the base R computing software, CRAN is the main source for stable, published versions of packages. The function install.packages() automatically looks in CRAN for whatever package you name inside the function.\r\n\r\nUsing other installation functions, it’s also possible to download packages from various other sources, like GitHub. But most of the time, those packages are still in development, which means various features could become inoperable over time (as opposed to making defunct code backwards-compatible). Getting packages from CRAN is the best way to ensure that your code will still run months and sometimes years into the future\r\nObjects, functions, and arguments\r\nObject refers to any data structure in R, with special attributes and methods according to the object type. Most objects are open to exploration and transformation.\r\nExamples of objects: Data frames, linear models, vectors, tables, variables, ggplot objects, matrices, etc.\r\nFunctions are the verbs of the R language. They operate on objects in your R environment, and behave according to their arguments. On this website, functions are written in monospace font and followed by a pair of parentheses, ()\r\nExamples of functions: mean(), summary(), factor(), exp()\r\nArguments are the settings and information supplied to a given function. Each new argument should be separated from the rest with a comma.\r\nSome functions don’t require any arguments, like R.Version() and sessionInfo(). These functions, when run, supply information about the R environment, but don’t operate on an object.\r\nOther functions only require a single argument: an object. For example, the function head() takes many different types of objects, and supplies the first 6 observations within a given object.\r\nBut say you want a different number of observations. head() allows you to manually set those observations with n =.\r\nTyping head(object, n = 10) will return the first 10 observations in a given object.\r\nMost functions come with default arguments that are submitted to the function implicitly. In the case of head(), the default method for n = is 6.\r\nALL arguments can be specified with the syntax argumentName = value. But most functions are written to recognize certain types of values as belonging to a specific argument, and operate on them accordingly.\r\nSo head(object, 10) will also give the first 10 observations of object, since head() knows to recognize an object of type integer as belonging to n =.\r\nYou can view any function’s arguments by going to its documentation. In the R console, run ?functionName, and it will appear in the help window. Under the “Usage” section, you will find information on the function’s syntax and arguments. If an argument is alone, without an equals sign, =, that argument doesn’t have default and must be submitted by the user to run. However, if a given argument contains an equals sign and a value to the right of the equals sign, that value is the argument’s default.\r\nDocumentation\r\nAny function or package’s documentation is the primary authority on the appropriate usage of that function. Typically, a function’s documentation will contain:\r\nA brief description of the function\r\nAn overview of the functin’s usage\r\nA detailed description of each argument needed for the function to run\r\nFurther function details\r\nValue, which indicates what gets returned when the function is run\r\nExamples of the function in use\r\nThe documentation for any function is available via the R console by running ?functionName.\r\nSometimes, you can find documentation that goes into greater detail by searching the function’s documentation online.\r\nWhen you are having problems with your code, it’s smart to check the function’s documentation before looking elsewhere. With practice and a little persistence, you will become more comfortable with reading and interpreting help documentation.\r\nInstalling packages\r\nPackages are collections of functions. As we’ll see shortly, we use functions as code to view, manipulate, and analyze our data.\r\nThere are packages that come built-in with R. These have names like {base}, {utils}, and {stats}.\r\nSince R is open-source, users are able to create their own packages so that other R users can use them. These packages are available in places like the Comprehensive R Archive Network (CRAN for short) and GitHub.\r\nLucky for us, packages are easily retrieved from the R Console. If you haven’t already, run the following code from your console to download the packages that we’ll be needing for Fall Semester. You only need to do this once:\r\n\r\n\r\ninstall.packages(\"tidyverse\")\r\ninstall.packages(\"skimr\")\r\ninstall.packages(\"epiR\")\r\ninstall.packages(\"devtools\")\r\ndevtools::install_github(\"potato-nathan/epiAssist\")\r\n\r\n\r\n\r\nLoading libraries\r\nTo enable R to use a package’s functions in our current project environment, we need to load the packages using the library() function.\r\nIn a fresh code chunk, call in the {tidyverse} and {skimr} packages using the following code:\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(skimr)\r\n\r\n\r\n\r\nNotice that when we install packages, we need to specify their names using quotes, but when we load them into R using library(), R recognizes them as package objects automatically, and so don’t need quotes. You can surround them with quotes and it will load all the same.\r\n\r\n\r\n\r\n",
      "last_modified": "2021-11-16T11:01:31-05:00"
    },
    {
      "path": "help_projects.html",
      "title": "Creating a New Project",
      "description": "This page provides advice on establishing a workflow and creating new projects with RStudio\n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\nCreate folders to store your work\r\nCreate a New Project in RStudio\r\n\r\nIn R, we organize our work by projects. It is best practice to keep no more than one project in any single folder on your computer. First, we should establish the file folders from which we’ll be working.\r\nCreate folders to store your work\r\nIf you haven’t already, now would be a good time to:\r\nCreate a new folder on your computer called “705 Lab”.\r\nWithin that folder, create a folder for whatever lab you’re working on, “Lab X”.\r\nFinally, within that folder, create a folder for your data, called “data”\r\nSave your dataset into the folder called “data”. This helps keep things organized as the project grows.\r\nPlease make this a habit. We will expect you to do this for every lab, as it will keep your work organized and will keep you happy.\r\nCreate a New Project in RStudio\r\nNow open RStudio and take a deep breath. Don’t panic. This will all be very familiar in a few short months.\r\nInitiate a new project by going to File and clicking New Project.\r\nSelect Existing Directory:\r\nInitiate a new project in an existing directoryIn the following window, hit Browse. Navigate to the folder where you’d like to house your new project (or create a new one if you haven’t already. Open it, then hit Open.\r\n\r\n\r\n\r\n",
      "last_modified": "2021-11-16T11:01:34-05:00"
    },
    {
      "path": "help_rstudio.html",
      "title": "Help with RStudio",
      "description": "This page provides links to introduce you to the RStudio environment.\n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\nRStudio Basics\r\nThe R Environment\r\nManaging window panes\r\nChange RStudio color theme\r\nRMarkdown keyboard shortcuts\r\nSession information and help documentation\r\n\r\nExtended resource guide to RStudio\r\n\r\nRStudio Basics\r\nThe R Environment\r\n\r\n\r\n\r\nA: File (New project, New file > R Markdown)B: Pop-out window (also works in R Markdown)C: Manage window panesD: New fileE: Find packages, check for updates, manage environment optionsF: Find cheatsheets (dplyr, ggplot2, R Markdown)G: Current R ProjectH: Console historyI: Plot viewerJ: File pathK: Browse help documentationL: Click to return to working directory homeM: Click to expand or shrink window pane or…N: Drag to manage window size\r\nManaging window panes\r\nDid you lose track of one of your window panes? Use the View tab in the top bar to return to a view with all panes\r\n\r\nChange RStudio color theme\r\nIs the white RStudio background burning your retnas? Need a change of scenery? No need to go outside!\r\nIn the top bar, go to Tools > Global Options, and then select “Appearance”:\r\nChange appearanceRMarkdown keyboard shortcuts\r\nYou can find me mashing the following keys in Windows:\r\nShortcut\r\nKeys (Windows)\r\nKeys (MacOS)\r\nRun a line of code\r\n(With typing cursor on line) ctrl + enter\r\n(With typing cursor on line) cmd + enter\r\nRun a chunk of code\r\n(With typing cursor in chunk) ctrl + shift + enter\r\n(With typing cursor in chunk) cmd + shift + enter\r\nCreate a new chunk\r\nctrl + alt + i\r\ncmd + option + i\r\nType a pipe\r\nctrl + shift + m\r\ncmd + shift + m\r\nType an assignment operator\r\nalt + -\r\noption + -\r\nStop running\r\n(In Console) esc\r\n(In Console) esc\r\nSession information and help documentation\r\nIf for some reason your code is hitting an error, here are some useful commands you might run in your RStudio Console to help troubleshoot:\r\nFunction Documentation\r\nCommand: ?functionName\r\nWhat it does: Pulls up the documentation for whatever you type in place of functionName, and displays it in the Help window. A function’s documentation is often (but not always) the final word on how a function is meant to behave in R. It will provide information on the function’s arguments (and their default values), the resulting output (“Values”), and will give you examples of how to implement that function.\r\nSession information\r\nCommand: sessionInfo()\r\nWhat it does: If you’re experiencing issues with a certain package, maybe it’s time for an update. You can check your current version of R using this command. Find information on specific packages by using the argument ‘package = packageName’.\r\nYou can also find which version of RStudio you have by running RStudio.Version()\r\nView Traceback\r\nCommand: traceback()\r\nWhat it does: If your code keeps hanging up on an error, you can often find clues as to why it’s happening by viewing the underlying code that’s causing your code to halt. Traceback will print the list of functions that were called before the error occurred. It’s literally “tracing your steps” right before the error happened. The language itself in the traceback is usually pretty cryptic and hard to read at first. Don’t let that stop you from looking at it when you encounter problems.\r\nMemory storage information\r\nCommand: object_size()\r\nWhat it does: For the purposes of this lab, you most likely won’t need to worry about your computer’s RAM. Nevertheless, the nerds among us might find it interesting to inspect the size of an object that we’ve saved to our R Environment. Use object_size(objectName) to inspect a given object. Alternatively, inspect overall RAM usage and limitations with memory.size() and memory.limit().\r\nExtended resource guide to RStudio\r\nR Studio will be the launching pad for all of our lab assignments in this course.\r\nThe RStudio environment itself can feel overwhelming at first. Luckily, there are excellent resources already available to help get you acquainted:\r\nFor those of you who prefer to learn by reading, this website’s first tutorial, “Getting started with R/RStudio”, is a good place to start.\r\nDr. Eric Green used to run a summer workshop called “I Eat Data Science for Breakfast”. He gives a stellar tutorial to the RStudio environment starting at 10:49 in his Week 1 video.\r\n\r\n\r\n\r\nFigure 1: I Eat Data Science for Breakfast\r\n\r\n\r\n\r\nIf you were checking your email at all over the summer, you may also already be familiar with Duke Librarian, John Little, and his series called RFun. Feel free to review his video introducing the RStudio environment\r\n\r\n\r\n\r\nFigure 2: RFun with John Little\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2021-11-16T11:01:37-05:00"
    },
    {
      "path": "index.html",
      "title": "Biostat Lab",
      "description": "The hub for DGHI Biostat & Epi lab assignments, help pages, and whirly gigs\n",
      "author": [],
      "contents": "\r\n\r\n\r\n\r\nFigure 1: Probability comparisons\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2021-11-16T11:02:22-05:00"
    },
    {
      "path": "lab_0.html",
      "title": "Lab 0 (705)",
      "description": "__Due:__ September 17, 2021 by 14:00 ET\n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\nLAB MATERIALS\r\nSlides\r\nLab 0 Goals\r\nTask 1: Establish a workflow\r\nCreate folders (directories!) and projects to store your work\r\nCreate a New Project in RStudio\r\n\r\nTask 2: Familiarize yourself with R Markdown\r\nTask 3: Install packages and load libraries\r\nInstalling packages\r\nLoad libraries\r\n\r\nTask 4: Load data\r\nObjects\r\nThe assignment operator (<-)\r\n\r\nTask 5: Explore the data\r\nTask 6: Create variable mage\r\nPipes (%>%)\r\n\r\nTask 7: Frequency distributions of mage\r\nTask 8: Create variable magec\r\ncase_when()\r\nConvert a character variable to a factor\r\n\r\nTask 9: Cross-tab of mage and magec\r\nMethod 1\r\nMethod 2\r\n\r\nTask 10: Save new dataset\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nNote: This lab is not graded (i.e. does not contribute to your course evaluation, so don’t fret)\r\nLAB MATERIALS\r\nR Markdown file for Lab 0 Click link to download. Fill it in with your answers to the following lab tasks. Once you’re finished, rename it as Lab0_FirstinitialYourlastname.Rmd, and submit it using the Sakai dropbox.\r\nLab_0_kenya.rds - data file available in the Resources folder of the Sakai course webpage\r\nSlides\r\n\r\n\r\n\r\nfitvids('.shareagain', {players: 'iframe'});\r\n\r\nLab 0 Goals\r\nBy the end of this lab, you will have demonstrated a basic familiarity with the R coding environment and R Markdown files. You will be able to:\r\nStart a new project and import data\r\nUnderstand useful R terminology\r\nUse summary statistics to describe the Kenya dataset\r\nDerive an ordinal categorical variable from a continuous one\r\nGenerate histograms and boxplots using ggplot2\r\nCreate a simple cross-tabulation of two variables\r\nTask 1: Establish a workflow\r\nIn R, we organize our work by projects. It is best practice to keep no more than one project in any single folder on your computer. First, we should establish the file folders from which we’ll be working. This should always be your first step when beginning an analysis.\r\nCreate folders (directories!) and projects to store your work\r\nIf you haven’t already, now would be a good time to:\r\nCreate a new folder on your computer, calling it “705 Lab”.\r\nWithin that folder, create a folder for this lab, called “Lab 0”.\r\nFinally, within that folder, create a folder for your data, called “data”\r\nSave the dataset titled Lab_0_kenya.rds (downloaded from Sakai) into the folder called “data”\r\nThe resulting file path for your data file should look like this:\r\n“/705 Lab/Lab 0/data/Lab_0_kenya.rds”\r\nPlease make this a habit. We will expect you to do this for every lab, as it will keep your work organized and will keep you happy.\r\nCreate a New Project in RStudio\r\nNow open RStudio and take a deep breath. Don’t panic. This will all be very familiar in a few short months.\r\nInitiate a new project by going to File and clicking New Project.\r\n Then select Existing Directory and hit Browse. Navigate to the folder titled Lab 0. Open it, then hit Open.\r\nIf you’ve done this correctly, your new folder, “Lab 0” should appear in the bottom-right R Studio window pane, under the tab “Files” Likewise, the folder name, “Lab 0” should appear in the upper-right hand corner, indicating the current project’s name.\r\n\r\nCurrent project indicator is marked as “G” in this helpful screenshot of the R Studio environment\r\nTask 2: Familiarize yourself with R Markdown\r\nNow, you will open a special kind of document known as “R Markdown”. This is a text editor (like Word or Google Docs), but with a twist. You can implement the instructions you have written in the R programming language directly within the document. That is, you can “run code” directly within the document. This makes data analysis an interactive, iterative (and therefore fun?) process that usually proceeds as follows:\r\nThe word “run” can have so many meanings. So exactly what do we mean when we say “run code”?\r\nIt’s a term meant to describe the process of allowing a computing program or software to operate upon something you’ve created in your computing environment. In R, you can \"run code directly in the R console by typing your code and hitting enter. R Markdown can also run chunks of code, which we’ll see in a moment.\r\nWrite some code to generate new variables and perform statistical analysis\r\nRun the code\r\nObserve how your dataset behaved\r\nTake a few notes (for Science)\r\nTweak code\r\nRepeat\r\nFor each lab, we will provide you with a skeleton Markdown file. If you haven’t already, download that file (“lab0_705_fall2021.Rmd”, available at the top of this page) and save it to your folder named “Lab 0”.\r\nNow open the file in RStudio. You can just double-click on it from the file folder. It will appear in the RStudio window pane, but might feel a little pinched. Luckily, RStudio allows Markdowns to pop-out.\r\nClick the white square at the top of the Lab 0 document (circled in red in the photo below) to do just that:\r\nPop-outTask 3: Install packages and load libraries\r\nInstalling packages\r\nPackages are collections of functions. As we’ll see shortly, we use functions as code to inspect, manipulate, and analyze our data. They are the verbs of the R language.\r\nThere are packages that come built-in with R. These have names like {base}, {utils}, and {stats}.\r\nSince R is open-source, users are able to create their own packages so that other R users can use them. These packages are available in places like the Comprehensive R Archive Network (CRAN for short) and GitHub.\r\nLucky for us, packages are easily retrieved from the R Console. If you haven’t already, run the following code from the R Console, one line at a time, to download the packages that we’ll be needing for this semester. Unless you uninstall R, __*you should only ever have to do this once__*:\r\n\r\n\r\ninstall.packages(\"tidyverse\")\r\n\r\n\r\n\r\n\r\n\r\ninstall.packages(\"skimr\")\r\n\r\n\r\n\r\n\r\n\r\ninstall.packages(\"tableone\")\r\n\r\n\r\n\r\n\r\n\r\ninstall.packages(\"epiR\")\r\n\r\n\r\n\r\n\r\n\r\ninstall.packages(\"devtools\")\r\n\r\n\r\n\r\n\r\n\r\ndevtools::install_github(\"potato-nathan/epiAssist\")\r\n\r\n\r\n\r\nLoad libraries\r\nUnlike installing packages, every time we open a new R session, we need to enable a package’s use in the R environment. To enable R to use a specific package and its functions, we can load them using the library() function.\r\nIn a fresh code chunk, call in the {tidyverse} and {skimr} packages using the following code:\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(skimr)\r\n\r\n\r\n\r\nNotice that when we install packages, we need to specify their names using quotes because the package name is not yet known to your own copy of RStudio. On the other hand, when we load them into the R environment using library(), R automatically recognizes them as the names of packages, so they don’t require quotations.\r\nTask 4: Load data\r\nYou will use the dataset Lab_0_kenya.rds for this lab. You’ve hopefully already saved it to the folder Lab 0 > data. You will use the function readRDS() to import the data file from your computer’s folder.\r\nSince our project has been created within the “Lab 0” folder, it is thus our “Working Directory”, and RStudio will automatically start from that folder when we give it a function that asks it to look in our file directory.\r\nWithin our function, all we need to do is specify the file name, and that it’s in the folder called “data”.\r\nUse the following code to load your data into R and give it the name kenya.\r\n\r\n\r\nkenya <- readRDS('data/Lab_0_kenya.rds')\r\n\r\n\r\n\r\nWhen we load our data into R, it becomes what, in R, is called a data frame, which is the R term used for a dataset object. Without going into too much detail, it’s like having a spreadsheet of data with rows (i.e. different individual records) and columns (i.e. variables). For those of you familiar with mathematical terminology, it’s like a matrix.\"\r\nObjects\r\nIf functions are the verbs of the R language, objects are the nouns. Just like nouns, there are many different types of objects, which we will learn about throughout the semester. For now, you just need to understand that an object is anything in your R environment that is able to be explored, transformed, or analyzed by R functions. Objects also have unique names. In the code above, kenya is a data frame object.\r\nTo further illustrate, in the chunk of code below, fruit becomes a character vector object of length 3, number becomes a numeric vector object of length 1, and logical becomes a logical vector object of length 1.\r\nYou can use class() to inspect an object’s type, and object.size() to inspect an object’s size. We will learn more about vectors in a later assignment.\r\nMost functions require specific types of objects.\r\nThe assignment operator (<-)\r\nThe <- is called the Assignment Operator. We use it to assign names to objects in our coding environment:\r\nWe can use our assignment operator for characters, numbers, logical operators, etc.:\r\n\r\n\r\nfruit <- c(\"oranges\", \"papayas\", \"apricots\")\r\n\r\nnumber <- 99\r\n\r\nlogical <- FALSE\r\n\r\n\r\n\r\nNow that the above values are stored in our environment, we can use them in other functions or operations as predefined variables:\r\n\r\n\r\npaste(fruit, \"are orange\", sep = \" \")\r\n#> [1] \"oranges are orange\"  \"papayas are orange\"  \"apricots are orange\"\r\n\r\nnumber + 1\r\n#> [1] 100\r\n\r\nisTRUE(logical)\r\n#> [1] FALSE\r\n\r\n\r\n\r\nWe’ve done the same thing with our dataset, giving it the name kenya. We might use the function head() to view the first six rows in the dataset. This is a quick and easy way to glance at our dataset and its accompanying variables:\r\n\r\n\r\nhead(kenya)\r\n\r\n\r\n\r\nTask 5: Explore the data\r\nFamiliarize yourself with the data by using the commands ncol(), nrow(), class(), names() and skim().\r\nAre there any string/character variables?\r\nAre there any variable or value labels?\r\nDo any variables have notes?\r\nSimilar to head(), we can feed our kenya data frame to various functions that tell us other useful information about it. As a tip, you can use $ in the format [datasetName]$[variableName] to refer to a specific variable/column within a dataset.\r\nUse ncol() to print the number of columns in our data frame\r\nUse nrow() to print the number of rows\r\nUse names() to view each variable’s name.\r\nUse class() to view each variable’s “type”\r\nUse skim() to print summary statistics for each variable in the data frame\r\nTask 6: Create variable mage\r\nUsing a pipe (%>%) and the mutate() function, create a new variable, mage for mother’s age (as an integer) at the time of each child’s birth (note – some of these mothers have had multiple children).\r\nThis is calculated from variables b3 (month code of child’s birth) and v011 (month code of mother’s birth). The difference between the values of these variables is in months, so divide by 12 to get years. See the data dictionary for a more detailed description of month codes and how to use them. Use as.integer() around your calculation to truncate the calculated values for mage to integers.\r\n\r\nMost datasets are accompanied by documentation or data dictionaries, which are a description of the variables in the data set and other relevant pieces of information. Be sure to read through these guidelines if you haven’t already.\r\nFor this task, the mutate() function will work with the following syntax.\r\n\r\n\r\n# don't forget to write over your old dataframe using `kenya <-`\r\n\r\ndata <- data %>%\r\n  mutate(newVariableName = (oldVariable1 - oldVariable2))\r\n\r\n\r\n\r\nPipes (%>%)\r\nOne of the most useful tools in the tidyverse package is a little thing called a pipe. It’s represented with the symbol %>%, and allows us to express a series of operations in a continuous string of code, rather than using the assignment operator over and over.\r\nAs an example, let’s pretend we have a dataset that is a record of birds struck by aircraft in the United Sates over the past few decades. We want to know the frequency distribution of sky conditions for birds struck over 5,000 feet within the borders of North Carolina.\r\nIn base R, we might code it like this:\r\n\r\n\r\nbirds_NC <- subset(birds, state == \"NC\")\r\n\r\nbirds_NC <- subset(birds_NC, height > 5000)\r\n\r\ntable(birds_NC$sky)\r\n\r\n\r\n\r\nWith a pipe, %>%, both our code and output becomes more tidy. More importantly, it’s easy to read:\r\n\r\n\r\nbirds %>%\r\n  filter(state == \"NC\", height > 5000) %>%\r\n  group_by(sky) %>%\r\n  count()\r\n\r\n\r\n\r\nThe pipe can be interpeted as signifying “and then”. In the above example, the pipe tells the R console to take the dataset birds, and then filter by state and height, and then group by sky, and then count the observations in each group.\r\nTask 7: Frequency distributions of mage\r\nSuppose you want to break down mage into an ordinal categorical variable with three categories. First, we might inspect the frequency distribution (in one-way frequency table) for mage. Do this using table().\r\ntable() works by identifying unique values within a variable, and then counts their occurence.\r\nIt works on character variables, categorical (factor) variables, and even numbers.\r\nAs was mentioned earlier, we can tell R to look at specific variables inside our dataframe with the $ sign. The syntax looks like this:\r\n\r\n\r\ndataframeName$variableName\r\n\r\n\r\n\r\nIf we want to include a count of NA values in our table, we can also use the argument useNA = 'always' within table().\r\nUse table() to look at kenya$mage, then consider the following questions:\r\nAre there any missing values for mage? If so, how many?\r\nWhich range of ages appear the most frequently in mage?\r\nTask 8: Create variable magec\r\nUsing mage, generate a new variable with three categories: “<18”, “18-39”, and “≥ 40”, naming the new variable magec\r\nmagec stands for: {m}other’s\r\n{age}\r\n{c}ategorical\r\nSet the values for magec to be 0,1,2, where 0 corresponds to the youngest age group (<18).\r\nWe recommend you do this in the following steps:\r\nUse a pipe (%>%) and then mutate() to create a new variable, magec,\r\nWithin your mutate() command, use case_when() to create a series of conditional statements that assign numbers 0, 1, and 2 to each category\r\nOn a new line of code, use factor() to assign labels to each level of your new variable\r\nFinally, once you get it to work, don’t forget to use the assignment operator to save your changes to the kenya data frame.\r\n\r\nValues:\r\n0: <18\r\n1: 18-39\r\n2: ≥ 40\r\ncase_when()\r\nThis function is used to create conditional rules when creating new variables with mutate(). It allows you to create a series of if-then (conditional) statements based on variables within your data. At first, the syntax for case_when() might strike you as a little overly complicated, especially for coding binary variables. But as your variables become more complex, case_when() really shines as a highly efficient way to create new variables on a series of complex conditions.\r\nAn example of the syntax is as follows:\r\n\r\n\r\ncase_when(size == 'small' ~ 0, \r\n          size == 'medium' ~ 1,\r\n          size == 'large' ~ 2,\r\n          TRUE ~ NA)\r\n\r\n\r\n\r\nHere, the tildes represents a formula. To the left of the formula is a logical operation that can evaluate to either TRUE or FALSE\r\n\r\nClick here for a list of logical operators available in the R language\r\nTo the right of the tilde, we put the value that we want to return if the logical operation evaluates to TRUE. If it is FALSE or NULL, case_when() behaves by moving on and testing the next conditional statement.\r\nIf it were run on a dataset containing a variable called size, the literal translation of the above code would go something like this:\r\n\r\nFor each row in the dataset,  \r\n    * if `size` equals 'small', then return 0\r\n    * if `size` equals 'medium', then return 1\r\n    * if `size` equals 'large', then return 2\r\n    * if `size` is any other real value, then return `NA`\r\n\r\n\r\nThis sort of literal translation of a programming language into readable English is what’s known as “pseudo-code”\r\nWhen we do this within a mutate() function, the returned values get assigned to the new variable for each row of the dataset as they’re evaluated.\r\nThe right side of the formula needs to always produce a value of the same variable type, but otherwise you have a high degree of freedom in what can be returned when the conditional statement is TRUE, including mathematical operations on other variables.\r\nSee the documentation for case_when() for more examples of this function’s capabilities\r\nConvert a character variable to a factor\r\nConvert a variable to a factor with the following syntax:\r\n\r\n\r\ndata$variableYouWantToFactor <- factor(data$variableYouWantToFactor,\r\n                                       labels = c(\"Label for 0\", \"Label for 1\", \"Label for 2\"))\r\n\r\n\r\n\r\nTask 9: Cross-tab of mage and magec\r\nLook at a cross-tabulation (two-way table) of mage and magec to ensure that magec was created correctly. Be sure missing values were handled properly (all observations that have a missing value for mage should be assigned the R missing value “NA” for magec). Try the two separate methods for cross-tabulation, as we will be using both for separate purposes later in the semester:\r\nMethod 1\r\nType “?table” in the console for help with how to create a 2x2 table. Note: the order of the variables in the command controls which one is in the rows and which is in the columns. Experiment to make your table readable.\r\n\r\n\r\n# example code:\r\n\r\ntable(data$x, data$y, useNA = 'always')\r\n\r\n\r\n\r\nMethod 2\r\nWe can also use tidyverse functions to accomplish a two-way tabulation of our variables of interest. These functions will become increasingly relevant and useful, and are a big reason why R is such a popular platform for data science. We will use a pipe (%>%), group_by(), another pipe, and count() to get the same output given by table().\r\n\r\n\r\n# example code:\r\n\r\n# notice that we don't want to assign this operation to a name\r\n# we just want to view the output, hence the lack of \"data <- \"\r\ndata %>%\r\n  group_by(x, y) %>%\r\n  count()\r\n\r\n\r\n\r\nA translation of the above code to written instructions would go as follows, where and then represents the grammatical equivalent of our pipe, %>%:\r\n“Take dataset, data, and then group_by variable x, and within those groups, group_by variable y, and then count the values in each of our groups.”\r\nHere’s a link if you’re interested in learning more about pipes. Or just take a look at this tweet:\r\n\r\n\r\n\r\nTask 10: Save new dataset\r\nUsing function saveRDS(), save the new dataset in the same directory as our original data, using the following format: “firstInitial_YourLastName_lab0.rds”\r\nsaveRDS() takes two primary arguments:\r\nThe dataframe object you want to save\r\nThe location in which you’d like it saved as a .rds file\r\nDon’t forget the following:\r\nThe file locations should be in quotes, so that R knows to read it as a character string\r\nYour file should be saved in your local Lab 0 folder, data/\r\n\r\n\r\n\r\n",
      "last_modified": "2021-11-16T11:02:28-05:00"
    },
    {
      "path": "lab_01.html",
      "title": "Lab 01 (707)",
      "description": "__Due:__ February 12, 2022 by 14:00 ET\n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\nLAB MATERIALS (TODO)\r\nLab 01 Goals\r\nLab 01 Grading scheme\r\nPackages\r\n\r\nCompetencies\r\nImport data from .csv\r\nComma separated values (.csv)\r\nImport from .csv (read.csv())\r\n\r\nPlots with {ggplot2}\r\nHistograms (geom_histogram())\r\nScatter plots (geom_point())\r\nQ-Q Plots (qqnorm())\r\nNote on Q-Q Plots\r\n\r\nCreating new variables\r\nlog() transform\r\nSquaring with ^2\r\nCategorical variables with case_when()\r\n\r\nSkewness and Kurtosis ({moments} package)\r\nPearson and Spearman rank correlation coefficients\r\nBonus: Correlation coefficient matrices\r\n\r\nLinear Regression Models\r\nUnivariate (single-variable) models with lm()\r\nMultivariate (multi-variable) models with lm()\r\nLinear models: a brief tour\r\n\r\nPlotting line of best fit\r\n1. Plot first, then model\r\n2. Model first, then plot\r\n\r\nModel Evaluation\r\nResiduals plots\r\nHeteroskedasticity\r\n\r\nModel comparison\r\nF-test\r\n\r\nNested models (task 13, but fancy)\r\nIntro\r\nnest()\r\nModels as list-columns\r\nUnnesting\r\n\r\n\r\n\r\noutline TASK STATUS - import data from .csv draft - done - plots with {ggplot2} draft - done - creating new variables draft - done - skewness and kurtosis draft - done - pearson and spearman rank CC started - linear regression models not started - model evaluation not started - nested models not started - model comparison not started\r\n\r\n\r\n\r\nLAB MATERIALS (TODO)\r\nLab 01 Goals\r\nEstimate crude measures of (linear) association using univariable linear regression models\r\nEstimate crude measures of non-linear association using univariable linear regression models with linear transformed and categorical variables.\r\nEstimate adjusted measures of association using multivariable linear regression models\r\nLab 01 Grading scheme\r\n[[@JOE grading scheme correct?]]\r\nCompetency\r\nPoints\r\n.Rmd file runs without error\r\n10\r\nTable 1a - Pearson CC\r\n3\r\nTable 1b - Spearman rank CC\r\n3\r\nTable 2 - Univariate models\r\n10\r\nTable 3 - F Statistic\r\n3\r\nTask 4 - Multivariate models\r\n10\r\nFigures\r\n15 (1 point each)\r\nShort answers 1 - 8\r\n56 (7 points each)\r\nTotal\r\n100\r\nPackages\r\n{moments}\r\n{car}\r\n{tidyverse}\r\nCompetencies\r\nImport data from .csv\r\nComma separated values (.csv)\r\nCSV stands for “comma separated values”. It’s a way of organizing data. When a file extension reads .csv, spreadsheet applications like Excel understand that to mean that the data is stored according to a simple set of rules.\r\nSee if you can spot the pattern.\r\nThe raw data looks like this:\r\n\r\nName, Date, App, date location, date ranking, second date\r\nJohn, 01-19-2019, Bumble, sip and paint, 7, 0\r\nNiko, 02-14-2019, Hinge, sky diving, 10, 0\r\nArwin, 02-28-2019, Bumble, ice skating, 8, 0\r\nChad, 03-12-2019, Tinder, movie night, 4, 0\r\nOleksiy, 03-15-2019, Bumble, axe throwing, 8, 0\r\nManuel, 03-19-2019, Bumble, bike ride, 9, 0\r\nVince, 03-27-2019, Ship, barcade, 7, 0\r\nAdamu, 04-01-2019, Hinge, botanical garden, 8.5, 0 \r\nRyan, 04-21-2019, Christian mingle, mass, 7.5, 0\r\n\r\nAnd a spreadsheet application displays the data like this:\r\nSo a computer would display the comma-separated data like this:\r\n\r\nName\r\nDate\r\nApp\r\ndate location\r\ndate ranking\r\nsecond date\r\nJohn\r\n01-19-2019\r\nBumble\r\nsip and paint\r\n6.9\r\n0\r\nNiko\r\n02-14-2019\r\nHinge\r\nsky diving\r\n9.9\r\n0\r\nArwin\r\n02-28-2019\r\nBumble\r\nice skating\r\n8.2\r\n0\r\nChad\r\n03-12-2019\r\nTinder\r\nmovie night\r\n4.4\r\n0\r\nOleksiy\r\n03-15-2019\r\nBumble\r\naxe throwing\r\n8.3\r\n0\r\nManuel\r\n03-19-2019\r\nBumble\r\nbike ride\r\n8.9\r\n0\r\nVince\r\n03-27-2019\r\nShip\r\nbarcade\r\n7.3\r\n0\r\nAdamu\r\n04-01-2019\r\nHinge\r\nbotanical garden\r\n8.5\r\n0\r\nRyan\r\n04-21-2019\r\nChristian mingle\r\nmass\r\n7.5\r\n0\r\n\r\n\r\nExpand for the four simple rules of .csv data\r\ncommas delineate columns\r\na new line delineates a new row\r\nthe first row of data determines column names and number of columns\r\nthe values between commas are data\r\nImport from .csv (read.csv())\r\nread.csv() works a lot like readRDS(), except it can interact with .csv files.\r\nBefore you try to import your data, do yourself a favor and open the .csv file in Excel to inspect it.\r\nThe first 10 rows of regdat.csvNow that you’ve done that, note that read.csv() allows the following arguments relevant to importing this lab’s data:\r\nfile = \"\" - just like readRDS, you need to provide a file name in quotes\r\nheader = - this argument takes either TRUE or FALSE. If set to TRUE, the function will use the first row of the data as the column names (“headers”)\r\ncol.names = c() - takes a vector that is the same length as the number of columns in the dataset. It will use the values in this vector as the column names.\r\nHere is an example with filler column names:\r\n\r\n\r\ndata <- read.csv(\"regdat.csv\", header = FALSE, col.names = c(\"columnA\", \"columnB\", \"columnC\", \"columnD\"))\r\n\r\n\r\n\r\nDon’t forget to use <- to assign a name to your imported data frame!\r\nPlots with {ggplot2}\r\nHistograms (geom_histogram())\r\nIf you need a refresher on creating histograms with {ggplot2}, you can find it here.\r\nScatter plots (geom_point())\r\nTODO?\r\nWe create scatter plots by assigning an x and y aesthetic (aes()) to a ggplot object, and then adding a layer called geom_point()\r\n\r\n\r\n\r\nQ-Q Plots (qqnorm())\r\nA Q-Q plot allows us to compare a variable’s observed distribution against its “theoretical” distribution. It takes a continuous variable, converts each value to a Z-score (using the variable’s mean and standard deviation), and then plots the Z-score against the original observed value.\r\nHere are the first 10 values of regdat$weight, and their accompanying Z-scores\r\n\r\nObserved Values\r\nZ Values\r\n85.0\r\n-0.6979123\r\n105.0\r\n0.1272606\r\n108.0\r\n0.2675710\r\n92.0\r\n-0.4018917\r\n112.5\r\n0.7389858\r\n112.0\r\n0.4482006\r\n104.0\r\n0.0847131\r\n69.0\r\n-1.9916133\r\n94.5\r\n-0.2348232\r\n68.5\r\n-2.0751279\r\n\r\nWe can use qqnorm() to calculate and plot these values. Here is an example using regdat$weight, with histogram for reference:\r\n\r\n\r\nqqnorm(regdat$weight, main = \"Normal Q-Q Plot of Weight (lbs.)\")\r\nhist(regdat$weight)\r\n\r\n\r\n\r\n\r\n\r\nIf weight were normally distributed, we would expect a straight diagonal line. See the note on Q-Q plots below for an example of this.\r\nBonus: qqnorm(), but with ggplot()\r\nFor those interested:\r\nRemember the discussion of how most objects contain more information than initially meets the eye?\r\nThe same goes for qqnorm objects. They aren’t just a bunch of dots. They contain vectors of the theoretical and observed (“Sampled”) quantiles. If we convert the qqnorm object to a data.frame, we can use ggplot() to call those specific columns for plotting:\r\n\r\n\r\nregdat$ln_wt <- log(regdat$weight)\r\n\r\nqqnorm_data <- data.frame(qqnorm(regdat$ln_wt))\r\n\r\nggplot(data = qqnorm_data, aes(x = x, y = y)) +\r\n  geom_point(shape = 1, size = 3) + \r\n  labs(title = \"Q-Q plot for ln(weight)\") +\r\n  xlab(\"Theoretical Quantiles for ln_wt\") +\r\n  ylab(\"Sample Quantiles for ln_wt\")\r\n\r\n\r\n\r\n\r\nNote on Q-Q Plots\r\nFor a quick comparison, we can use a simple model to visualize how a truly normal distribution would appear on a Q-Q plot:\r\nThis page from U Virginia is an excellent resource.\r\nIf we extract the mean and standard deviation from our height, we can construct a normal distribution and plot it.\r\n\r\n\r\nx_hat <- mean(regdat$height)\r\n\r\nx_sd <- sd(regdat$height)\r\n\r\nnormal <- rnorm(500, mean = x_hat, sd = x_sd)\r\n\r\nplot(normal)\r\n\r\nhist(normal)\r\n\r\nqqnorm(normal)\r\n\r\n\r\n\r\n\r\nCreating new variables\r\nlog() transform\r\nIn R, log() takes the natural log of a given value:\r\n\r\n\r\nlog(10)\r\n\r\n\r\n[1] 2.302585\r\n\r\n#> [1] 2.302585\r\n\r\n\r\n\r\nIf given a vector of values, log() will take the natural log of each of those values individually:\r\nNot relevant to Lab 01, but you can also specify what base you want your log to be in with the argument base =:\r\n\r\n[1] 0 1 2 3\r\n\r\nWhy do a log transformation?\r\nSay we have a variable whose distribution is right-skewed:\r\n\r\n\r\n\r\n\r\n\r\n\r\nWe can use a natural log transformation to correct for that skew: [[@joe what kind of language would we like to use here?]]\r\n(https://www.datanovia.com/en/lessons/transform-data-to-normal-distribution-in-r/)\r\n\r\n\r\n\r\nSquaring with ^2\r\nIn R, you can raise a base by any exponent using ^. For example, 103 would be calculated like this:\r\n\r\n\r\n10^3\r\n\r\n#> [1] 1000\r\n\r\n\r\n\r\nIf you give R a vector of numbers, followed by an exponent, it will perform the operation on each value individually:\r\n\r\n\r\nsimple_vector <- c(3, 3.3, 12, 1)\r\nsimple_vector^2\r\n\r\n#> [1]   9.00  10.89 144.00   1.00\r\n\r\n\r\n\r\nRecall that data frames are just lists of vectors!\r\nYou might consider using %>% and mutate() to square a variable.\r\nCategorical variables with case_when()\r\nFor a refresher on how to generate categorical variables from continuous variables, you can refer back to when we created magec in last semester’s first lab.\r\nSkewness and Kurtosis ({moments} package)\r\nTo better understand the measures of skewness and kurtosis, please watch this video.\r\n\r\n\r\nHere is a link for those who would prefer to read about it.\r\nThe parts on skewness and kurtosis start at minute 2:40, but the beginning of the video might provide a welcome review on how to describe different distributions.\r\nIn R, we can calculate both skewness and kurtosis using a package called {moments} and its handy functions skewness() and kurtosis():\r\n\r\n\r\nskewness(regdat$weight)\r\n\r\nkurtosis(regdat$weight)\r\n\r\n#> [1] 0.5843876\r\n#> [1] 3.765937\r\n\r\n\r\n\r\n\r\nYou will need to install {moments} with install.packages(\"moments\")\r\n[[@Joe : need chi2 test of joint skewness-kurtosis\r\nThe instructions for Lab 01 ask for a “Skewness-Kurtosis” test, which, computed in Stata, is a joint computation.\r\nThe Methods and formulas section in the Stata Documentation mentions a Royston adjustment to the D’Agostino omnibus test of normality. We may need to hard-code this calculation in order to get a test statistic similar to that rendered in the Stata output for sktest ln_wt.\r\nHowever, interpreting the raw tests of skewness and kurtosis should suffice for our purposes of assessing for normality. ]]\r\nPearson and Spearman rank correlation coefficients\r\nWe use correlation coefficients to measure the strength and direction of the linear association between two continuous variables. [[@joe: is the phrasing fine here?]]\r\nCorrelation coefficients can range from -1 to 1. Values closer to 1 represent a strong positive correlation. Those closer to -1 represent a strong negative correlation. The closer the correlation coefficient, R, is to 0, the weaker the association between the two variables.\r\n\r\n\r\n\r\n\r\n\r\n\r\nTo calculate in R, use the function cor(x, y, method = c(\"pearson\", \"kendall\", \"spearman\")) Specify your x and y variables and set the method to your cheeky brit of choice.\r\n\r\nThe Pearson is the parametric test. Both Spearman and Kendall refer to rank-based (non-parametric) measures of association for when our data is non-normal.\r\nIf no method is specified, “pearson” is the default.\r\nBonus: Correlation coefficient matrices\r\nIf you want to have a very particular kind of fun, try the following:\r\nRemove all non-continuous variables from your data frame and assign it a new name, like cont_data.\r\nUse R to run cor(cont_data), where the data frame of continuous variable vectors is the only argument.\r\n\r\nYou can use {dplyr}’s select() to remove variables by name. There are many many ways to do this.\r\nWhat did it do? (Go look at Table 1 in the assignment. )\r\nLinear Regression Models\r\nBuilding models in R is fun!\r\nUnivariate (single-variable) models with lm()\r\nRemember the concept of R formulas?\r\nRegression models make use of formulas in a very intuitive way. Here is the format:\r\n\r\n\r\nlm(response_var ~ predictor_var, data = a_data_frame)\r\n\r\n\r\n\r\nlm stands for “linear model”. If we assign a name to our model, we can access various components within the model, which we can index with $.\r\nWe then use summary() to view an overview of the model’s most important components.\r\n\r\n\r\nm1 <- lm(Sepal.Length ~ Petal.Length, data = iris)\r\nsummary(m1)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = Sepal.Length ~ Petal.Length, data = iris)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-1.24675 -0.29657 -0.01515  0.27676  1.00269 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)   4.30660    0.07839   54.94   <2e-16 ***\r\nPetal.Length  0.40892    0.01889   21.65   <2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.4071 on 148 degrees of freedom\r\nMultiple R-squared:   0.76, Adjusted R-squared:  0.7583 \r\nF-statistic: 468.6 on 1 and 148 DF,  p-value: < 2.2e-16\r\n\r\nMultivariate (multi-variable) models with lm()\r\nYou can also use lm() to program models with multiple linear predictor variables. You can formulate your equation as you build your model, using arithmetic operators on the right-hand side of the formula:\r\n\r\n\r\nlm(response_var ~ predictor_var1 + predictor_var2, data = a_data_frame)\r\n\r\n\r\n\r\n\r\n\r\nmlm1 <- lm(Sepal.Length ~ Petal.Length + Species, data = iris)\r\n\r\nsummary(mlm1)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = Sepal.Length ~ Petal.Length + Species, data = iris)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-0.75310 -0.23142 -0.00081  0.23085  1.03100 \r\n\r\nCoefficients:\r\n                  Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)        3.68353    0.10610  34.719  < 2e-16 ***\r\nPetal.Length       0.90456    0.06479  13.962  < 2e-16 ***\r\nSpeciesversicolor -1.60097    0.19347  -8.275 7.37e-14 ***\r\nSpeciesvirginica  -2.11767    0.27346  -7.744 1.48e-12 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.338 on 146 degrees of freedom\r\nMultiple R-squared:  0.8367,    Adjusted R-squared:  0.8334 \r\nF-statistic: 249.4 on 3 and 146 DF,  p-value: < 2.2e-16\r\n\r\nLinear models: a brief tour\r\nIn this section, we’ll show you how to access a model’s coefficients, confidence intervals, residuals, and other values that might come in handy.\r\nRemember how we said that you can extract numbers from your model when you assign it a name? If we inspect m1 using $ to index the object by name, we’ll see some useful items at our disposal.\r\nYou can obtain the model’s coefficients with coef(your_model)\r\n\r\n\r\ncoef(m1)\r\n\r\n\r\n (Intercept) Petal.Length \r\n   4.3066034    0.4089223 \r\n\r\nAnd you can calculate 95% confidence intervals with confint(your_model, level = 0.95).\r\n\r\n\r\nconfint(m1,  level = .95)\r\n\r\n\r\n                 2.5 %    97.5 %\r\n(Intercept)  4.1516972 4.4615096\r\nPetal.Length 0.3715907 0.4462539\r\n\r\nAnd don’t forget that you can inspect your model with summary()\r\n\r\n\r\nsummary(m1)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = Sepal.Length ~ Petal.Length, data = iris)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-1.24675 -0.29657 -0.01515  0.27676  1.00269 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)   4.30660    0.07839   54.94   <2e-16 ***\r\nPetal.Length  0.40892    0.01889   21.65   <2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.4071 on 148 degrees of freedom\r\nMultiple R-squared:   0.76, Adjusted R-squared:  0.7583 \r\nF-statistic: 468.6 on 1 and 148 DF,  p-value: < 2.2e-16\r\n\r\nOur model also contains the columns of data (variables) used in its calculation:\r\n\r\n\r\nhead(m1$model)\r\n\r\n\r\n  Sepal.Length Petal.Length\r\n1          5.1          1.4\r\n2          4.9          1.4\r\n3          4.7          1.3\r\n4          4.6          1.5\r\n5          5.0          1.4\r\n6          5.4          1.7\r\n\r\n\r\nSepal.Length\r\nPetal.Length\r\n5.1\r\n1.4\r\n4.9\r\n1.4\r\n4.7\r\n1.3\r\n4.6\r\n1.5\r\n5.0\r\n1.4\r\n5.4\r\n1.7\r\n\r\nIt also has the resulting fitted values:\r\n\r\n\r\nhead(m1$fitted.values)\r\n\r\n\r\n       1        2        3        4        5        6 \r\n4.879095 4.879095 4.838202 4.919987 4.879095 5.001771 \r\n\r\nThe fitted values are what result when we plug each row into the model equation. We can use the first row of our data to observe this directly:\r\n\r\n\r\nx <- m1$model[1,2] # Petal.Length of row 1\r\n\r\nB0 <- m1$coefficients[1] # beta-naught\r\n\r\nB1 <- m1$coefficients[2] # beta-one\r\n\r\n# formula:\r\nfitted <- B0 + B1 * x\r\nfitted\r\n\r\n\r\n(Intercept) \r\n   4.879095 \r\n\r\nThe resulting value is the “fitted” value– or the estimated value for the response value according to our model, given the value of our predictor variable, 1.4\r\nNotice that our model also contains residuals, or the differences between our observed values for y (our response variable), and the values predicted by our model:\r\n\r\n\r\nhead(m1$residuals)\r\n\r\n\r\n         1          2          3          4          5          6 \r\n 0.2209054  0.0209054 -0.1382024 -0.3199868  0.1209054  0.3982287 \r\n\r\nWe can find the first row’s residual value with the fitted value that we calculated above:\r\n\r\n\r\nm1$model[1,1] - # observed Sepal.Length of row 1\r\n  fitted\r\n\r\n\r\n(Intercept) \r\n  0.2209054 \r\n\r\nPlotting line of best fit\r\n{ggplot2} provides a wide variety of flexible tools for visualizing models. There are two main ways we might visualize a regression models:\r\nPlot, then have {ggplot2} calculate line of best fit\r\nBuild model, then plot using model object\r\nHere is a highly useful tutorial for how to plot different models in {ggplot2}. It includes splines, Loess, and multiple variatons on quadratics.\r\nWe will use both throughout the semester, so we will give a brief intro to both methods here.\r\n1. Plot first, then model\r\nTo perform this method, we need to start with a scatter plot using {ggplot2}:\r\n\r\n\r\np <- ggplot(data = toy, aes(x = ran1, y = pos)) +\r\n  geom_point() +\r\n  labs(x = \"Age (years)\", y = \"Points\", title = \"This is fake data\")\r\n\r\np\r\n\r\n\r\n\r\n\r\n\r\nNotice that we have assigned a name to our plot, p. By doing this, we can add layers to the original plot without needing to reproduce the code each time.\r\nOnce we have our plot, we just need to add another layer, stat_smooth(), and designate a method and formula. Here, we’re building a univariate, linear model. Observe how we parameterize these details within the function:\r\n\r\n\r\np + stat_smooth(method = \"lm\", formula = y ~ x)\r\n\r\n\r\n\r\n\r\nNotice that if we don’t designate a method, stat_smooth() will default to plotting a smoothed “loess” line, which we’ll learn more about later in the semester:\r\n\r\n\r\n\r\nFor those curious, we can also print our model’s equation in the plot. The code is a bit dense, but straightforward:\r\n\r\n\r\nrequire(ggpmisc)\r\n\r\np + stat_smooth(method = \"lm\", formula = y ~ x) +\r\n  stat_poly_eq(formula = y ~ x, \r\n                aes(label = paste(..eq.label.., ..rr.label.., sep = \"~~~\")), \r\n                parse = TRUE)\r\n\r\n\r\n\r\n\r\n\r\nFrom this post on SO.\r\n2. Model first, then plot\r\nWe can also plot our data and line of best fit by using a model as the data object required by {ggplot2}.\r\nRemember our example model from earlier:\r\n\r\n\r\nCall:\r\nlm(formula = Sepal.Length ~ Petal.Length, data = iris)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-1.24675 -0.29657 -0.01515  0.27676  1.00269 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)   4.30660    0.07839   54.94   <2e-16 ***\r\nPetal.Length  0.40892    0.01889   21.65   <2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.4071 on 148 degrees of freedom\r\nMultiple R-squared:   0.76, Adjusted R-squared:  0.7583 \r\nF-statistic: 468.6 on 1 and 148 DF,  p-value: < 2.2e-16\r\n\r\nm1 will serve as our data object within ggplot(). We can start by creating a scatter plot of the response and predictor variables from our formula. But instead of using the original data frame (regdat), we’ll use data = m1:\r\n\r\n\r\np2 <- ggplot(data = m1, aes(x = Petal.Length, y = Sepal.Length)) +\r\n  geom_point()\r\n\r\np2\r\n\r\n\r\n\r\n\r\nFrom here, we can add a geom_smooth() or stat_smooth() layer, using method = \"lm\" to fit an ordinary least squares regression line to our data:\r\n\r\n\r\np2 + geom_smooth(method = \"lm\")\r\n\r\n\r\n\r\n\r\nThis second method becomes more important as our models increase in complexity. It also will helps in plotting residuals (see next section).\r\nModel Evaluation\r\nOnce we’ve built a model, there are various measures we might use to evaluate how well that model fits our data. Here we discuss two of those measures: residuals and heteroscedasticity.\r\nResiduals plots\r\nOur model object contains a vector of residuals, which are the observed Y minus the predicted Y:\r\n\r\n\r\nhead(m1$residuals)\r\n\r\n\r\n         1          2          3          4          5          6 \r\n 0.2209054  0.0209054 -0.1382024 -0.3199868  0.1209054  0.3982287 \r\n\r\nWe could generate a histogram of our residuals. In the following code, notice that {ggplot2} finds our residuals when we refer to them as .resid:\r\n\r\n\r\nggplot(data = m1, aes(x = .resid)) + \r\n  geom_histogram(binwidth = .1, fill = \"seagreen\") +\r\n  xlab('Residuals') +\r\n  ggtitle(\"Histogram of residuals of Iris Petal Length regressed on Sepal Length\")\r\n\r\n\r\n\r\n\r\nBut we can also use that vector of residuals, .resid to create a scatter plot of our fitted data/observed data (which one?) against our residuals:\r\n\r\n\r\nggplot(data = m1, aes(x = .fitted, y = .resid)) +\r\n  geom_point() +\r\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\r\n  labs(x = \"Fitted values\", \r\n       y = \"Residuals\", \r\n       title = \"Residual plot of iris petal length regressed on sepal length\")\r\n\r\n\r\n\r\n\r\n\r\nNotice that we have added a dashed horizontal line at y = 0 by adding a layer to our plot: + geom_hline(yintercept = 0, linetype = \"dashed\")\r\n\r\n\r\nggplot(data = m1, aes(x = Petal.Length, y = .resid)) +\r\n  geom_point() +\r\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\r\n  labs(x = \"Fitted values\", \r\n       y = \"Residuals\", \r\n       title = \"Residual plot of iris petal length regressed on sepal length\")\r\n\r\n\r\n\r\n\r\nThis tells us if there is any bias in our model’s fit. If we observe a visible trend in our residual plot [TODO]\r\nOne such trend we might observe is heteroskedasticity,\r\nHeteroskedasticity\r\nSimply put, heteroskedasticity is when the variability of Y (our response variable) is different depending on X (our predictor variable).\r\nIn the following plot, is the probability of an accurate prediction of Y higher when X is closer to 0 or 100?\r\n\r\n\r\n\r\n\r\n\r\nggplot(het, aes(x = x, y = z)) +\r\n  geom_point() +\r\n  labs(x = \"X\", y = \"Y\", title = \"More fake data\")\r\n\r\n\r\n\r\n\r\nWikipedia is telling me that this is Greek for “different” (hetero) “dispersion” (skedasis)\r\nThe data above is heteroskedastilisticexpialidocious.\r\nLink, if you don’t get the reference\r\nBreusch-Pagan/CookWeisberg\r\nWe can use a chi-squared test to assess how heteroskedastic our data really is. We can do this using ncvTest() from the package {car}.\r\nHere’s a test of our iris model, whose residuals we visualized above:\r\n\r\n\r\nncvTest(m1)\r\n\r\n\r\nNon-constant Variance Score Test \r\nVariance formula: ~ fitted.values \r\nChisquare = 2.493218, Df = 1, p = 0.11434\r\n\r\n\r\nFor proof of method, see “References” section in documentation\r\nMeanwhile, we’re more certain in the heterskedastilistic nature of the fake data:\r\n\r\n\r\nncvTest(lm(z~x, data = het))\r\n\r\n\r\nNon-constant Variance Score Test \r\nVariance formula: ~ fitted.values \r\nChisquare = 27.79159, Df = 1, p = 1.3511e-07\r\n\r\nModel comparison\r\nWe can use an F-statistic to compare models when we add or omit variables. Section 9.2 of OpenIntro contains a great introduction to model selection. In it, they recommend two different model selection strategies:\r\nBackwards elimination - where we generate a fully saturated model, then take one variable out at a time.\r\nForward selection - where we start with a single predictor variable, and add one new variable at a time.\r\nWith each step in either of these processes, we need some kind of test to help us evaluate if the removal/addition of a variable has improved the model.\r\nIn OpenIntro, they use R2. But we can also use an F-test.\r\nF-test\r\nUse anova(aModel, anotherModel) to compute the F-statistic for the comparison of two models.\r\nFurther up on this page, we created a model that aimed to predict iris sepal length using measures of iris petal length.\r\nHere’s that model in brief:\r\n\r\n\r\nsummary(m1)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = Sepal.Length ~ Petal.Length, data = iris)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-1.24675 -0.29657 -0.01515  0.27676  1.00269 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)   4.30660    0.07839   54.94   <2e-16 ***\r\nPetal.Length  0.40892    0.01889   21.65   <2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.4071 on 148 degrees of freedom\r\nMultiple R-squared:   0.76, Adjusted R-squared:  0.7583 \r\nF-statistic: 468.6 on 1 and 148 DF,  p-value: < 2.2e-16\r\n\r\nWe may wonder if using a quadratic formula renders a better model:\r\n\r\n\r\nm1_quadratic <- lm(Sepal.Length ~ Petal.Length + I(Petal.Length^2), data = iris)\r\nsummary(m1_quadratic)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = Sepal.Length ~ Petal.Length + I(Petal.Length^2), \r\n    data = iris)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-1.0684 -0.2348  0.0121  0.2049  0.9146 \r\n\r\nCoefficients:\r\n                  Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)        5.05833    0.14036  36.038  < 2e-16 ***\r\nPetal.Length      -0.16435    0.09427  -1.743   0.0834 .  \r\nI(Petal.Length^2)  0.08146    0.01318   6.181 5.96e-09 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.3639 on 147 degrees of freedom\r\nMultiple R-squared:  0.8095,    Adjusted R-squared:  0.8069 \r\nF-statistic: 312.3 on 2 and 147 DF,  p-value: < 2.2e-16\r\n\r\nCompare the two outputs and notice that our Adjusted R-squared has increased. This is a good sign.\r\nBut an F-statistic would also tell us… [[@joe I'll let you explain here]]\r\n\r\n\r\nanova(m1, m1_quadratic, test = \"F\")\r\n\r\n\r\nAnalysis of Variance Table\r\n\r\nModel 1: Sepal.Length ~ Petal.Length\r\nModel 2: Sepal.Length ~ Petal.Length + I(Petal.Length^2)\r\n  Res.Df    RSS Df Sum of Sq      F    Pr(>F)    \r\n1    148 24.525                                  \r\n2    147 19.466  1    5.0593 38.206 5.955e-09 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\n\r\nIf you want to know more about how this test is performed, this article by Danielle Navarro is a great place to look.\r\nThe main thing of importance when performing an F-test on two models is that the smaller of the two models (the “null” model) cannot contain variables that are not in the larger model (the “alternative” or “full” model).\r\nNested models (task 13, but fancy)\r\nIntro\r\nThis section is drawn from Hadley Wickham’s “R for Data Science”. Honestly, Hadley does a much better job. I’m mostly using this to learn it myself.\r\n[[@joe this is probably too much. I would think that we should just keep things simple and have students filter the dataset into male and female, then run the model on separate datasets.]]\r\nFor task 13, you will likely just use %>% and filter() to subset their data by sex, and then run separate regressions on those two separate data frames.\r\nThis is a quick explanation of how to do it at scale (with many categories in a single subsetting variable):\r\nnest()\r\nR allows us to make “nested” datasets according to categorical groupings. We can then use tidyverse functions to generate linear models on each of those subsets in our nested data.\r\nFirst, we need to group and nest our dataset according to a category. In the iris dataset, we could use species:\r\n\r\n\r\niris_by_species <- iris %>%\r\n  group_by(Species) %>%\r\n  nest()\r\n\r\n\r\n\r\nThe printed output of that dataset looks like this:\r\n\r\n# A tibble: 3 x 2\r\n# Groups:   Species [3]\r\n  Species    data             \r\n  <fct>      <list>           \r\n1 setosa     <tibble [50 x 4]>\r\n2 versicolor <tibble [50 x 4]>\r\n3 virginica  <tibble [50 x 4]>\r\n\r\n\r\nThree tibbles in a tibble?\r\nHere’s how we would look at the setosa tibble:\r\n\r\n\r\nhead(iris_by_species$data[[1]])\r\n\r\n\r\n\r\n\r\nSepal.Length\r\nSepal.Width\r\nPetal.Length\r\nPetal.Width\r\n5.1\r\n3.5\r\n1.4\r\n0.2\r\n4.9\r\n3.0\r\n1.4\r\n0.2\r\n4.7\r\n3.2\r\n1.3\r\n0.2\r\n4.6\r\n3.1\r\n1.5\r\n0.2\r\n5.0\r\n3.6\r\n1.4\r\n0.2\r\n5.4\r\n3.9\r\n1.7\r\n0.4\r\n\r\nModels as list-columns\r\nNow that our data frame has been nested, we need a function for fitting models:\r\n\r\n\r\n\r\nWe’ll use purrr::map() to apply that function to each of our nested data frames. By using this along with mutate(), we can add a new column to our nesting data frame that contains the models:\r\n\r\n\r\niris_by_species <- iris_by_species %>%\r\n  mutate(model = map(data, iris_model)) # here, `data` refers to the column in our nested data frame\r\n\r\niris_by_species\r\n\r\n\r\n# A tibble: 3 x 3\r\n# Groups:   Species [3]\r\n  Species    data              model \r\n  <fct>      <list>            <list>\r\n1 setosa     <tibble [50 x 4]> <lm>  \r\n2 versicolor <tibble [50 x 4]> <lm>  \r\n3 virginica  <tibble [50 x 4]> <lm>  \r\n\r\nFinally, we can add the residuals of each model as another column in our nested data:\r\n\r\n\r\nrequire(modelr)\r\niris_by_species <- iris_by_species %>%\r\n  mutate(resids = map2(data, model, add_residuals))\r\n\r\niris_by_species\r\n\r\n\r\n# A tibble: 3 x 4\r\n# Groups:   Species [3]\r\n  Species    data              model  resids           \r\n  <fct>      <list>            <list> <list>           \r\n1 setosa     <tibble [50 x 4]> <lm>   <tibble [50 x 5]>\r\n2 versicolor <tibble [50 x 4]> <lm>   <tibble [50 x 5]>\r\n3 virginica  <tibble [50 x 4]> <lm>   <tibble [50 x 5]>\r\n\r\nUnnesting\r\nTo actually work with the results of our model, we need to unnest() it:\r\n\r\nSpecies\r\nSepal.Length\r\nSepal.Width\r\nPetal.Length\r\nPetal.Width\r\nresid\r\nsetosa\r\n5.1\r\n3.5\r\n1.4\r\n0.2\r\n-0.0743734\r\nsetosa\r\n4.9\r\n3.0\r\n1.4\r\n0.2\r\n-0.0480470\r\nsetosa\r\n4.7\r\n3.2\r\n1.3\r\n0.2\r\n-0.1217207\r\nsetosa\r\n4.6\r\n3.1\r\n1.5\r\n0.2\r\n0.0914425\r\nsetosa\r\n5.0\r\n3.6\r\n1.4\r\n0.2\r\n-0.0612102\r\nsetosa\r\n5.4\r\n3.9\r\n1.7\r\n0.4\r\n0.1861371\r\n\r\nDo these residuals look different than our previous model of residuals?\r\n\r\n\r\nggplot(data = m1, aes(x = Petal.Length, y = .resid)) +\r\n  geom_point() +\r\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\r\n  labs(x = \"Fitted values\", y = \"Residuals\", title = \"Residual plot of iris petal length regressed on sepal length\", subtitle = \"(Original model)\")\r\n\r\n\r\n\r\n\r\n\r\n\r\nggplot(data = resids, aes(x = Petal.Length, y = resid)) +\r\n  geom_point() +\r\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\r\n  labs(x = \"Petal.Length\", y = \"Residuals\", title = \"Residual plot of iris petal length regressed on sepal length\", subtitle = \"(nested models by species)\")\r\n\r\n\r\n\r\n\r\nWhat about the residuals of a model with a categorical predictor term for species?\r\n\r\n\r\nm1_species <- lm(Sepal.Length ~ Petal.Length + Species, data = iris)\r\n\r\n\r\n\r\n\r\n\r\nggplot(data = m1_species, aes(x = Sepal.Length, y = .resid)) +\r\n  geom_point() +\r\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\r\n  labs(x = \"Petal.Length\", y = \"Residuals\", title = \"Residual plot of iris petal length regressed on sepal length\", subtitle = \"With species predictor variable\")\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2021-11-16T11:03:28-05:00"
    },
    {
      "path": "lab_01_alternate.html",
      "title": "Lab 01 (707)",
      "description": "__Due:__ February 12, 2022 by 14:00 ET\n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\nLab 01 Goals\r\nLab 01 Grading scheme\r\nPackages\r\nCompetencies\r\nImport data from .csv\r\nComma separated values (.csv)\r\nImport from .csv (read.csv())\r\n\r\nPlots with {ggplot2}\r\nHistograms (geom_histogram())\r\nScatter plots (geom_point())\r\nQ-Q Plots (qqnorm())\r\nNote on Q-Q Plots\r\n\r\nCreating new variables\r\nlog() transform\r\nSquaring with ^2\r\nCategorical variables with case_when()\r\n\r\nChecking normality and collinearity\r\nskewness() and kurtosis()\r\nPearson and Spearman rank correlation coefficients\r\nBonus: Correlation coefficient matrices\r\n\r\n\r\nLinear Regression Models\r\nUnivariate (single-variable) models with lm()\r\nMultivariate (multi-variable) models with lm()\r\nLinear model objects: a brief tour\r\n\r\nPlotting line of best fit\r\n1. Plot first, then model\r\n2. Model first, then plot\r\n\r\nModel evaluation\r\nResiduals plots\r\nHeteroskedasticity\r\n\r\nModel comparison\r\nF-test\r\n\r\nNested models (task 13, but fancy)\r\nIntro\r\nnest()\r\nModels as list-columns\r\nUnnesting\r\n\r\n\r\n\r\n\r\n\r\n\r\nLAB MATERIALS\r\nLab 01 Goals\r\nEstimate crude measures of (linear) association using univariable linear regression models\r\nEstimate crude measures of non-linear association using univariable linear regression models with linear transformed and categorical variables.\r\nEstimate adjusted measures of association using multivariable linear regression models\r\nLab 01 Grading scheme\r\n[[@JOE grading scheme correct?]]\r\nCompetency\r\nPoints\r\n.Rmd file runs without error\r\n10\r\nTable 1a - Pearson CC\r\n3\r\nTable 1b - Spearman rank CC\r\n3\r\nTable 2 - Univariate models\r\n10\r\nTable 3 - F Statistic\r\n3\r\nTask 4 - Multivariate models\r\n10\r\nFigures\r\n15 (1 point each)\r\nShort answers 1 - 8\r\n56 (7 points each)\r\nTotal\r\n100\r\nPackages\r\n{moments}\r\n{car}\r\n{tidyverse}\r\nCompetencies\r\n\r\nImport data from .csv\r\nImport data from .csv\r\nComma separated values (.csv)\r\nCSV stands for “comma separated values”. It’s a way of organizing data. When a file extension reads .csv, spreadsheet applications like Excel understand that to mean that the data is stored according to a simple set of rules.\r\nSee if you can spot the pattern.\r\nThe raw data looks like this:\r\n\r\nName, Date, App, date location, date ranking, second date\r\nJohn, 01-19-2019, Bumble, sip and paint, 7, 0\r\nNiko, 02-14-2019, Hinge, sky diving, 10, 0\r\nArwin, 02-28-2019, Bumble, ice skating, 8, 0\r\nChad, 03-12-2019, Tinder, movie night, 4, 0\r\nOleksiy, 03-15-2019, Bumble, axe throwing, 8, 0\r\nManuel, 03-19-2019, Bumble, bike ride, 9, 0\r\nVince, 03-27-2019, Ship, barcade, 7, 0\r\nAdamu, 04-01-2019, Hinge, botanical garden, 8.5, 0 \r\nRyan, 04-21-2019, Christian mingle, mass, 7.5, 0\r\n\r\nAnd a spreadsheet application displays the data like this:\r\nSo a computer would display the comma-separated data like this:\r\n\r\nName\r\nDate\r\nApp\r\ndate location\r\ndate ranking\r\nsecond date\r\nJohn\r\n01-19-2019\r\nBumble\r\nsip and paint\r\n6.9\r\n0\r\nNiko\r\n02-14-2019\r\nHinge\r\nsky diving\r\n9.9\r\n0\r\nArwin\r\n02-28-2019\r\nBumble\r\nice skating\r\n8.2\r\n0\r\nChad\r\n03-12-2019\r\nTinder\r\nmovie night\r\n4.4\r\n0\r\nOleksiy\r\n03-15-2019\r\nBumble\r\naxe throwing\r\n8.3\r\n0\r\nManuel\r\n03-19-2019\r\nBumble\r\nbike ride\r\n8.9\r\n0\r\nVince\r\n03-27-2019\r\nShip\r\nbarcade\r\n7.3\r\n0\r\nAdamu\r\n04-01-2019\r\nHinge\r\nbotanical garden\r\n8.5\r\n0\r\nRyan\r\n04-21-2019\r\nChristian mingle\r\nmass\r\n7.5\r\n0\r\n\r\n\r\nExpand for the four simple rules of .csv data\r\ncommas delineate columns\r\na new line delineates a new row\r\nthe first row of data determines column names and number of columns\r\nthe values between commas are data\r\nImport from .csv (read.csv())\r\nread.csv() works a lot like readRDS(), except it can interact with .csv files.\r\nBefore you try to import your data, do yourself a favor and open the .csv file in Excel to inspect it.\r\nThe first 10 rows of regdat.csvNow that you’ve done that, note that read.csv() allows the following arguments relevant to importing this lab’s data:\r\nfile = \"\" - just like readRDS, you need to provide a file name in quotes\r\nheader = - this argument takes either TRUE or FALSE. If set to TRUE, the function will use the first row of the data as the column names (“headers”)\r\ncol.names = c() - takes a vector that is the same length as the number of columns in the dataset. It will use the values in this vector as the column names.\r\nHere is an example with filler column names:\r\n\r\n\r\ndata <- read.csv(\"regdat.csv\", \r\n                 header = FALSE, \r\n                 col.names = c(\"columnA\", \"columnB\", \"columnC\", \"columnD\"))\r\n\r\n\r\n\r\nDon’t forget to use <- to assign a name to your imported data frame!\r\n\r\nPlots with {ggplot2}\r\nPlots with {ggplot2}\r\nHistograms (geom_histogram())\r\nIf you need a refresher on creating histograms with {ggplot2}, you can find it here.\r\nScatter plots (geom_point())\r\nTODO?\r\nWe create scatter plots by assigning an x and y aesthetic (aes()) to a ggplot object, and then adding a layer called geom_point()\r\n\r\n\r\n\r\nQ-Q Plots (qqnorm())\r\nA Q-Q plot allows us to compare a variable’s observed distribution against its “theoretical” distribution. It takes a continuous variable, converts each value to a Z-score (using the variable’s mean and standard deviation), and then plots the Z-score against the original observed value.\r\nHere are the first 10 values of regdat$weight, and their accompanying Z-scores\r\n\r\nObserved Values\r\nZ Values\r\n85.0\r\n-0.6979123\r\n105.0\r\n0.1272606\r\n108.0\r\n0.2675710\r\n92.0\r\n-0.4018917\r\n112.5\r\n0.7389858\r\n112.0\r\n0.4482006\r\n104.0\r\n0.0847131\r\n69.0\r\n-1.9916133\r\n94.5\r\n-0.2348232\r\n68.5\r\n-2.0751279\r\n\r\nWe can use qqnorm() to calculate and plot these values. Here is an example using regdat$weight, with histogram for reference:\r\n\r\n\r\nqqnorm(regdat$weight, main = \"Normal Q-Q Plot of Weight (lbs.)\")\r\nhist(regdat$weight)\r\n\r\n\r\n\r\n\r\n\r\nIf weight were normally distributed, we would expect a straight diagonal line. See the note on Q-Q plots below for an example of this.\r\nBonus: qqnorm(), but with ggplot()\r\nFor those interested:\r\nRemember the discussion of how most objects contain more information than initially meets the eye?\r\nThe same goes for qqnorm objects. They aren’t just a bunch of dots. They contain vectors of the theoretical and observed (“Sampled”) quantiles. If we convert the qqnorm object to a data.frame, we can use ggplot() to call those specific columns for plotting:\r\n\r\n\r\nregdat$ln_wt <- log(regdat$weight)\r\n\r\nqqnorm_data <- data.frame(qqnorm(regdat$ln_wt))\r\n\r\nggplot(data = qqnorm_data, aes(x = x, y = y)) +\r\n  geom_point(shape = 1, size = 3) + \r\n  labs(title = \"Q-Q plot for ln(weight)\") +\r\n  xlab(\"Theoretical Quantiles for ln_wt\") +\r\n  ylab(\"Sample Quantiles for ln_wt\")\r\n\r\n\r\n\r\n\r\nNote on Q-Q Plots\r\nFor a quick comparison, we can use a simple model to visualize how a truly normal distribution would appear on a Q-Q plot:\r\nThis page from U Virginia is an excellent resource.\r\nIf we extract the mean and standard deviation from our height, we can construct a normal distribution and plot it.\r\n\r\n\r\nx_hat <- mean(regdat$height)\r\n\r\nx_sd <- sd(regdat$height)\r\n\r\nnormal <- rnorm(500, mean = x_hat, sd = x_sd)\r\n\r\nplot(normal)\r\n\r\nhist(normal)\r\n\r\nqqnorm(normal)\r\n\r\n\r\n\r\n\r\n\r\nCreating new variables\r\nCreating new variables\r\nlog() transform\r\nIn R, log() takes the natural log of a given value:\r\n\r\n\r\nlog(10)\r\n\r\n\r\n[1] 2.302585\r\n\r\n#> [1] 2.302585\r\n\r\n\r\n\r\nIf given a vector of values, log() will take the natural log of each of those values individually:\r\nNot relevant to Lab 01, but you can also specify what base you want your log to be in with the argument base =:\r\n\r\n[1] 0 1 2 3\r\n\r\nWhy do a log transformation?\r\nSay we have a variable whose distribution is right-skewed:\r\n\r\n\r\n\r\n\r\n\r\n\r\nWe can use a natural log transformation to correct for that skew: [[@joe what kind of language would we like to use here?]]\r\n(https://www.datanovia.com/en/lessons/transform-data-to-normal-distribution-in-r/)\r\n\r\n\r\n\r\nSquaring with ^2\r\nIn R, you can raise a base by any exponent using ^. For example, 103 would be calculated like this:\r\n\r\n\r\n10^3\r\n\r\n#> [1] 1000\r\n\r\n\r\n\r\nIf you give R a vector of numbers, followed by an exponent, it will perform the operation on each value individually:\r\n\r\n\r\nsimple_vector <- c(3, 3.3, 12, 1)\r\nsimple_vector^2\r\n\r\n#> [1]   9.00  10.89 144.00   1.00\r\n\r\n\r\n\r\nRecall that data frames are just lists of vectors!\r\nYou might consider using %>% and mutate() to square a variable.\r\nCategorical variables with case_when()\r\nFor a refresher on how to generate categorical variables from continuous variables, you can refer back to when we created magec in last semester’s first lab.\r\n\r\nChecking normality and collinearity\r\nChecking normality and collinearity\r\nTo better understand the measures of skewness and kurtosis, please watch this video.\r\n\r\n\r\nHere is a link for those who would prefer to read about it.\r\nThe parts on skewness and kurtosis start at minute 2:40, but the beginning of the video might provide a welcome review on how to describe different distributions.\r\nskewness() and kurtosis()\r\nIn R, we can calculate both skewness and kurtosis using a package called {moments} and its handy functions skewness() and kurtosis():\r\n\r\n\r\nskewness(regdat$weight)\r\n\r\nkurtosis(regdat$weight)\r\n\r\n#> [1] 0.5843876\r\n#> [1] 3.765937\r\n\r\n\r\n\r\n\r\nYou will need to install {moments} with install.packages(\"moments\")\r\n[[@Joe : need chi2 test of joint skewness-kurtosis\r\nThe instructions for Lab 01 ask for a “Skewness-Kurtosis” test, which, computed in Stata, is a joint computation.\r\nThe Methods and formulas section in the Stata Documentation mentions a Royston adjustment to the D’Agostino omnibus test of normality. We may need to hard-code this calculation in order to get a test statistic similar to that rendered in the Stata output for sktest ln_wt.\r\nHowever, interpreting the raw tests of skewness and kurtosis should suffice for our purposes of assessing for normality. ]]\r\nPearson and Spearman rank correlation coefficients\r\nWe use correlation coefficients to measure the strength and direction of the linear association between two continuous variables. [[@joe: is the phrasing fine here?]]\r\nCorrelation coefficients can range from -1 to 1. Values closer to 1 represent a strong positive correlation. Those closer to -1 represent a strong negative correlation. The closer the correlation coefficient, R, is to 0, the weaker the association between the two variables.\r\n\r\n\r\n\r\n\r\n\r\n\r\nTo calculate in R, use the function cor(x, y, method = c(\"pearson\", \"kendall\", \"spearman\")) Specify your x and y variables and set the method to your cheeky brit of choice.\r\n\r\nThe Pearson is the parametric test. Both Spearman and Kendall refer to rank-based (non-parametric) measures of association for when our data is non-normal.\r\nIf no method is specified, “pearson” is the default.\r\nBonus: Correlation coefficient matrices\r\nIf you want to have a very particular kind of fun, try the following:\r\nRemove all non-continuous variables from your data frame and assign it a new name, like cont_data.\r\nUse R to run cor(cont_data), where the data frame of continuous variable vectors is the only argument.\r\n\r\nYou can use {dplyr}’s select() to remove variables by name. There are many many ways to do this.\r\nWhat did it do? (Go look at Table 1 in the assignment. )\r\n\r\nLinear Regression Models\r\nLinear Regression Models\r\nBuilding models in R is fun!\r\nUnivariate (single-variable) models with lm()\r\nRemember the concept of R formulas?\r\nRegression models make use of formulas in a very intuitive way. Here is the format:\r\n\r\n\r\nlm(response_var ~ predictor_var, data = a_data_frame)\r\n\r\n\r\n\r\nlm stands for “linear model”. If we assign a name to our model, we can access various components within the model, which we can index with $.\r\nWe then use summary() to view an overview of the model’s most important components.\r\n\r\n\r\nm1 <- lm(Sepal.Length ~ Petal.Length, data = iris)\r\nsummary(m1)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = Sepal.Length ~ Petal.Length, data = iris)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-1.24675 -0.29657 -0.01515  0.27676  1.00269 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)   4.30660    0.07839   54.94   <2e-16 ***\r\nPetal.Length  0.40892    0.01889   21.65   <2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.4071 on 148 degrees of freedom\r\nMultiple R-squared:   0.76, Adjusted R-squared:  0.7583 \r\nF-statistic: 468.6 on 1 and 148 DF,  p-value: < 2.2e-16\r\n\r\nMultivariate (multi-variable) models with lm()\r\nYou can also use lm() to program models with multiple linear predictor variables. You can formulate your equation as you build your model, using arithmetic operators on the right-hand side of the formula:\r\n\r\n\r\nlm(response_var ~ predictor_var1 + predictor_var2, data = a_data_frame)\r\n\r\n\r\n\r\n\r\n\r\nmlm1 <- lm(Sepal.Length ~ Petal.Length + Species, data = iris)\r\n\r\nsummary(mlm1)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = Sepal.Length ~ Petal.Length + Species, data = iris)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-0.75310 -0.23142 -0.00081  0.23085  1.03100 \r\n\r\nCoefficients:\r\n                  Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)        3.68353    0.10610  34.719  < 2e-16 ***\r\nPetal.Length       0.90456    0.06479  13.962  < 2e-16 ***\r\nSpeciesversicolor -1.60097    0.19347  -8.275 7.37e-14 ***\r\nSpeciesvirginica  -2.11767    0.27346  -7.744 1.48e-12 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.338 on 146 degrees of freedom\r\nMultiple R-squared:  0.8367,    Adjusted R-squared:  0.8334 \r\nF-statistic: 249.4 on 3 and 146 DF,  p-value: < 2.2e-16\r\n\r\nLinear model objects: a brief tour\r\nIn this section, we’ll show you how to access a model’s coefficients, confidence intervals, residuals, and other values that might come in handy.\r\nRemember how we said that you can extract numbers from your model when you assign it a name? If we inspect m1 using $ to index the object by name, we’ll see some useful items at our disposal.\r\nYou can obtain the model’s coefficients with coef(your_model)\r\n\r\n\r\ncoef(m1)\r\n\r\n\r\n (Intercept) Petal.Length \r\n   4.3066034    0.4089223 \r\n\r\nAnd you can calculate 95% confidence intervals with confint(your_model, level = 0.95).\r\n\r\n\r\nconfint(m1,  level = .95)\r\n\r\n\r\n                 2.5 %    97.5 %\r\n(Intercept)  4.1516972 4.4615096\r\nPetal.Length 0.3715907 0.4462539\r\n\r\nAnd don’t forget that you can inspect your model with summary()\r\n\r\n\r\nsummary(m1)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = Sepal.Length ~ Petal.Length, data = iris)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-1.24675 -0.29657 -0.01515  0.27676  1.00269 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)   4.30660    0.07839   54.94   <2e-16 ***\r\nPetal.Length  0.40892    0.01889   21.65   <2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.4071 on 148 degrees of freedom\r\nMultiple R-squared:   0.76, Adjusted R-squared:  0.7583 \r\nF-statistic: 468.6 on 1 and 148 DF,  p-value: < 2.2e-16\r\n\r\nOur model also contains the columns of data (variables) used in its calculation:\r\n\r\n\r\nhead(m1$model)\r\n\r\n\r\n  Sepal.Length Petal.Length\r\n1          5.1          1.4\r\n2          4.9          1.4\r\n3          4.7          1.3\r\n4          4.6          1.5\r\n5          5.0          1.4\r\n6          5.4          1.7\r\n\r\n\r\nSepal.Length\r\nPetal.Length\r\n5.1\r\n1.4\r\n4.9\r\n1.4\r\n4.7\r\n1.3\r\n4.6\r\n1.5\r\n5.0\r\n1.4\r\n5.4\r\n1.7\r\n\r\nIt also has the resulting fitted values:\r\n\r\n\r\nhead(m1$fitted.values)\r\n\r\n\r\n       1        2        3        4        5        6 \r\n4.879095 4.879095 4.838202 4.919987 4.879095 5.001771 \r\n\r\nThe fitted values are what result when we plug each row into the model equation. We can use the first row of our data to observe this directly:\r\n\r\n\r\nx <- m1$model[1,2] # Petal.Length of row 1\r\n\r\nB0 <- m1$coefficients[1] # beta-naught\r\n\r\nB1 <- m1$coefficients[2] # beta-one\r\n\r\n# formula:\r\nfitted <- B0 + B1 * x\r\nfitted\r\n\r\n\r\n(Intercept) \r\n   4.879095 \r\n\r\nThe resulting value is the “fitted” value– or the estimated value for the response value according to our model, given the value of our predictor variable, 1.4\r\nNotice that our model also contains residuals, or the differences between our observed values for y (our response variable), and the values predicted by our model:\r\n\r\n\r\nhead(m1$residuals)\r\n\r\n\r\n         1          2          3          4          5          6 \r\n 0.2209054  0.0209054 -0.1382024 -0.3199868  0.1209054  0.3982287 \r\n\r\nWe can find the first row’s residual value with the fitted value that we calculated above:\r\n\r\n\r\nm1$model[1,1] - # observed Sepal.Length of row 1\r\n  fitted\r\n\r\n\r\n(Intercept) \r\n  0.2209054 \r\n\r\n\r\nPlotting line of best fit\r\nPlotting line of best fit\r\n{ggplot2} provides a wide variety of flexible tools for visualizing models. There are two main ways we might visualize a regression models:\r\nPlot, then have {ggplot2} calculate line of best fit\r\nBuild model, then plot using model object\r\nHere is a highly useful tutorial for how to plot different models in {ggplot2}. It includes splines, Loess, and multiple variatons on quadratics.\r\nWe will use both throughout the semester, so we will give a brief intro to both methods here.\r\n1. Plot first, then model\r\nTo perform this method, we need to start with a scatter plot using {ggplot2}:\r\n\r\n\r\np <- ggplot(data = toy, aes(x = ran1, y = pos)) +\r\n  geom_point() +\r\n  labs(x = \"Age (years)\", y = \"Points\", title = \"This is fake data\")\r\n\r\np\r\n\r\n\r\n\r\n\r\n\r\nNotice that we have assigned a name to our plot, p. By doing this, we can add layers to the original plot without needing to reproduce the code each time.\r\nOnce we have our plot, we just need to add another layer, stat_smooth(), and designate a method and formula. Here, we’re building a univariate, linear model. Observe how we parameterize these details within the function:\r\n\r\n\r\np + stat_smooth(method = \"lm\", formula = y ~ x)\r\n\r\n\r\n\r\n\r\nNotice that if we don’t designate a method, stat_smooth() will default to plotting a smoothed “loess” line, which we’ll learn more about later in the semester:\r\n\r\n\r\n\r\nFor those curious, we can also print our model’s equation in the plot. The code is a bit dense, but straightforward:\r\n\r\n\r\nrequire(ggpmisc)\r\n\r\np + stat_smooth(method = \"lm\", formula = y ~ x) +\r\n  stat_poly_eq(formula = y ~ x, \r\n                aes(label = paste(..eq.label.., ..rr.label.., sep = \"~~~\")), \r\n                parse = TRUE)\r\n\r\n\r\n\r\n\r\n\r\nFrom this post on SO.\r\n2. Model first, then plot\r\nWe can also plot our data and line of best fit by using a model as the data object required by {ggplot2}.\r\nRemember our example model from earlier:\r\n\r\n\r\nCall:\r\nlm(formula = Sepal.Length ~ Petal.Length, data = iris)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-1.24675 -0.29657 -0.01515  0.27676  1.00269 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)   4.30660    0.07839   54.94   <2e-16 ***\r\nPetal.Length  0.40892    0.01889   21.65   <2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.4071 on 148 degrees of freedom\r\nMultiple R-squared:   0.76, Adjusted R-squared:  0.7583 \r\nF-statistic: 468.6 on 1 and 148 DF,  p-value: < 2.2e-16\r\n\r\nm1 will serve as our data object within ggplot(). We can start by creating a scatter plot of the response and predictor variables from our formula. But instead of using the original data frame (regdat), we’ll use data = m1:\r\n\r\n\r\np2 <- ggplot(data = m1, aes(x = Petal.Length, y = Sepal.Length)) +\r\n  geom_point()\r\n\r\np2\r\n\r\n\r\n\r\n\r\nFrom here, we can add a geom_smooth() or stat_smooth() layer, using method = \"lm\" to fit an ordinary least squares regression line to our data:\r\n\r\n\r\np2 + geom_smooth(method = \"lm\")\r\n\r\n\r\n\r\n\r\nThis second method becomes more important as our models increase in complexity. It also will helps in plotting residuals (see next section).\r\n\r\nModel Evaluation\r\nModel evaluation\r\nOnce we’ve built a model, there are various measures we might use to evaluate how well that model fits our data. Here we discuss two of those measures: residuals and heteroscedasticity.\r\nResiduals plots\r\nOur model object contains a vector of residuals, which are the observed Y minus the predicted Y:\r\n\r\n\r\nhead(m1$residuals)\r\n\r\n\r\n         1          2          3          4          5          6 \r\n 0.2209054  0.0209054 -0.1382024 -0.3199868  0.1209054  0.3982287 \r\n\r\nWe could generate a histogram of our residuals. In the following code, notice that {ggplot2} finds our residuals when we refer to them as .resid:\r\n\r\n\r\nggplot(data = m1, aes(x = .resid)) + \r\n  geom_histogram(binwidth = .1, fill = \"seagreen\") +\r\n  xlab('Residuals') +\r\n  ggtitle(\"Histogram of residuals of Iris Petal Length regressed on Sepal Length\")\r\n\r\n\r\n\r\n\r\nBut we can also use that vector of residuals, .resid to create a scatter plot of our fitted data/observed data (which one?) against our residuals:\r\n\r\n\r\nggplot(data = m1, aes(x = .fitted, y = .resid)) +\r\n  geom_point() +\r\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\r\n  labs(x = \"Fitted values\", \r\n       y = \"Residuals\", \r\n       title = \"Residual plot of iris petal length regressed on sepal length\")\r\n\r\n\r\n\r\n\r\n\r\nNotice that we have added a dashed horizontal line at y = 0 by adding a layer to our plot: + geom_hline(yintercept = 0, linetype = \"dashed\")\r\n\r\n\r\nggplot(data = m1, aes(x = Petal.Length, y = .resid)) +\r\n  geom_point() +\r\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\r\n  labs(x = \"Fitted values\", \r\n       y = \"Residuals\", \r\n       title = \"Residual plot of iris petal length regressed on sepal length\")\r\n\r\n\r\n\r\n\r\nThis tells us if there is any bias in our model’s fit. If we observe a visible trend in our residual plot [TODO]\r\nOne such trend we might observe is heteroskedasticity,\r\nHeteroskedasticity\r\nSimply put, heteroskedasticity is when the variability of Y (our response variable) is different depending on X (our predictor variable).\r\nIn the following plot, is the probability of an accurate prediction of Y higher when X is closer to 0 or 100?\r\n\r\n\r\n\r\n\r\n\r\nggplot(het, aes(x = x, y = z)) +\r\n  geom_point() +\r\n  labs(x = \"X\", y = \"Y\", title = \"More fake data\")\r\n\r\n\r\n\r\n\r\nWikipedia is telling me that this is Greek for “different” (hetero) “dispersion” (skedasis)\r\nThe data above is heteroskedastilisticexpialidocious.\r\nLink, if you don’t get the reference\r\nBreusch-Pagan/CookWeisberg\r\nWe can use a chi-squared test to assess how heteroskedastic our data really is. We can do this using ncvTest() from the package {car}.\r\nHere’s a test of our iris model, whose residuals we visualized above:\r\n\r\n\r\nncvTest(m1)\r\n\r\n\r\nNon-constant Variance Score Test \r\nVariance formula: ~ fitted.values \r\nChisquare = 2.493218, Df = 1, p = 0.11434\r\n\r\n\r\nFor proof of method, see “References” section in documentation\r\nMeanwhile, we’re more certain in the heterskedastilistic nature of the fake data:\r\n\r\n\r\nncvTest(lm(z~x, data = het))\r\n\r\n\r\nNon-constant Variance Score Test \r\nVariance formula: ~ fitted.values \r\nChisquare = 27.79159, Df = 1, p = 1.3511e-07\r\n\r\n\r\nModel comparison\r\nModel comparison\r\nWe can use an F-statistic to compare models when we add or omit variables. Section 9.2 of OpenIntro contains a great introduction to model selection. In it, they recommend two different model selection strategies:\r\nBackwards elimination - where we generate a fully saturated model, then take one variable out at a time.\r\nForward selection - where we start with a single predictor variable, and add one new variable at a time.\r\nWith each step in either of these processes, we need some kind of test to help us evaluate if the removal/addition of a variable has improved the model.\r\nIn OpenIntro, they use R2. But we can also use an F-test.\r\nF-test\r\nUse anova(aModel, anotherModel) to compute the F-statistic for the comparison of two models.\r\nFurther up on this page, we created a model that aimed to predict iris sepal length using measures of iris petal length.\r\nHere’s that model in brief:\r\n\r\n\r\nsummary(m1)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = Sepal.Length ~ Petal.Length, data = iris)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-1.24675 -0.29657 -0.01515  0.27676  1.00269 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)   4.30660    0.07839   54.94   <2e-16 ***\r\nPetal.Length  0.40892    0.01889   21.65   <2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.4071 on 148 degrees of freedom\r\nMultiple R-squared:   0.76, Adjusted R-squared:  0.7583 \r\nF-statistic: 468.6 on 1 and 148 DF,  p-value: < 2.2e-16\r\n\r\nWe may wonder if using a quadratic formula renders a better model:\r\n\r\n\r\nm1_quadratic <- lm(Sepal.Length ~ Petal.Length + I(Petal.Length^2), data = iris)\r\nsummary(m1_quadratic)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = Sepal.Length ~ Petal.Length + I(Petal.Length^2), \r\n    data = iris)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-1.0684 -0.2348  0.0121  0.2049  0.9146 \r\n\r\nCoefficients:\r\n                  Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)        5.05833    0.14036  36.038  < 2e-16 ***\r\nPetal.Length      -0.16435    0.09427  -1.743   0.0834 .  \r\nI(Petal.Length^2)  0.08146    0.01318   6.181 5.96e-09 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.3639 on 147 degrees of freedom\r\nMultiple R-squared:  0.8095,    Adjusted R-squared:  0.8069 \r\nF-statistic: 312.3 on 2 and 147 DF,  p-value: < 2.2e-16\r\n\r\nCompare the two outputs and notice that our Adjusted R-squared has increased. This is a good sign.\r\nBut an F-statistic would also tell us… [[@joe I'll let you explain here]]\r\n\r\n\r\nanova(m1, m1_quadratic, test = \"F\")\r\n\r\n\r\nAnalysis of Variance Table\r\n\r\nModel 1: Sepal.Length ~ Petal.Length\r\nModel 2: Sepal.Length ~ Petal.Length + I(Petal.Length^2)\r\n  Res.Df    RSS Df Sum of Sq      F    Pr(>F)    \r\n1    148 24.525                                  \r\n2    147 19.466  1    5.0593 38.206 5.955e-09 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\n\r\nIf you want to know more about how this test is performed, this article by Danielle Navarro is a great place to look.\r\nThe main thing of importance when performing an F-test on two models is that the smaller of the two models (the “null” model) cannot contain variables that are not in the larger model (the “alternative” or “full” model).\r\n\r\nNested models (task 13, but fancy)\r\nNested models (task 13, but fancy)\r\nIntro\r\nThis section is drawn from Hadley Wickham’s “R for Data Science”. Honestly, Hadley does a much better job. I’m mostly using this to learn it myself.\r\n[[@joe this is probably too much. I would think that we should just keep things simple and have students filter the dataset into male and female, then run the model on separate datasets.]]\r\nFor task 13, you will likely just use %>% and filter() to subset their data by sex, and then run separate regressions on those two separate data frames.\r\nThis is a quick explanation of how to do it at scale (with many categories in a single subsetting variable):\r\nnest()\r\nR allows us to make “nested” datasets according to categorical groupings. We can then use tidyverse functions to generate linear models on each of those subsets in our nested data.\r\nFirst, we need to group and nest our dataset according to a category. In the iris dataset, we could use species:\r\n\r\n\r\niris_by_species <- iris %>%\r\n  group_by(Species) %>%\r\n  nest()\r\n\r\n\r\n\r\nThe printed output of that dataset looks like this:\r\n\r\n# A tibble: 3 x 2\r\n# Groups:   Species [3]\r\n  Species    data             \r\n  <fct>      <list>           \r\n1 setosa     <tibble [50 x 4]>\r\n2 versicolor <tibble [50 x 4]>\r\n3 virginica  <tibble [50 x 4]>\r\n\r\n\r\nThree tibbles in a tibble?\r\nHere’s how we would look at the setosa tibble:\r\n\r\n\r\nhead(iris_by_species$data[[1]])\r\n\r\n\r\n\r\n\r\nSepal.Length\r\nSepal.Width\r\nPetal.Length\r\nPetal.Width\r\n5.1\r\n3.5\r\n1.4\r\n0.2\r\n4.9\r\n3.0\r\n1.4\r\n0.2\r\n4.7\r\n3.2\r\n1.3\r\n0.2\r\n4.6\r\n3.1\r\n1.5\r\n0.2\r\n5.0\r\n3.6\r\n1.4\r\n0.2\r\n5.4\r\n3.9\r\n1.7\r\n0.4\r\n\r\nModels as list-columns\r\nNow that our data frame has been nested, we need a function for fitting models:\r\n\r\n\r\n\r\nWe’ll use purrr::map() to apply that function to each of our nested data frames. By using this along with mutate(), we can add a new column to our nesting data frame that contains the models:\r\n\r\n\r\niris_by_species <- iris_by_species %>%\r\n  mutate(model = map(data, iris_model)) # here, `data` refers to the column in our nested data frame\r\n\r\niris_by_species\r\n\r\n\r\n# A tibble: 3 x 3\r\n# Groups:   Species [3]\r\n  Species    data              model \r\n  <fct>      <list>            <list>\r\n1 setosa     <tibble [50 x 4]> <lm>  \r\n2 versicolor <tibble [50 x 4]> <lm>  \r\n3 virginica  <tibble [50 x 4]> <lm>  \r\n\r\nFinally, we can add the residuals of each model as another column in our nested data:\r\n\r\n\r\nrequire(modelr)\r\niris_by_species <- iris_by_species %>%\r\n  mutate(resids = map2(data, model, add_residuals))\r\n\r\niris_by_species\r\n\r\n\r\n# A tibble: 3 x 4\r\n# Groups:   Species [3]\r\n  Species    data              model  resids           \r\n  <fct>      <list>            <list> <list>           \r\n1 setosa     <tibble [50 x 4]> <lm>   <tibble [50 x 5]>\r\n2 versicolor <tibble [50 x 4]> <lm>   <tibble [50 x 5]>\r\n3 virginica  <tibble [50 x 4]> <lm>   <tibble [50 x 5]>\r\n\r\nUnnesting\r\nTo actually work with the results of our model, we need to unnest() it:\r\n\r\nSpecies\r\nSepal.Length\r\nSepal.Width\r\nPetal.Length\r\nPetal.Width\r\nresid\r\nsetosa\r\n5.1\r\n3.5\r\n1.4\r\n0.2\r\n-0.0743734\r\nsetosa\r\n4.9\r\n3.0\r\n1.4\r\n0.2\r\n-0.0480470\r\nsetosa\r\n4.7\r\n3.2\r\n1.3\r\n0.2\r\n-0.1217207\r\nsetosa\r\n4.6\r\n3.1\r\n1.5\r\n0.2\r\n0.0914425\r\nsetosa\r\n5.0\r\n3.6\r\n1.4\r\n0.2\r\n-0.0612102\r\nsetosa\r\n5.4\r\n3.9\r\n1.7\r\n0.4\r\n0.1861371\r\n\r\nDo these residuals look different than our previous model of residuals?\r\n\r\n\r\nggplot(data = m1, aes(x = Petal.Length, y = .resid)) +\r\n  geom_point() +\r\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\r\n  labs(x = \"Fitted values\", y = \"Residuals\", title = \"Residual plot of iris petal length regressed on sepal length\", subtitle = \"(Original model)\")\r\n\r\n\r\n\r\n\r\n\r\n\r\nggplot(data = resids, aes(x = Petal.Length, y = resid)) +\r\n  geom_point() +\r\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\r\n  labs(x = \"Petal.Length\", y = \"Residuals\", title = \"Residual plot of iris petal length regressed on sepal length\", subtitle = \"(nested models by species)\")\r\n\r\n\r\n\r\n\r\nWhat about the residuals of a model with a categorical predictor term for species?\r\n\r\n\r\nm1_species <- lm(Sepal.Length ~ Petal.Length + Species, data = iris)\r\n\r\n\r\n\r\n\r\n\r\nggplot(data = m1_species, aes(x = Sepal.Length, y = .resid)) +\r\n  geom_point() +\r\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\r\n  labs(x = \"Petal.Length\", y = \"Residuals\", title = \"Residual plot of iris petal length regressed on sepal length\", subtitle = \"With species predictor variable\")\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2021-11-16T11:04:22-05:00"
    },
    {
      "path": "lab_1.html",
      "title": "Lab 1 (705)",
      "description": "__Due:__ October 1, 2021 by 14:00 ET\n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\nLAB MATERIALS\r\nLab 1 Goals\r\nLab 2 Grading scheme\r\nTask 1: Load packages and data\r\nTask 2: Recode variables\r\nTask2a: bord5\r\nTask 2b: male\r\nTask 2c: mweight\r\nTask 2d: mheight\r\nTask 2e: mbmi\r\n\r\nTask 3: Frequency histograms\r\nCreating histograms in {ggplot2}\r\n\r\nTask 4: Boxplots\r\nCreating a boxplot in {ggplot2}\r\n\r\nTask 5: Frequency table (Table 1)\r\nCross-tabulation with group_by() and summarize()\r\n\r\nTask 6: Data dictionary (Table 2)\r\nTask 7: Short answer\r\n\r\n\r\n\r\n\r\nLAB MATERIALS\r\nR Markdown file for Lab 1 Click link to download. Fill it in with your answers to the following lab tasks. When you’re ready to submit, name it as Lab1_FirstinitialYourlastname.Rmd, and submit it using the Sakai dropbox.\r\nExcel document for Tables 1 & 2\r\nLab_1_kenya.rds - data file available on Sakai\r\nLab 1 Goals\r\nGenerate derived variables\r\nIdentify and recode special values\r\nRun descriptive statistics for continuous and categorical variables\r\nGenerate graphics for continuous variables\r\nGenerate a complete data dictionary\r\nLab 2 Grading scheme\r\nCompetency\r\nPoints\r\n.Rmd file runs without error\r\n10\r\nTask 3 (Histograms)\r\n15 (5 each)\r\nTask 4 (Boxplots)\r\n15 (5 each)\r\nTask 5 (Table 1)\r\n10\r\nTask 6 (Data Dictionary - original variables)\r\n20\r\nTask 6 (Data Dictionary - new variables)\r\n10\r\nTask 7 (short answer)\r\n20\r\nTotal\r\n100\r\nTask 1: Load packages and data\r\nIn the chunk of code named load packages, use the function library() to load packages {skimr} and {tidyverse}.\r\nIn the following chunk, named load data, use the function readRDS() to load in the dataset named Lab_1_kenya.rds. Use the assignment operator, <- to give it the name kenya.\r\nReminder: To run a line of code from within a code chunk in R Markdown, use the keyboard shortcut ctrl + enter (Windows) / cmd + enter (Mac)\r\n\r\nFind more useful keyboard shortcuts here\r\nTask 2: Recode variables\r\nSimilar to Lab 0, construct the following derived variables (Tasks 2a - 2e) using a pipe (%>%) and mutate(). For each, examine the component variables for coded special values (i.e. “NA”) and be sure to set derived variable values appropriately. For categorical variables, you will be expected to use case_when() to create conditional clauses, just as you did in Lab 0 to derive magec.\r\nSome helpful links:\r\nUsing pipes and mutate() in the previous lab\r\nUsing case_when() to derive magec\r\nMore on pipes\r\nMore on factors\r\nFor the categorical variables: Label all variables and the coded values for the categorical ones using the factor() command. That is, add meaningful labels to both the variable name itself and to the levels (i.e. categories) of the categorical variable to make printed output more readable.\r\nIn the .Rmd named lab1_705_fall2021.Rmd, we have provided you with an example of the correct coding of bord5. Please use this to assist you as you re-code the other variables that follow. Please code each new variable within the code chunk named after that variable.\r\nTask2a: bord5\r\nbord5: A dichotomous categorical (i.e. binary) variable indicating birth order of the child. You will use the variable bord (birth order). Create the categories for bord5 according to the following levels and labels:\r\nLevel\r\nLabel\r\n0\r\nbirth order 1 through 4\r\n1\r\nbirth order 5+\r\nTask 2b: male\r\nmale: A dichotomous categorical (i.e. binary) variable indicating that the child is male. Based on variable b4. Create the categories for male according to the following levels and labels:\r\nLevel\r\nLabel\r\n0\r\nfemale\r\n1\r\nmale\r\nTask 2c: mweight\r\nmweight: Continuous variable for maternal weight at time of interview (in kilograms). Based on variable v437. Note that v437 contains 1 implied decimal place. Divide by 10 to get kilograms.\r\nTask 2d: mheight\r\nmheight: Continuous maternal height at time of interview (in meters). Based on variable v438. Note that this variable is in centimeters and also contains 1 implied decimal place. Divide by 10 to get meters.\r\nTask 2e: mbmi\r\nmbmi: maternal body mass index (BMI). mweight / mheight^2.\r\nTask 3: Frequency histograms\r\nGenerate frequency histograms of the continuous variables mweight, mheight and mbmi using {ggplot2}. Put meaningful axis labels and a title on each figure. Note that since we look at histograms of each of the variables individually, these are univariable visualizations (i.e. one variable at a time).\r\nPlease read the description below on how to generate histograms in {ggplot}, then use the code chunks named mweight hist, mheight hist, and mbmi hist to generate each plot in its own designated chunk.\r\nCreating histograms in {ggplot2}\r\nThe code for creating histograms using ggplot is fairly straightforward, and we will demonstrate using the starwars dataset. It provides a list of Star Wars characters, with details on their height, weight, gender, home world, and so on:\r\nstarwars is loaded into R every time you load the rest of the tidyverse package.\r\n\r\nname\r\nheight\r\nmass\r\nhair_color\r\nskin_color\r\neye_color\r\nbirth_year\r\nsex\r\ngender\r\nhomeworld\r\nspecies\r\nLuke Skywalker\r\n172\r\n77\r\nblond\r\nfair\r\nblue\r\n19.0\r\nmale\r\nmasculine\r\nTatooine\r\nHuman\r\nC-3PO\r\n167\r\n75\r\nNA\r\ngold\r\nyellow\r\n112.0\r\nnone\r\nmasculine\r\nTatooine\r\nDroid\r\nR2-D2\r\n96\r\n32\r\nNA\r\nwhite, blue\r\nred\r\n33.0\r\nnone\r\nmasculine\r\nNaboo\r\nDroid\r\nDarth Vader\r\n202\r\n136\r\nnone\r\nwhite\r\nyellow\r\n41.9\r\nmale\r\nmasculine\r\nTatooine\r\nHuman\r\nLeia Organa\r\n150\r\n49\r\nbrown\r\nlight\r\nbrown\r\n19.0\r\nfemale\r\nfeminine\r\nAlderaan\r\nHuman\r\nOwen Lars\r\n178\r\n120\r\nbrown, grey\r\nlight\r\nblue\r\n52.0\r\nmale\r\nmasculine\r\nTatooine\r\nHuman\r\n\r\nSay we want to use ggplot to create a histogram of character height. Where do we start?\r\n“gg” stands for Grammar of graphics. {ggplot2} allows us to follow simple rules, a “grammar”, to construct visually appealing graphics and plots.\r\nRegardless of what kind of visualization we want to create, the first step will always be to initialize a graphical field using the command ggplot(). We supply this function with our dataset using the argument data =.\r\nWe can also supply it with universal “mapping aesthetics”. What does that mean? Generally speaking, “aesthetics” are the different variables we want to use in our plot. Eventually, we are going to add “layers” to the plot. A universal aesthetic just means that we want the variable to apply to ALL layers. In the case of our histogram, we want height to always be on the x-axis. So we will designate height as a universal aesthetic. We designate aesthetics by putting them inside the aesthetic function, aes().\r\n\r\n\r\nggplot(data = starwars, mapping = aes(x = height))\r\n\r\n\r\n\r\n\r\nNotice how the above plotting field is initialized, with character height mapped to the x axis. Luckily, histograms only take an x aesthetic, so we can generate our histogram by simply adding a layer using geom_histogram(). {ggplot2} uses a plus sign, + to add layers to a plot. Here, we will add a histogram layer and a labels layer (labs())\r\n\r\n\r\nggplot(data = starwars, mapping = aes(x = height)) +\r\n  geom_histogram() +\r\n  labs(x = \"Height (cm)\", y = \"Count\", title = \"Histogram of Star Wars Character Height (cm)\")\r\n\r\n\r\n\r\n\r\nNavigate to this page for {ggplot2} resources. The page also includes our own tutorial that details the concepts specifically required to complete these labs.\r\nPlease pay special attention to how you label your visualizations, and be specific! Statisticians and others reviewing your research tend to be quite persnickety when it comes to titles and axis labels. And with good reason! What use is a visualization without proper labels?\r\nFind many an anecdotal answer on r/dataisugly\r\nAs time permits, we encourage you to take advantage of the external resources provided in the sidebar in order to gain a better understanding of plotting with {ggplot2}\r\nTask 4: Boxplots\r\nNow you’re going to generate boxplots of mweight, mheight, and mbmi for the levels of magec. Put meaningful axis labels and a title on each figure. Note that since we look at boxplots of each of the variables according to the levels of magec, these are bivariable visualizations (i.e. showing two variables at a time).\r\nPlease use the appropriately labelled code chunks provided in your R Markdown assignment document (labelled mweight box, mheight box, etc.) to generate your boxplots.\r\nCreating a boxplot in {ggplot2}\r\nThe steps to creating a boxplot are the same as those for creating a histogram, except we will create the boxplot layer using geom_boxplot() instead of geom_histogram(). Additionally, in order to stratify our plot, we will need to provide the plotting space with a y aesthetic, magec.\r\nIf you need a little more guidance, learn to create a boxplot here\r\nAnd learn how to create a stratified boxplot here\r\nTask 5: Frequency table (Table 1)\r\nIf you haven’t already, download the Excel file that’s linked at the top of this webpage. Then open the file, which is named Lab1_Tables.xlsx.\r\nFill in the sheet called Table 1 with the frequency counts and percentages for the levels of the 3 categorical variables you have generated. Calculate percentages only for the non-missing values. Round percentages to 1 decimal place.\r\nIn the code chunks named bord5 freq, etc., we would like you to use a pipe (%>%) with group_by(), summarize(), and mutate() to generate summary statistics. Please read the tutorial below to gain a better understanding of group_by() and summarize().\r\nHint: To generate percentages based on sub-group frequency counts, you will need to use mutate() to create another column on your summary data frame that divides each frequency count by the sum() of each subgroup’s total count.\r\nCross-tabulation with group_by() and summarize()\r\ngroup_by() works by grouping rows into discrete categories based on categorical variables in the data frame. It then allows a function like summarize() to calculate summary statistics on those sub-groups, including percentages and frequency counts.\r\nAs an example, we can use the mtcars dataset, which is always available from within R:\r\n\r\n\r\nhead(mtcars)\r\n\r\n\r\n\r\n\r\n\r\nmpg\r\ncyl\r\ndisp\r\nhp\r\ndrat\r\nwt\r\nqsec\r\nvs\r\nam\r\ngear\r\ncarb\r\nMazda RX4\r\n21.0\r\n6\r\n160\r\n110\r\n3.90\r\n2.620\r\n16.46\r\nV-shape\r\nmanual\r\n4\r\n4\r\nMazda RX4 Wag\r\n21.0\r\n6\r\n160\r\n110\r\n3.90\r\n2.875\r\n17.02\r\nV-shape\r\nmanual\r\n4\r\n4\r\nDatsun 710\r\n22.8\r\n4\r\n108\r\n93\r\n3.85\r\n2.320\r\n18.61\r\nstraight\r\nmanual\r\n4\r\n1\r\nHornet 4 Drive\r\n21.4\r\n6\r\n258\r\n110\r\n3.08\r\n3.215\r\n19.44\r\nstraight\r\nautomatic\r\n3\r\n1\r\nHornet Sportabout\r\n18.7\r\n8\r\n360\r\n175\r\n3.15\r\n3.440\r\n17.02\r\nV-shape\r\nautomatic\r\n3\r\n2\r\nValiant\r\n18.1\r\n6\r\n225\r\n105\r\n2.76\r\n3.460\r\n20.22\r\nstraight\r\nautomatic\r\n3\r\n1\r\n\r\n\r\nFind out what other datasets are available within R by running the command library(help = \"datasets\")\r\nSay we want to know the frequency counts of cars with 4, 6, and 8 cylinders (variable: cyl) based on whether or not a car has an automatic or manual transmission (variable am, 0 = automatic, 1 = manual).\r\nWe can use group_by() to create subgroups according to am and cyl. Using group_by() by itself doesn’t alter the appearance of our data frame, but our observations will now be grouped implicitly according to transmission type and number of cylinders.\r\n\r\n\r\nmtcars %>%\r\n  group_by(am, cyl)\r\n\r\n\r\n\r\n\r\nmpg\r\ncyl\r\ndisp\r\nhp\r\ndrat\r\nwt\r\nqsec\r\nvs\r\nam\r\ngear\r\ncarb\r\n21.0\r\n6\r\n160\r\n110\r\n3.90\r\n2.620\r\n16.46\r\nV-shape\r\nmanual\r\n4\r\n4\r\n21.0\r\n6\r\n160\r\n110\r\n3.90\r\n2.875\r\n17.02\r\nV-shape\r\nmanual\r\n4\r\n4\r\n22.8\r\n4\r\n108\r\n93\r\n3.85\r\n2.320\r\n18.61\r\nstraight\r\nmanual\r\n4\r\n1\r\n21.4\r\n6\r\n258\r\n110\r\n3.08\r\n3.215\r\n19.44\r\nstraight\r\nautomatic\r\n3\r\n1\r\n18.7\r\n8\r\n360\r\n175\r\n3.15\r\n3.440\r\n17.02\r\nV-shape\r\nautomatic\r\n3\r\n2\r\n18.1\r\n6\r\n225\r\n105\r\n2.76\r\n3.460\r\n20.22\r\nstraight\r\nautomatic\r\n3\r\n1\r\n\r\nWe might use summarize() to operate on these sub-groups. summarize() works with the following syntax:\r\n\r\n\r\nmtcars %>%\r\n  group_by(am, cyl) %>%\r\n  summarize(columnName = Function(variable), \r\n            anotherColumnName = anotherFunction(variable),\r\n            etc., etc....)\r\n\r\n\r\n\r\n\r\nIf you prefer the British English spelling of “summarise”, R also recognizes summarise().\r\nDepending on whether our variable of interest is categorical or continuous, we can use summary statistic functions like n() (for raw counts), mean(), sd(), median(), IQR(), min(), and max() within our summarize() function. Except for certain functions like n(), which doesn’t take any arguments, the rest of these functions take a variable name specifying which variable should be used in the calculation.\r\nWe can start by using n() for sub-group frequency counts. Notice that, similar to the syntax used with mutate(), we name the summary statistic to the left of the equals sign, and specify the function to the right.\r\n\r\n\r\nmtcars %>%\r\n  group_by(am, cyl) %>%\r\n  summarize(Counts = n())\r\n\r\n\r\n\r\n\r\nam\r\ncyl\r\nCounts\r\nautomatic\r\n4\r\n3\r\nautomatic\r\n6\r\n4\r\nautomatic\r\n8\r\n12\r\nmanual\r\n4\r\n8\r\nmanual\r\n6\r\n3\r\nmanual\r\n8\r\n2\r\n\r\nWe can also use mutate() to operate on our summary table as if it were its own data frame.\r\nWithin mutate(), we create a new variable with NewVariable =, and equate it to the following operation: Counts / sum(Counts, na.rm = TRUE) (sub-group counts divided by the sum of sub-group counts).\r\nNote: na.rm = TRUE instructs the function to remove NA values from the sum of values in our sub-group.\r\n\r\n\r\nmtcars %>%\r\n  group_by(am, cyl) %>%\r\n  summarize(Counts = n()) %>%\r\n  mutate(Proportions = Counts / sum(Counts, na.rm = TRUE))\r\n\r\n\r\n\r\n\r\nam\r\ncyl\r\nCounts\r\nProportions\r\nautomatic\r\n4\r\n3\r\n0.1578947\r\nautomatic\r\n6\r\n4\r\n0.2105263\r\nautomatic\r\n8\r\n12\r\n0.6315789\r\nmanual\r\n4\r\n8\r\n0.6153846\r\nmanual\r\n6\r\n3\r\n0.2307692\r\nmanual\r\n8\r\n2\r\n0.1538462\r\n\r\nAlso notice that we’re able to do this all within a single string of code using %>%!\r\nTask 6: Data dictionary (Table 2)\r\nIn your Excel spreadsheet, the sheet titled Table 2 is our own data dictionary for the Kenya DHS dataset, with columns added to annotate the variables and provide summary statistics. Using output from skim():\r\nFill in the columns in this table for the variables originally in the dataset.\r\nAdd rows for the 7 new variables that you have created in both Labs 0 and 1.\r\nTask 7: Short answer\r\nEnter your response into your own RMarkdown file, under the heading called “Task 7: Short answer”\r\nPrompt: Examine the range and proportion of missing values for each of the 7 variables you have created in Labs 0 and 1. Are there characteristics of any of these variables that are concerning (e.g., missing, suspicious or impossible values)? In contemplating analysis of these data, what do you think should be done with anomalous information? What effect would missing values have on the validity of your analyses (e.g., how might missing or extreme values affect inferences)? (Response no longer than 250 words, please)\r\n\r\n\r\n\r\n",
      "last_modified": "2021-11-16T11:04:24-05:00"
    },
    {
      "path": "lab_2.html",
      "title": "Lab 2 (705)",
      "description": "__Due:__ October 22, 2021 by 14:00 ET\n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\nLAB MATERIALS\r\nLab 2 Goals\r\nLab 2 Grading scheme\r\nTask 1: Load libraries and data\r\nTask 2: Code derived variables\r\nReference and index levels of a categorical variable\r\nTask 2a: size\r\nTask 2b: belowavg\r\nTask 2c: pnc\r\nTask 2d: rural\r\nTask 2e: education\r\nTask 2f: death\r\nTask 2g: time\r\n\r\nTask 3: Close the cohort\r\nTask 3a: Exclude irrelevant observations\r\nTask 3b: Re-code time\r\nTask 3c: Recode time to [time + 1]\r\nTask 3d: Recode death\r\nTask 3e: Save new dataset\r\n\r\nTask 4: Frequency histograms (using CLOSED COHORT)\r\nTask 5: Boxplots of time (using CLOSED COHORT)\r\nTask 6: Frequency counts and percentages (using CLOSED COHORT)\r\nTask 7: Update data dictionary (using FULL COHORT)\r\nTask 8: Short Answer\r\n\r\n\r\n\r\n\r\nLAB MATERIALS\r\nR Markdown file for Lab 2 Click link to download. Fill it in with your answers to the following lab tasks. When you’re ready to submit, name it as Lab2_FirstinitialYourlastname.Rmd, and submit it using the Sakai dropbox.\r\nExcel document for Tables 1 & 2\r\nLab_2_kenya.rds - data file available on Sakai\r\nLab 2 Goals\r\nGenerate derived variables\r\nIdentify and recode special values\r\nCreate a closed cohort\r\nGenerate graphics for continuous variables\r\nRun descriptive statistics for continuous and categorical variables\r\nGenerate a complete data dictionary\r\nLab 2 Grading scheme\r\nCompetency\r\nPoints\r\n.Rmd file runs without error\r\n10\r\nTask 4 (Histograms)\r\n15 (5 each)\r\nTask 5 (Boxplots)\r\n15 (5 each)\r\nTask 6 (Table 1)\r\n20\r\nTask 7 (Data Dictionary - old variables)\r\n5\r\nTask 7 (Data Dictionary - new variables)\r\n10\r\nTask 8 (short answer)\r\n25\r\nTotal\r\n100\r\n\r\nLate policy: 5 point deduction per 24 hour period past due date and time\r\nTask 1: Load libraries and data\r\nINSTRUCTIONS: For this assignment, use the dataset that we have supplied you in the Sakai Resources tab – Lab_2_kenya.rds.\r\n\r\nFind it under Resources > Lab > Datasets > Lab_2_kenya.rds\r\nFor packages, you will need {tidyverse} and {skimr}\r\nTask 2: Code derived variables\r\nINSTRUCTIONS: Construct the 7 derived variables listed below using %>% and mutate(). Use case_when() when you need your new variable to assign values on a conditional basis.\r\nFor each new variable, be sure to examine the component variables for missing values (i.e. NA) and be sure to set derived variable values appropriately (e.g., check to make sure that your NA values have not been mistakenly added to another category).\r\nAlso be sure that variables are of the correct type. Convert all categorical variables to factors with factor(), and label them appropriately. Please code each new variable within the code chunk named after that variable.\r\nReference and index levels of a categorical variable\r\nThis distinction between reference and index levels of a categorical variable will become important as we move into calculating measures of association like absolute risk, risk differences, risk ratios, and odds ratios.\r\nWe will go into this in greater depth in forthcoming lab assignments, but for now it is simply important to recognize that a “reference level” is the designated “baseline” against which other levels of a categorical variable (our “index levels”) are being compared.\r\nIn these labs, we have asked you to code categorical variables in such a way that the reference level of those variables is coded as 0, and thus takes on the lowest level when it’s converted to a factor categorical variable. R automatically recognizes the lowest level of any factor variable as that variable’s reference group, against which characteristics of participants in other levels of that variable are compared.\r\nIn terms of two-by-two contingency tables, index and reference levels are often presented like this:\r\n\r\nOutcome A \r\nOutcome B\r\n\r\nExposure\r\nDisease\r\nNo Disease\r\nTotal\r\nIndex group\r\nA1\r\nB1\r\nN1\r\nReference group\r\nA0\r\nB0\r\nN0\r\nThe following equations then apply, paying special attention to the location of the reference group:\r\nRisk (Index)\r\n\\(R_1 = A_1 / N_1\\)\r\nRisk (Reference)\r\n\\(R_0 = A_0 / N_0\\)\r\nRisk Difference\r\n\\(RD = R_1 - R_0\\)\r\nRisk Ratio\r\n\\(RR = R_1 / R_0\\)\r\nIncidence Odds Ratio\r\n\\(IOR = \\frac{A_1}{B_1} \\div \\frac{A_0}{B_0}\\)\r\nTask 2a: size\r\nsize: Categorical variable describing size of child at birth (subjectively described by mother). Use variable m18 to code the new variable with the following levels and labels (note that greater values indicate smaller size):\r\nLevel\r\nLabel\r\n0\r\nvery large\r\n1\r\nlarger than average\r\n2\r\naverage\r\n3\r\nsmaller than average\r\n4\r\nvery small\r\nTask 2b: belowavg\r\nbelowavg: Dichotomous variable indicating if the child’s size was below average. Use the newly derived variable size to code the new variable with the following levels and labels:\r\nLevel\r\nLabel\r\n0\r\naverage, larger than average, or very large\r\n1\r\nsmaller than average or very small\r\nTask 2c: pnc\r\npnc: Dichotomous categorical variable for any prenatal care. Use the variable m2n to code the new variable with the following levels and labels.\r\nLevel\r\nLabel\r\n0\r\nno prenatal care (reference)\r\n1\r\nreceived prenatal care (index)\r\nTask 2d: rural\r\nrural: Dichotomous indicator of rural residence. Use the variable v025 to code the new variable with the following levels and labels:\r\nLevel\r\nLabel\r\n0\r\nurban\r\n1\r\nrural\r\nTask 2e: education\r\neducation: Categorical educational level obtained by mother. Use the variable s109 to code the new variable with the following levels and labels.\r\nLevel\r\nLabel\r\n0\r\ndid not attend school\r\n1\r\nprimary school only\r\n2\r\npost-primary education\r\nTask 2f: death\r\ndeath: Dichotomous categorical variable for whether or not a child was dead or alive at the time of interview. Based on the variables b5, code death with the following levels and labels:\r\nLevel\r\nLabel\r\n0\r\nAlive at interview (reference)\r\n1\r\nDead at interview (index)\r\nTask 2g: time\r\ntime: Continuous variable for the age at death OR age at interview for children still alive.\r\nYou will use case_when() to create this derived variable according to the following conditions:\r\nIf a child was dead at the time of interview, assign it the value in variable b7 (child’s age at death).\r\nIf a child was alive at the time of interview, calculate their age using using v008 (date of interview) and b3 (date of birth). Be sure to note that times are given in months.\r\nTask 3: Close the cohort\r\nINSTRUCTIONS: Create the equivalent of a closed cohort for the analysis of 5-year childhood mortality from the dataset.\r\nNote: “Closed cohorts” is a topic that we will cover in more detail in Module 7 (week 11, the first week of November), so please don’t worry if this feels unfamiliar. The steps needed to complete this task are described in detail below. We hope that performing the actual task of closing a cohort before we learn about it formally in Module 7 will help to crystallize concepts around Populations, Time, Measures of Disease Frequency and Association.\r\nFor now, we will stick with the following description:\r\nIn a closed cohort, everyone must be at risk of the outcome at entry into the cohort, and all members of the cohort must remain at risk until they experience the outcome or complete the entire risk period for the outcome.\r\nHere is an example of the various logical outcomes around which we will be closing the cohort:\r\nClosed cohortA closed cohort allows for estimation of absolute (unconditional) risks and associated effect measures. For this study, the risk period for child mortality begins at birth and ends when a child dies, or when 60 months of life have been completed. I recommend completing the steps below in order to generate what we want. There are other orders in which to do this, but sometimes you will get incorrect results. If you’re adventurous, we recommend trying other ways to code this; you’ll learn a lot from that exercise.\r\nANOTHER NOTE: Complete all of task 3 before completing the remaining tasks.\r\nTask 3a: Exclude irrelevant observations\r\nINSTRUCTIONS: For this task, you will use %>% and filter() to exclude those observations from the dataset where the child was alive (death == 0) and follow-up time was less than 60 months (time < 60) Think about which Boolean operator you’ll need to use to exclude those children on both of those conditions. You should end up with 16,828 records (i.e. rows) in the new data frame.\r\nIn this step, you should also use an assignment operator (<-) to assign this closed cohort to a different object name (effectively creating a “new” data frame). That way, you will be able to work with this data frame separately, while maintaining access to the original cohort of n = 22,534.\r\nTask 3b: Re-code time\r\nINSTRUCTIONS: If a child was still alive by their 5th birthday, we need to re-code their variable for time to 60 (we are effectively censoring these observations). These children were alive at 5 years of age and so should not be counted as deaths in our analysis.\r\nYou will do this in your newly closed cohort data frame using mutate() to re-generate time according to the following conditions:\r\nIf time >= 60, then set time to 60\r\nOtherwise, use the original coding for time.\r\nHint: In case_when() you can use the statement TRUE ~ time at the end of your list of conditional statements to express this sentiment of “otherwise, use time”.\r\nTask 3c: Recode time to [time + 1]\r\nINSTRUCTIONS: For this task, re-code time to indicate the month of life during which death or interview occurred (add 1 month to the current value). The range on time should now be 1 month – 61 months.\r\nTo help understand why we might do this, imagine that the interview occurred in the same month that a child was born. For them, time is coded as 0. But for the purposes of analysis, that child’s age should be considered as 1 month. Every observation for time contains this error, which is why we must add 1 to time.\r\n\r\nLinking to our in-class learning, we would formally identify this as a type of “systematic error”\r\nJust like in 3b, you will use mutate() to simply re-generate time with the equation time + 1.\r\nTask 3d: Recode death\r\nNow that time is re-coded, we have some conflicting information between the variables death and time. Consider this question: since we are only considering the mortality status of children before the age of 5, should children who died after 60 months be coded as dead or alive?\r\nThe answer: They are technically alive within the time frame of our analysis\r\nINSTRUCTIONS: Re-code death so that when time >= 61, death == \"alive\".\r\nTask 3e: Save new dataset\r\nSave this new dataset containing 16,828 records (make sure to give it a new name).\r\nJust as in lab 0, use saveRDS() to complete this task, using the following syntax:\r\n\r\n\r\nsaveRDS(dataName, file = \"NewDatasetName.rds\")\r\n\r\n\r\n\r\nTask 4: Frequency histograms (using CLOSED COHORT)\r\nGenerate frequency histograms of time, overall and separately for levels of death. Use labs() to put meaningful axis labels and a title on each figure.\r\nTip: You can filter your data and create a visualization in a single pipeline by connecting your filter() command to ggplot() with %>%. As a quick example, we will use the starwars dataset to create a histogram of character heights. But say we only want to see the distribution of Human heights. We could do it like this:\r\n\r\n\r\nstarwars %>%\r\n  filter(species == \"Human\") %>%\r\n  ggplot(aes(x = height)) +\r\n  geom_histogram(binwidth = 10, fill = '#BBAC12')\r\n\r\n\r\n\r\n\r\n\r\nPlease note that in the ggplot() command, we do not need to supply the data = argument. Your pipe, %>% as already taken care of that behind-the scenes.\r\nOn the other hand, if you wanted to generate a series of stratified histograms according to a variable like gender, you could instead use facet_wrap():\r\n\r\n\r\nggplot(data = starwars, aes(x = height)) +\r\n  geom_histogram(binwidth = 20, fill = \"#AC12BB\") +\r\n  facet_wrap(. ~ gender)\r\n\r\n\r\n\r\n\r\nHelp with histograms in ggplot2\r\nTask 5: Boxplots of time (using CLOSED COHORT)\r\nSimilar to your histograms, generate boxplots of time, overall and separately for levels of death. Use labs() to put meaningful axis labels and a title on each figure.\r\nHelp with boxplots in ggplot2\r\nTask 6: Frequency counts and percentages (using CLOSED COHORT)\r\nIf you haven’t already, download the Excel file that’s linked at the top of this webpage. Then open the file, which is named Lab2_Tables.xlsx.\r\nFill in the sheet named Table 1 with the frequency counts and percentages for the levels of the new categorical variables you have generated. Calculate percentages only for the non-missing values. Round percentages to 1 decimal place.\r\nRefer to the previous lab for help on frequency counts and percentages.\r\nVariables:\r\nSize at birth (size)\r\nSize at birth categorical (belowavg)\r\nPrenatal care (pnc)\r\nResidence type (rural)\r\nMother’s education (education)\r\nDeath by 5 years (death)\r\nTask 7: Update data dictionary (using FULL COHORT)\r\nUsing the second sheet in the Excel file, which has been named Table 2, update your data dictionary for the Kenya dataset by adding the newly created variables.\r\nTask 8: Short Answer\r\nExamine the range and proportion of missing for each of the variables 7 you have created in this lab. Are there characteristics of any of these variables that are concerning (e.g., missing, suspicious or impossible values)? In contemplating analysis of these data, what do you think should be done with anomalous information? What effect would missing values have on the validity of your analyses (e.g., how might missing or extreme values affect inferences)?\r\nProvide your answer to this question within your own RMarkdown file\r\n\r\n\r\n\r\n",
      "last_modified": "2021-11-16T11:04:26-05:00"
    },
    {
      "path": "lab_3.html",
      "title": "Lab 3 (705)",
      "description": "__Due:__ November 12, 2021 by 14:00 ET\n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\nLAB MATERIALS\r\nLab 3 Goals\r\nLab 3 Grading scheme\r\nTask 1: Load libraries and dataset\r\nTask 2: BIG PICTURE\r\nTask 3: Table 1\r\nTabular analysis of continuous variables by mortality status\r\nOverall summary statistics for each variable variable\r\nStratified summary statistics\r\nT-tests and Kruskal-Wallis tests\r\n\r\nTask 4: Table 2\r\nTabular analysis of categorical variables by death\r\n{tableone} package\r\nCreate a vector of variables\r\nConstruct {tableone} objects\r\nPrint table\r\nFisher’s exact test of significance\r\n\r\nTask 5: Table 3\r\nRisk Differences, Risk Ratios, and Odds Ratios\r\nTake a deep breath.\r\nFrequency counts\r\nRisk\r\nRisk difference (RD), Risk Ratio (RR), and Odds Ratio (OR)\r\nmAssoc() for Measures of Association\r\n\r\nTask 6: Side-by-side boxplots\r\nTask 7: Short answer\r\nTask 8: Short answer\r\nTask 9: Short answer\r\nTask 10: Short answer\r\nTask 11: Short answer\r\n\r\n\r\n\r\n\r\nLAB MATERIALS\r\nR Markdown file for Lab 3 Click link to download. Fill it in with your answers to the following lab tasks. When you’re ready to submit, name it as Lab3_FirstinitialYourlastname.Rmd, and submit it using the Sakai dropbox.\r\nExcel document with Tables 1, 2, and 3\r\nLab_3_kenya.rds - data file available on Sakai\r\nLab 3 Goals\r\nAssess the bivariate relationships between outcome and covariables and the statistical association between each of the two variables\r\nEstimate the epidemiologic measures of association (risk difference, risk ratio and odds ratio) between risk factors and outcome\r\nLab 3 Grading scheme\r\nCompetency\r\nPoints\r\n.Rmd file runs without error\r\n10\r\nTask 3 (Table 1)\r\n5\r\nTask 4 (Table 2)\r\n10\r\nTask 5 (Table 3)\r\n20\r\nTask 6 (stratified boxplots)\r\n10 (5 each)\r\nTask 7 (short answer)\r\n5\r\nTask 8 (short answer)\r\n10\r\nTask 9 (short answer)\r\n5\r\nTask 10 (short answer)\r\n5\r\nTask 11 (short answer)\r\n20\r\nTotal\r\n100\r\n\r\nLate policy: 5 point deduction per 24 hour period past due date and time\r\nTask 1: Load libraries and dataset\r\nFor this assignment, we’ll need packages {tidyverse}, {skimr}, {epiR}, {tableone}, and {epiAssist}\r\nFor your data, you will use the dataset Lab_3_kenya.rds, located in the Sakai Resources folder for this course. This has all of the variables that we’ve created in labs 0, 1, and 2, and only contains those observations in our closed cohort (n = 16,828).\r\nTask 2: BIG PICTURE\r\nJust think on this for a moment. Internalize it. Come back to it:\r\nFor this and subsequent labs, we are considering\r\nChild mortality within 5 years as our outcome (response) variable\r\nBirth order >= 5 as the main risk factor variable and\r\nOther variables as potential confounders or modifiers of the relationship between mortality and birth order.\r\nCausal diagram of birth order (E: “exposure”) on childhood mortality (D: “disease”)Task 3: Table 1\r\nTabular analysis of continuous variables by mortality status\r\nINSTRUCTIONS: Fill in Table 1 with the descriptive statistics for maternal BMI (mbmi) and maternal age (mage), overall and by the two levels of child mortality within 5 years (death).\r\n\r\nReminder that you can find Tables 1-3 in the Excel file provided in the Lab Materials for this lab\r\nThis table also requires you to use R to calculate statistical tests (T-test and Kruskal-Wallis) of the association between the outcome, death, and maternal BMI and age (i.e., comparing the distributions of maternal BMI and maternal age for the children who died versus those who were alive).\r\nRound p-values to 3 decimal places.\r\nRead on for advice on how to generate overall and stratified summary statistics, as well as instructions for how to perform the two statistical tests.\r\nOverall summary statistics for each variable variable\r\nThe skim() function provided by {skimr} makes this task a simple one. However, unlike previous labs where we ran a summary on the entire dataset, in this task we want individual variables. To keep your output clean, we recommend you generate your summary statistics only for the variables of interest.\r\nWe can use a pipe with skimr() to call specific variables, like this:\r\n\r\n\r\ndata %>%\r\n  skim(variable)\r\n\r\n\r\n\r\nStratified summary statistics\r\nSince we’re already doing things in a pipe, we can use the filter function to retrieve summary statistics based on mortality status. To stratify by the variable death, we should add the group_by() function to our pipeline, specifying that we want to “group by death, then skim the variable mbmi”:\r\n\r\n\r\ndata %>%\r\n  group_by(death) %>%\r\n  skim(mbmi)\r\n\r\n\r\n\r\nNote: Table 1 asks for counts of records with mbmi and mage data available vs. those missing, both overall and by the 2 levels of death (i.e. dead and alive). If you look closely, this means that we need counts of counts of kids with data available on mbmi and mage, overall, within levels of death, and those missing values. skim() does not give you raw counts of the variable death that account for those missing values.\r\nUse table() or a pipe, group_by() and count() to get the counts of children in each group of death (i.e. by levels of death). Then use the column of your skim output, n_missing, to subtract missing values from groups\r\nT-tests and Kruskal-Wallis tests\r\nT-tests and one of their nonparametric cousins, Kruskal-Wallis, can be used to compare means between between groups (i.e. between sub-populations), with the null hypothesis (H0) being that all means are equal.\r\nThere are a few simple functions from the {stats} package that we can use to manually run these tests. To implement, we use functions t.test() and kruskal.test(). Both take the same arguments:\r\n\r\nMany functions in R can perform a task like generating tables, while computing a p-value as a matter of course (see Task 4.\r\nA formula: depVar ~ indepVar\r\nA data frame\r\nIn this case, we are interested in knowing whether or not the means for mbmi and mage differ by strata of child mortality status, death.\r\nWhat this means is that when we’re building our formula, the “y”/dependent variable (the resulting means), should be on the left, while the “x”/independent variable (a categorical stratifying variable), should be on the right.\r\n\r\nFormulas: In R, “formulas” follow different rules depending on the function. As a general rule, most take the form y ~ x. Go here to learn more.\r\nExample of T-tests and Kruskal-Wallis\r\nAs an example, we might use the starwars dataset, available through the {dplyr} package (in the tidyverse), to inspect the body mass of different characters by gender. Let’s take a look at the raw data:\r\n\r\nname\r\nheight\r\nmass\r\nhair_color\r\nskin_color\r\neye_color\r\nbirth_year\r\nsex\r\ngender\r\nhomeworld\r\nspecies\r\nLuke Skywalker\r\n172\r\n77\r\nblond\r\nfair\r\nblue\r\n19.0\r\nmale\r\nmasculine\r\nTatooine\r\nHuman\r\nC-3PO\r\n167\r\n75\r\nNA\r\ngold\r\nyellow\r\n112.0\r\nnone\r\nmasculine\r\nTatooine\r\nDroid\r\nR2-D2\r\n96\r\n32\r\nNA\r\nwhite, blue\r\nred\r\n33.0\r\nnone\r\nmasculine\r\nNaboo\r\nDroid\r\nDarth Vader\r\n202\r\n136\r\nnone\r\nwhite\r\nyellow\r\n41.9\r\nmale\r\nmasculine\r\nTatooine\r\nHuman\r\nLeia Organa\r\n150\r\n49\r\nbrown\r\nlight\r\nbrown\r\n19.0\r\nfemale\r\nfeminine\r\nAlderaan\r\nHuman\r\nOwen Lars\r\n178\r\n120\r\nbrown, grey\r\nlight\r\nblue\r\n52.0\r\nmale\r\nmasculine\r\nTatooine\r\nHuman\r\n\r\nOur formal question might look like this: “Does mean character height differ by a character’s gender?” (For the purposes of example, assume that the 83 characters are a sample from a large population of characters)\r\nWe can calculate summary statistics for sub-group frequency counts and means:\r\n\r\n\r\nstarwars %>%\r\n  filter(!is.na(gender)) %>%\r\n  group_by(gender) %>%\r\n  summarize(n = n(), mean = mean(height, na.rm = TRUE))\r\n\r\n\r\n\r\n\r\ngender\r\nn\r\nmean\r\nfeminine\r\n17\r\n164.6875\r\nmasculine\r\n66\r\n176.5161\r\n\r\nWe can also visualize the respective distributions of height by gender using density plots.\r\n\r\n\r\n\r\n\r\nThe hypotheses of the two-sample t-test ask whether the mean height of feminine and masculine are different. To address this question, data from the 17 feminine and 66 masculine characters are used as samples drawn from the larger population.\r\nOur t-test would effectively tell us whether or not the observed difference in sample mean of height supports the conclusion that the population mean height is different between the two groups.\r\n\r\n\r\nt.test(height ~ gender, data = starwars)\r\n\r\n#>  Welch Two Sample t-test\r\n#>\r\n#> data:  height by gender\r\n#> t = -1.5596, df = 37.315, p-value = 0.1273\r\n#> alternative hypothesis: true difference in means between group feminine and group masculine \r\n#> is not equal to 0\r\n#> 95 percent confidence interval:\r\n#>  -27.191682   3.534423\r\n#> sample estimates:\r\n#>  mean in group feminine mean in group masculine \r\n#>                164.6875                176.5161 \r\n\r\n\r\n\r\n\r\nFunction output is indicated by lines that start with #>\r\nHowever, since the category for gender == 'feminine' only contains 17 observations, we might decide that a nonparametric test (one that doesn’t assume a normal distribution), might be more appropriate for the small sample size. In this case we can use the Kruskal-Wallis test:\r\n\r\n\r\nkruskal.test(height ~ gender, data = starwars)\r\n\r\n#> Kruskal-Wallis rank sum test\r\n#> \r\n#> data:  height by gender\r\n#> Kruskal-Wallis chi-squared = 8.6845, df = 1, p-value = 0.003209\r\n\r\n\r\n\r\n\r\nNote that our decision tree for statistical tests of continuous outcome variables would get us to the Wilcoxon Rank Sum Test. In R, this is implemented the same as the Kruskal Wallis test, but uses the function wilcox.test(). We have used kruskal.test() because this is the test we will perform for Table 1.\r\nNotice the difference in the conclusion you would draw from either test. For the t-test, we should conclude that we cannot reject the null hypothesis, whereas the nonparametric test tells us that there is a statistically significant difference in the mean height of the two groups.\r\nTask 4: Table 2\r\nTabular analysis of categorical variables by death\r\nINSTRUCTIONS: Table 2 contains three distinct columns for frequency counts and percentages.\r\nThe first, “All Children” requires the overall frequency counts and percentages by the categorical variables of interest. These are the same values that you generated for Table 1 in both labs 1 and 2.\r\nThe next two columns, “Dead” and “Alive”, require frequency counts and percentages of each of the variables of interest, stratified by mortality status at 5 years, death. Be sure to calculate percentages only for the non-missing values and round percentages to 1 decimal place.\r\nThis table also includes statistical measures (Chi-Sq and Fisher’s exact test) of the association between mortality status and the variables in the table.\r\nRound p-values to 3 decimal places.\r\n{tableone} package\r\nWe will complete this task by first using the package {tableone}, a powerful tool for tabular analysis, especially for stratified analyses like this one.\r\nMore on {tableone}: Find a full tutorial of {tableone} here\r\nThe function CreateTableOne() will create a {tableone} object that generates mean and standard deviation for continuous variables, and frequency counts and percentages for factor variables.\r\nCreateTableOne() requires several arguments of central importance for it to run:\r\n1.) vars = - a vector of variable names, written as strings\r\n2.) strata = - a categorical variable for stratifying (if desired)\r\n3.) data = - your data frame\r\n4.) factorVars = - used to convert character variables to factors on the fly\r\nfactorVars: You shouldn’t need to use factorVars = for your kenya data, since we have already converted categorical variables to factors in the previous labs.\r\nCreate a vector of variables\r\nFor starters, we’ll need a vectorized list of our variables of interest, written as strings (in quotes).\r\nAs an example, we might use the derived variables in our kenya dataset for size and pnc. To make our code more readable, we will create a vector object that contains both variable names. To do this, we put both variable names in quotes, separating them by commas within a vector, c(). Then we need to assign the vector a name so that we can call it later. Here, we’ve named the vector variables.\r\n\r\n\r\nvariables <- c('size', 'pnc')\r\n\r\n\r\n\r\nConstruct {tableone} objects\r\nFirst, we will construct two different {tableone} objects: a “simple” table with univariate frequency counts, and a “stratified” table, with bivariate counts.\r\nAfter that, we’ll use print() to view the output generated by those two different objects.\r\nSimple table\r\nFor a simple table of summary statistics, we only need to use arguments vars = and data =. It’s best practice to assign the {tableone} object to a name, so that we can use it in a print() function in a moment.\r\n\r\n\r\nsimple_tab <- CreateTableOne(vars = variables, \r\n                             data = kenya)\r\n\r\n\r\n\r\nStratified table\r\nWe may also want to stratify our results by a second variable, like bord5.\r\n\r\n\r\nstratified_tab <- CreateTableOne(vars = variables, \r\n                                 data = kenya, \r\n                                 strata = 'bord5')\r\n\r\n\r\n\r\nPrint table\r\nWe can then use a print() function, with arguments specific to our {tableone} object, to view our simple and stratified tables.\r\nWith categorical variables, it’s typically best to use the argument showAllLevels = TRUE so that we see every category.\r\n\r\nSetting showAllLevels to FALSE will leave out the reference level for each variable.\r\nPrint simple table\r\n\r\n\r\nprint(simple_tab, showAllLevels = TRUE)\r\n\r\n#>              level                  Overall      \r\n#>  n                                   16828        \r\n#>  size (%)     Very large                26 ( 7.4) \r\n#>               Larger than average       76 (21.7) \r\n#>               Average                  158 (45.1) \r\n#>               Smaller than average      65 (18.6) \r\n#>               Very small                25 ( 7.1) \r\n#>  pnc (%)      No prenatal care          15 ( 9.1) \r\n#>               Received prenatal care   149 (90.9) \r\n\r\n\r\n\r\nPrint stratified table with chi-square test of independence\r\nWhereas the simple table prints overall frequency counts and percentages, our stratified table will take the stratifying variable (in the case of this example, bord5), and perform a chi-square test of independence for that stratifying variable and each of the variables of interest e.g. of size and pnc:\r\n\r\n\r\nprint(stratified_tab, showAllLevels = TRUE)\r\n\r\n#>              Stratified by bord5\r\n#>               level                  1-4 in Birth Order 5+ in Birth Order  p     test\r\n#>  n                                   13263              3565                         \r\n#>  size (%)     Very large                23 ( 9.3)          3 ( 2.9)        0.157     \r\n#>               Larger than average       51 (20.6)         25 (24.5)                  \r\n#>               Average                  108 (43.5)         50 (49.0)                  \r\n#>               Smaller than average      50 (20.2)         15 (14.7)                  \r\n#>               Very small                16 ( 6.5)          9 ( 8.8)                  \r\n#>  pnc (%)      No prenatal care           4 ( 3.7)         11 (20.0)        0.002     \r\n#>               Received prenatal care   105 (96.3)         44 (80.0)       \r\n\r\n\r\n\r\nNote that each row of the stratified results sum to the same result as given by the column for “Overall” in our simple_tab.\r\nAs the default for categorical variables, the {tableone} print() function performs a chi-square test of independence between `bord5’ and each of the variables of interest.\r\nThere’s one glaring issue here, however: neither of our stratified variables meet the assumption of normality! Each contains at least one cross-tabulation with counts that are less than 5.\r\nFisher’s exact test of significance\r\nWhen we look at our decision tree for hypothesis testing of categorical outcome variables, we might decide that we need to use a Fisher’s Exact test when our data doesn’t meet the assumption of all expected cell counts being at least 5 for a chi-square test of independence.\r\n\r\nYou can use the output from chisq.test() to find expected cell counts:\r\n\r\n\r\nsize_tab <- table(kenya$bord5, kenya$pnc)\r\nX2 <- chisq.test(size_tab)\r\nX2$expected\r\n\r\n\r\n\r\nTo do this in a {tableone} object, we just need to include exact = TRUE in our print() function to calculate a Fisher’s Exact test, the nonparametric equivalent of the chi-square test of independence.\r\n\r\n\r\nprint(stratified_tab, showAllLevels = TRUE, exact = TRUE)\r\n\r\n#>               Stratified by bord5\r\n#>               level                  1-4 in Birth Order 5+ in Birth Order p      test \r\n#>  n                                   13263              3565                          \r\n#>  size (%)     Very large                23 ( 9.3)          3 ( 2.9)        0.141 exact\r\n#>               Larger than average       51 (20.6)         25 (24.5)                   \r\n#>               Average                  108 (43.5)         50 (49.0)                   \r\n#>               Smaller than average      50 (20.2)         15 (14.7)                   \r\n#>               Very small                16 ( 6.5)          9 ( 8.8)                   \r\n#>  pnc (%)      No prenatal care           4 ( 3.7)         11 (20.0)        0.001 exact\r\n#>               Received prenatal care   105 (96.3)         44 (80.0)                   \r\n\r\n\r\n\r\nAlthough in this particular example we draw the same conclusions from the Fisher’s exact test as we do from the chi-square test of independence, this will not always be the case.\r\n\r\nThere are other arguments for {tableone} that allow you to specify exactly which variables do not meet the assumptions of the default parametric test (e.g. chi-square test of independence for categorical variables), and therefore perform the nonparametric test on only those variables. Similar steps apply for analysis of continuous variables (you could even try using the continuous variables from Task 3!)\r\nTask 5: Table 3\r\nRisk Differences, Risk Ratios, and Odds Ratios\r\nINSTRUCTIONS: Fill in Table 3 with frequency counts overall and by levels of child mortality within 5 years and with the stratum-specific risk of death (these are the same frequency count values, N, as in Table 2). Then use mAssoc() to compute the risk difference, risk ratio and the odds ratio of death comparing levels for each of the 5 variables to the reference level.\r\nTable 3\r\nWe use mAssoc() in the package {epiAssist} to generate these values. This function is a helper function for the original epi.2by2() function, which is available in the package {epiR}. epi.2by2() works the same as mAssoc(), except it only provides output for a single index/reference level pair. mAssoc() can do multiple index levels all at once.\r\nThe reference level for each variable is noted in the spreadsheet (“REF”). For the two 3-level variables, calculate risk differences and ratios for each non-reference level relative to the reference level.\r\nRound risks to 3 decimal places and ratios to 2 decimal places.\r\nTake a deep breath.\r\nDon’t let this table overwhelm you. We’re going to break it down step-by-step\r\nFrequency counts\r\nThe frequency counts in this table align with those generated for Table 2. Feel free to use Excel equations to transfer data from sheet 2 to sheet 3.\r\nFrequency countsRisk\r\nThe value for risk should be calculated as the risk of the outcome of interest (death == \"Dead\") according to each level of the exposure. Risk should be reported as a proportion, rounding to the third decimal.\r\nRiskRisk difference (RD), Risk Ratio (RR), and Odds Ratio (OR)\r\nMeasures of associationRD, RR, and OR are what are known as “measures of association” because they are used to compare outcomes between levels of a categorical variable. In epidemiology, we refer to the baseline level as the “reference level” and the comparator level as the “index level”.\r\nIn practical terms, for things like RRs and ORs, the index level goes in the numerator and the reference level goes in the denominator.\r\n\\[\\frac{index\\ \\ level}{reference\\ \\ level}\\]\r\nYou can use your underlying knowledge of these measures of association to calculate our answers by hand and confirm the results that we’ll obtain using the functions that follow.\r\nmAssoc() for Measures of Association\r\nFor this task, we will use mAssoc() to generate all three measures of association (RD, RR, and OR) in a singular output.\r\nTo use mAssoc(), we must take several steps to ensure that our data is in the proper shape for our measures to be calculated appropriately:\r\n1.) Create a table object using table()\r\n2.) Rearrange the table with flipTable() to put it in the appropriate alignment for use by mAssoc()\r\n3.) Submit our rearranged table to mAssoc().\r\nmAssoc()’s main input is a table object arranged in the following format:\r\n\r\n\r\n#   -----------  ----------  ----------\r\n#                Outcome +   Outcome -\r\n#   -----------  ----------  ----------\r\n#     Expose +   cell 1      cell 3          \r\n#     Expose -   cell 2      cell 4\r\n#   -----------  ----------  ----------\r\n\r\n\r\n\r\nAs a way of demonstrating, we will use the variable bord5, stratified by death to calculate our measures of association.\r\nStep 1: Create a table object with table()\r\nFirst, we need to generate a table object and assign it a name. Keep in mind that we want our exposure variable (in this case, bord5) along the left side of the table and our outcome variable (death) at the top of the table.\r\n\r\n\r\ntab_bord5 <- table(kenya$bord5, kenya$death)\r\ntab_bord5\r\n\r\n#>                      Alive  Dead\r\n#>  1-4 in Birth Order 11809  1454\r\n#>  5+ in Birth Order   3002   563\r\n\r\n\r\n\r\nStep 2: Rearrange table with flipTable()\r\nNotice that mAssoc() takes a table object with the cross-tabulation of outcome of interest (Outcome +) and exposure of interest (Expose +) in the top-left cell:\r\n\r\nReminder that the function mAssoc() is a wrapper function for the epi.2by2() function from package {epiR}.\r\n\r\n\r\n#   -----------  ----------  ----------\r\n#                Outcome +   Outcome -\r\n#   -----------  ----------  ----------\r\n#     Expose +   cell 1      cell 3          \r\n#     Expose -   cell 2      cell 4\r\n#   -----------  ----------  ----------\r\n\r\n\r\n\r\nIf we refer to our tableof bord5 and death, we’ll notice that it currently displays the inverse of the arrangement required by mAssoc():\r\n\r\n\r\ntab_bord5\r\n\r\n#>                       Alive  Dead\r\n#>  1-4 in Birth Order  11809   1454\r\n#>  5+ in Birth Order    3002    563\r\n\r\n\r\n\r\nThat is, our tabulation of bord5 currently takes the following form, with “Outcome-” and “Expose-” in the top-left corner. We need “Outcome+” and “Expose+” in that corner:\r\n\r\n\r\n#   -----------  ----------  ----------\r\n#                Outcome -   Outcome +\r\n#   -----------  ----------  ----------\r\n#     Expose -   cell 1      cell 3          \r\n#     Expose +   cell 2      cell 4\r\n#   -----------  ----------  ----------\r\n\r\n\r\n\r\nTo fix this, we can use flipTable(), assigning it back to the same name for the table object:\r\n\r\n\r\ntab_bord5 <- flipTable(tab_bord5)\r\n\r\ntab_bord5\r\n\r\n#>                      Dead  Alive\r\n#>  5+ in Birth Order    563   3002\r\n#>  1-4 in Birth Order  1454  11809\r\n\r\n\r\n\r\nGenerate measures of association with mAssoc()\r\nTo generate our measures of association and their accompanying confidence intervals, all we need to do is submit our rearranged table to mAssoc(), specifying that our data is cohort data with method = \"cohort.count\", and setting our confidence level to 0.95 with conf.level = .95:\r\n\r\n\r\nmAssoc(tab_bord5, method = \"cohort.count\", conf.level = 0.95)\r\n\r\n#> MEASURES OF ASSOCIATION FOR:  5+ in Birth Order (index) vs.  1-4 in Birth Order (referent)\r\n#> Point estimates and 95% CIs:\r\n#> -------------------------------------------------------------------\r\n#> Inc risk ratio                                 1.44 (1.32, 1.58)\r\n#> Odds ratio                                     1.52 (1.37, 1.69)\r\n#> Attrib risk in the exposed *                   0.05 (0.04, 0.06)\r\n#> Attrib fraction in the exposed (%)            30.58 (24.05, 36.56)\r\n#> Attrib risk in the population *                0.01 (0.00, 0.02)\r\n#> Attrib fraction in the population (%)          8.54 (6.20, 10.82)\r\n#> -------------------------------------------------------------------\r\n#> Uncorrected chi2 test that OR = 1: chi2(1) = 62.125 Pr>chi2 = <0.001\r\n#> Fisher exact test that OR = 1: Pr>chi2 = <0.001\r\n#>  Wald confidence limits\r\n#>  CI: confidence interval\r\n#>  * Outcomes per population unit \r\n\r\n\r\n\r\nReading mAssoc() output:\r\nThe output of interest from mAssoc() will be printed to the console. You can find it by clicking on the area with a red square around it in this screenshot:\r\n\r\nIf we set units = 100, our attributable risk in the exposed (risk difference) would be interpretable as a difference of x number of outcomes per 100 people in the index level when compared to the reference level. Since our population unit is set to 1, this number is simply interpreted as an individual’s risk difference of experiencing the outcome if they are in the index level of the exposure.\r\nHere is a quick guide to the components of the output relevant to this activity:\r\nInc risk ratio: Our basic risk ratioOdds ratio: Our basic odds ratioAttrib risk in the exposed: The risk difference. mAssoc() takes another argument, units =, which by default is set to 1. This regulates our output for the measure of risk difference.\r\nmAssoc() when exposure variable has 3+ levels\r\nNow that you’ve seen the code for bord5 as an example, if you’re paying close attention, you may have started to wonder, “But what about variables like education, when there’s more than one index level?”\r\nGood news: flipTable() and mAssoc() are designed to handle exposure variables with any number of levels. This means that the steps you take for bord5, male, and rural will also apply to magec and education; simple as that.\r\nRunning mAssoc() when your exposure variable has 2+ levels will render multiple outputs for each respective index variable. The person who wrote this function was even nice enough to label each of those levels and whether or not they are the index or the reference level.\r\nTask 6: Side-by-side boxplots\r\nINSTRUCTIONS: Generate boxplots of maternal BMI (mbmi) and maternal age (mage), stratified by levels of child mortality (death).\r\nClick here for more help with boxplots using {ggplot2}\r\nFor this activity, we need boxplots that plot continuous variables based on child mortality. In ggplot2, we can stratify boxplots by using a second categorical variable when we specify our aesthetics (aes()). In other words, if you set mbmi to the y-axis, you can use a categorical variable, like death as the variable on the x-axis.\r\nAdditionally, don’t forget to label your axes, give the plot a title, and save it using ggsave()\r\n\r\nWithin your call to geom_boxplot(), you can also change the color with fill = \"colorname\". Find a list of color names here\r\nHere’s an example of what we are looking for, using the starwars dataset. We’ve adjusted the dimensions a bit to accommodate one lone outlier. Care to guess who it is?\r\n\r\n\r\n\r\n\r\n\r\n\r\nFigure 1: A very heavy slug indeed\r\n\r\n\r\n\r\nTask 7: Short answer\r\nFor Table 1, what is the null hypothesis being tested for each the two variables. Do the two statistical tests agree? Is there a statistically significant difference in the distributions of either BMI or age by child mortality? Are there substantively/clinically significant differences in these distributions? If statistical and substantive differences disagree, why might that be the case?\r\nTask 8: Short answer\r\nFor Table 2, what is the null hypothesis being tested for the variables. Do the two statistical tests agree? Is there a statistically significant difference in any of these bivariate distributions? Are there substantively/clinically significant differences in these distributions?\r\nTask 9: Short answer\r\nAge has been evaluated both as a continuous and categorical variable. Do the tests of association for these 2 parameterizations of age agree? If not, why not?\r\nTask 10: Short answer\r\nFor Table 3, which risk factor has the strongest association with mortality? Provide a rationale for this assessment. Which factors are protective and which are harmful?\r\nTask 11: Short answer\r\nPrepare a brief paragraph describing and interpreting risk, RD and RR estimates (from Table 3) for the 60 month risk of death in association with birth order and maternal age.\r\nBegin the paragraph by reporting the overall risk or proportion of deaths in the study population. Don’t just repeat numbers that are already shown in the table; note the values of effect estimates you feel should be highlighted, but otherwise describe the results in words. Write this paragraph as if you were including it in the results section of a manuscript. (175 words maximum).\r\n\r\n\r\n\r\n",
      "last_modified": "2021-11-16T11:04:29-05:00"
    },
    {
      "path": "lab_4.html",
      "title": "Lab 4 (705)",
      "description": "__Due:__ November 23, 2021 by 14:00 ET\n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\nLAB MATERIALS\r\nLab 4 Goals\r\nLab 4 Grading scheme\r\nTask 1: Load libraries & data\r\nTask 2: Table 1\r\nTabular analysis of stratifying variables\r\nTask 2.1: Stratified frequency counts and risk\r\nTask 2.2: Create epi.2by2() object\r\nTask 2.3: Extract estimates of risk difference\r\nTask 2.4: Extract estimates of risk ratios\r\n\r\nTask 3: Short Answer\r\nTask 4: Short Answer\r\n\r\n\r\n\r\n\r\nLAB MATERIALS\r\nR Markdown file for Lab 4 Click link to download. Fill it in with your answers to the following lab tasks. When you’re ready to submit, name it as Lab4_FirstinitialYourlastname.Rmd, and submit it using the Sakai dropbox.\r\nExcel document with Table for Task 2\r\nLab_4_kenya.rds - data file available on Sakai\r\nLab 4 Goals\r\nCalculate measures of association between birth order (i.e. the primary exposure of interest) and child mortality by 5 years (i.e. the primary outcome of interest) within strata of other variables\r\nAssess for potential confounding, mediation or effect measure modification\r\nLab 4 Grading scheme\r\nCompetency\r\nPoints\r\n.Rmd file runs without error\r\n10\r\nTable 1 - Frequency counts and Risk\r\n20\r\nTable 1 - Risk difference\r\n20\r\nTable 1 - Risk ratio\r\n20\r\nTask 3 (short answer)\r\n15\r\nTask 4 (short answer)\r\n15\r\nTotal\r\n100\r\nTask 1: Load libraries & data\r\nFor this assignment, use the dataset ‘Lab_4_kenya.rds’.\r\nThis lab will also require the packages {tidyverse}, {fmsb}, {epiR}, and {epiAssist}\r\nYou may need to install {fmsb}. You can do so with the following code:\r\n\r\n\r\ninstall.packages(\"fmsb\")\r\n\r\n\r\n\r\nHopefully at this point in the semester, you have a pretty good grasp on how to load libraries and data. But if you’re still unsure about something:\r\nHelp loading libraries. Help loading data.\r\nTask 2: Table 1\r\nINSTRUCTIONS: Generate frequency counts of 5-year mortality (death) according to child’s birth order (bord5), stratified by the following variables:\r\nmale\r\nrural\r\nmagec\r\neducation\r\nUse Table 1 to record frequency counts and strata-specific risk of death before 60 months, by birth order. Then, use epi.2by2() to generate stratified measures of risk difference, risk ratios, and their accompanying confidence intervals.\r\nYou will also use the resulting epi.2by2() object to extract test statistics and p-values that test for potential effect measure modification by the stratifying variables.\r\nMoreover, you will use a pooled (i.e. adjusted) estimate of risk difference and risk ratio to assess for confounding by comparing those estimates to the crude (i.e. unadjusted) estimates of risk difference and risk ratio.\"\r\nBelow is a detailed guide on how to complete each step of these instructions.\r\nTabular analysis of stratifying variables\r\nTable 1 is an example of a stratified analysis of exposure and outcome variables. Essentially, we want to know if the four variables (male, rural, magec, and education) are confounders and/or effect measure modifiers of the relation between birth order and 5-year infant mortality. In other words, Questions of effect measure modification include whether the degree of association of child’s birth order and mortality status is different for boys and girls? What about for rural and urban families? Mothers of different age categories? And so on…\r\n\r\nRemember the BIG PICTURE that was explained in the previous lab\r\nWhen you open the Excel file that contains Table 1, here is what you will see:\r\nTable 1\r\nIf you haven’t already, download the Excel file at the top of this page to access Table 1\r\nWe will break Table 1 into sections, and walk you through how to obtain these values in R.\r\nTask 2.1: Stratified frequency counts and risk\r\nINSTRUCTIONS: Fill in the table with the numbers of deaths (death, the outcome) and the 60-month risk of death according to birth order (bord5, the exposure), stratified by:\r\nChild’s gender (variable male)\r\nRural/urban residence (variable rural)\r\nMaternal age (variable magec)\r\nMaternal education (variable education)\r\nTable 1 - Frequency counts and riskFor this task, we recommend that you create a table object with table(), similar to the one you created in lab 3, where your outcome of interest is mortality (death) and your exposure of interest is child birth order (bord5). Except this time, include a third stratifying variable.\r\n\r\nIf you’ve been enjoying the {tidyverse}, feel free to use group_by() and count(). But the table object will be useful in later steps.\r\ntable() allows for an unlimited number of stratifying levels. You keep listing variables, and table() will keep splitting the data into subsets.\r\nSay that we created a binary variable in our kenya dataset that indicated whether or not a mother’s BMI was above or below 25 kg/m2. Let’s call that variable mbmi_25. Here is an example of a set of contingency tables of bord5 and death, stratified by mbmi_25:\r\n\r\n\r\nmbmi_tab <- table(kenya$bord5, kenya$death, kenya$mbmi_25)\r\n\r\nmbmi_tab\r\n\r\n#> , ,  = <25\r\n#> \r\n#>                     \r\n#>                      Alive Dead\r\n#>   1-4 in Birth Order  7931 1071\r\n#>   5+ in Birth Order   2180  409\r\n#> \r\n#> , ,  = >=25\r\n#> \r\n#>                     \r\n#>                      Alive Dead\r\n#>   1-4 in Birth Order  3700  374\r\n#>   5+ in Birth Order    773  146\r\n\r\n\r\n\r\nHere, our stratifying variable is mbmi_25, which separates our tabulated data into two tables according to mothers who have a BMI greater than or equal to 25, and those with BMI less than 25.\r\nWe can generate row-wise proportions of our table by wrapping the table object in the proportions() function.\r\n\r\nrow-wise proportions, a.k.a. RISK of DEATH/NOT DEATH! As a reminder, rates (time in the denominator) are preferable measures of incidence of death when there are missing outcomes.\r\n\r\n\r\nproportions(mbmi_tab, margin = c(1, 3))\r\n\r\n\r\n#> , ,  = <25\r\n#> \r\n#>                     \r\n#>                           Alive       Dead\r\n#>   1-4 in Birth Order 0.88102644 0.11897356\r\n#>   5+ in Birth Order  0.84202395 0.15797605\r\n#> \r\n#> , ,  = >=25\r\n#> \r\n#>                     \r\n#>                           Alive       Dead\r\n#>   1-4 in Birth Order 0.90819833 0.09180167\r\n#>   5+ in Birth Order  0.84113166 0.15886834\r\n\r\n\r\n\r\n\r\nThe argument margin = designates along which we’d like to split our table when deriving proportions. By putting margin = c(1,3), we restrict our calculation of proportions to row (1) and strata (3). Notice how each row sums to 1. Try playing with this configuration to see how it works.\r\nTask 2.2: Create epi.2by2() object\r\nINSTRUCTIONS: Create an epi.2by2() object for each of the tables you generated above.\r\nepi.2by2() works just like mAssoc(). We need to rearrange our tables so that our outcome of interest and exposure of interest are in the top-left corner of each table.\r\nGood news: The function flipTable() is designed to work with stratified tables too!\r\nDo the following:\r\na. flipTable() so that the cell that contains the index level of the exposure (birth order) and the outcome of interest (mortality status) is in the top-left corner of either table.\r\nFor instance:\r\n\r\n\r\nmbmi_flip <- flipTable(mbmi_tab)\r\n\r\n\r\n\r\nb. Create an epi.2by2() object by using the flipTable() object, with confidence level of 0.95 and units set to 1.\r\n\r\n\r\nmbmi_epi <- epi.2by2(mbmi_flip, units = 1, conf.level = 0.95)\r\n\r\n\r\n\r\n\r\nWe set units = 1 to obtain risk difference as a proportion. The default unit of 100 would give us measures of risk “per 100 persons”.\r\nTask 2.3: Extract estimates of risk difference\r\nINSTRUCTIONS: Use the epi.2by2() object to extract the measure of risk difference for mortality in association with bord5 within each covariable stratum, along with test statistics and an overall (“adjusted”) pooled estimate of risk difference.\r\nTable 1 - Risk differencesIMPORTANT: The output for epi.2by() is NOT the full extent of the information contained within an epi.2by2() object! There is more to every object than what meets the eye.\r\n\r\n\r\n\r\nFigure 1: More than eye\r\n\r\n\r\n\r\nWhen you call an epi.2by2() object, you will notice that its printed output is a lot like that of mAssoc() from the previous lab. Except this time, there are “crude” and “M-H” estimates:\r\n\r\nThis output is what’s called the object’s “print method”\r\nBut if you inspect the epi.2by2() object (either in your R Environment, or by typing the object’s name, followed by $), you will find vectors, lists, data frames, and all sorts of sub-objects.\r\n\r\nSee this link if you are still uncertain about the functionality of $\r\nFollowing is how to locate the necessary information for this portion of Table 1\r\na. Risk difference and 95% CI\r\nTo access the risk differences of individual strata, use the following path to refer to that exact component of your epi.2by2 object:\r\n\r\n\r\nobject$massoc.detail$ARisk.strata.wald\r\n\r\n\r\n\r\n\r\nConfused by the nomenclature? massoc is Measures of Association. ARisk is Attributable Risk. strata refers to strata-specific measures. wald….. ask Larry or Liz.\r\nSo to obtain our measures of risk difference within separate stratum of mothers with BMI < 25 and mothers with BMI >= 25, we can use the following code:\r\n\r\n\r\nmbmi_epi$massoc.detail$ARisk.strata.wald\r\n\r\n#>          est      lower      upper\r\n#> 1 0.03900249 0.02344295 0.05456203\r\n#> 2 0.06706667 0.04182403 0.09230931\r\n\r\n\r\n\r\nYou might notice that this returns a single estimate for each stratum. Our table, however, asks for two estimates for each. What gives??\r\nSince we have pre-determined our “reference” and “index” levels, and know that a risk difference is defined as “risk in the level of interest minus risk in the reference level”, what’s the result when our reference level is also our level of interest?\r\nCheck your work!\r\nTo check which value belongs to which strata, you can calculate it by hand using the values for risk that you obtained in Task 2.1.\r\nYou can also use the function riskdifference() from package {fmsb} to quickly derive risk ratios and confidence intervals.\r\nTo learn how to use this function, you can view its documentation by running the following code in your console:\r\n\r\n\r\n?riskdifference\r\n\r\n\r\n\r\n\r\nThis function is pretty straightforward and I’m confident you can hack it 🦊\r\nb. Test of homogeneity and p-value\r\nInconveniently, epi.2by2() doesn’t actually provide a chi-squared test of homogeneity of stratified risk differences. We can obtain this estimate by using the function epiHomog(), which takes a flipTable() object as its primary argument.\r\nIf your flipTable() objects are properly oriented, they should suffice. Use your flipTable() objects as an argument in the function epiHomog() to return the test statistic and p-value for a test of homogeneity at a significance level of 0.05:\r\n\r\n\r\nepiHomog(aFlippedTable)\r\n\r\n\r\n\r\nUsing our flipTable() object that is stratified by mother’s BMI, we would run the following code to obtain the resulting output:\r\n\r\n\r\nepiHomog(mbmi_flip)\r\n\r\n#>   A tibble: 1 x 2\r\n#>    `Test statistic` `p-value`\r\n#>              <dbl>     <dbl>\r\n#>   1           3.44    0.0637\r\n\r\n\r\n\r\nInterpreting X2 test of homogeneity\r\nWhat exactly is the chi-square test of homogeneity for? Well, it’s all about effect measure modification. Suppose we focus on the risk difference measure. We use the risk difference measure to quantify the association of birth order with the risk of death by 60 months of age. In particular, we use it to quantify the risk of death for those with birth order 5+ in comparison to the risk of death for those with birth order 1-4 (i.e. the reference level) by subtracting the two risks.\r\nWhen we consider a third variable such as male, the chi-square test of homogeneity is used to evaluate whether there is evidence that the risk difference is different for male and for female children. If so, it is important that we report the risk difference separately for male and for female children. In an interesting feature, if there is effect measure modification on one effect measure scale (e.g. risk difference), then it may very well not be present on another scale (e.g. risk ratio). This is precisely why we talk about “effect measure modification” and not simply “effect modification”.\r\nc. Pooled estimate (Mantel-Haenszel)\r\nFinally, we want the pooled estimate of risk difference. You might have already found this in the original print method’s output of our epi.2by2() object, where it’s referred to as Attrib risk in the exposed (M-H).\r\nBut since we need to report this estimate to three decimal points (see gutter), we need to reach into our epi.2by2 object and find the exact Mantel-Haenszel pooled estimate. You can access it by calling the following path:\r\n\r\nAs per lab submission guidelines on rounding\r\n\r\n\r\nobject$massoc.detail$ARisk.mh.wald\r\n\r\n\r\n\r\n\r\nHere, mh stands for “Mantel-Haenszel”\r\nUsing our epi.2by2() object from before, mbmi_epi, we find this value and its resulting output as follows:\r\n\r\n\r\nmbmi_epi$massoc.detail$ARisk.mh.wald\r\n\r\n#>          est      lower      upper\r\n#> 1 0.04662555 0.02684395 0.06640716\r\n\r\n\r\n\r\nWe would interpret this pooled result by saying that, when accounting for confounding in the exposure due to a third variable (mbmi_25), risk of the outcome in the exposed was 0.05 (95% CI 0.03 to 0.07) higher than the unexposed.\r\nTask 2.4: Extract estimates of risk ratios\r\nThe process for obtaining risk ratios, test statistics for homogeneity between strata, and pooled estimates is very similar to that of obtaining risk differences. Except this time, we don’t need a separate function for finding the test of homogeneity – epi.2by2() generates the test statistic for homogeneity of risk ratios itself.\r\nTable 1 - Risk RatiosRisk ratios\r\nTo extract risk ratios from an epi.2by2() object, you can use the following path:\r\n\r\n\r\nobject$massoc.detail$RR.strata.wald\r\n\r\n\r\n\r\nAgain, when you’re finding these risk ratios, consider what we’re asking for in the table when we request the risk ratio of the reference level as it pertains to the reference level. What does it amount to when we take the ratio of something to itself?\r\n\r\n\r\n\r\nFigure 2: Tautology club\r\n\r\n\r\n\r\nOkay moving on…so our mbmi_epi object renders the following output:\r\n\r\n\r\nmbmi_epi$massoc.detail$RR.strata.wald\r\n\r\n#>        est    lower    upper\r\n#> 1 1.046320 1.027315 1.065676\r\n#> 2 1.079734 1.048089 1.112334\r\n\r\n\r\n\r\nCheck your work!\r\nTo check which value belongs to which strata, you can calculate it by hand using the values for risk that you obtained in Task 2.1.\r\nYou can also use the function riskratio() from package {fmsb} to quickly derive risk ratios and confidence intervals.\r\nTo learn how to use this function, you can view its documentation by running the following code in your console:\r\n\r\n\r\n?riskratio\r\n\r\n\r\n\r\nThis one works the same as riskdifference(). Look it up and give it a try!\r\nTest of homogeneity and p value\r\nThe test of homogeneity for risk ratios is available in the following location within our epi.2by2() object\r\n\r\n\r\nobject$massoc.detail$wRR.homog\r\n\r\n\r\n\r\nPerforming this on our mbmi_epi object yields the following result:\r\n\r\n\r\nmbmi_epi$massoc.detail$wRR.homog\r\n\r\n#>   test.statistic df   p.value\r\n#> 1       3.109451  1 0.0778392\r\n\r\n\r\n\r\nReflecting on this test statistic and the resulting p-value, given a significance level of 0.05, would we conclude that mother’s BMI is a confounder in assessing the influence of birth order on the 5 year risk of mortality of children in our study cohort?\r\nHover for answer:  YOU THINK YOU GET FREEBIES?? \r\nPooled estimate (Mantel-Haenszel)\r\nThe pooled estimate appears in the printed output when you call your epi.2by2() object from the environment, and is called Inc risk ratio (M-H). However, this is only reported to two decimals. To access the actual value instead of the rounded one, you can look for it in the following place within your epi.2by2() object:\r\n\r\n\r\nobject$massoc.detail$RR.mh.wald\r\n\r\n\r\n\r\nUsing our mbmi_epi object renders the following output:\r\n\r\n\r\nmbmi_epi$massoc.detail$RR.mh.wald\r\n\r\n#>        est    lower    upper\r\n#> 1 1.055389 1.039048 1.071988\r\n\r\n\r\n\r\nWe would interpret this as meaning that when we account for confounding due to mother’s BMI, the risk of experiencing the outcome among the exposed was 1.06 (95% CI 1.04 to 1.07) times greater than the outcome of experiencing risk among the unexposed.\r\nTask 3: Short Answer\r\nPROMPT: Do you see evidence for confounding for any of the stratification variables (male, rural, magec, and education)? Explain your response for each variable.\r\n(Hint, you will need to refer back to the crude association estimates of risk difference and risk ratio that you generated for Table 3 of Lab 3).\r\nTask 4: Short Answer\r\nPROMPT: For which covariables does it seem appropriate to report a single pooled adjusted estimate and for which does it seem necessary to report separate stratified estimates (i.e., do you see evidence of effect modification)? Explain your rationale for each covariable.\r\nDoes your answer differ depending on whether you are considering the risk difference or risk ratio as your measure of effect?\r\n\r\n\r\n\r\n",
      "last_modified": "2021-11-16T11:05:27-05:00"
    },
    {
      "path": "mind_map.html",
      "title": "Mapping Our Steps",
      "description": "This page gives a birds-eye view of our analysis, where we've come from and where we're headed.\n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\nThe data science life cycle\r\n\r\nThe data science life cycle\r\nIn Hadley Wickam and Garrett Grolemund’s book, R for Data Science, they introduce a model of tools found in the typical data analysis project:\r\nDS Life CycleThis process consists of:\r\nimporting your data into R;\r\ntidying your data by shaping and storing it in a consistent format (luckily, this has largely already been done for us).\r\nOnce the data is tidy and coherent, we will need to:\r\ntransform the data by narrowing in on observations of interest, creating new variables, and calculating summary statistics;\r\nvisualize it to gain new perspective and insights;\r\nmodel the data with a well-defined research question.\r\nAnd this will all be for naught if without effectively communicating our findings.\r\nIn the lab section for Biostat 705 and 707, we’ll be working in a similar loop. To this toolbox, we might also add Workflow Management, which encompasses the bulk of what we do.\r\nThe below visualization aims to provide a visualization of this as we encounter it in-action.\r\n\r\n\r\n{\"x\":{\"diagram\":\"\\n\\n      digraph boxes_and_circles {\\n      \\n      graph[rankdir = LR, fontname = Helvetica]\\n      \\n      subgraph lab_1 {\\n      \\n      # adding node statements\\n      \\n      node [shape = circle]\\n      \\n      lab0 [label = \\\"Lab 0\\\", penwidth = 2.0]\\n      \\n      #Workflow\\n      #Explore\\n      #Transform\\n      \\n      node [shape = box]\\n      \\n      aa [label = \\\"Create workspace\\\"]\\n      ab [label = \\\"Inspect dimensions\\\"]\\n      ac [label = \\\"Generate summary statistics\\\"]\\n      ad [label = \\\"Create derived variable for mothers age\\\"]\\n      ae [label = \\\"Create categorical variable for mothers age\\\"]\\n      \\n      \\n      \\n      # add definition using node IDs\\n      \\n      lab0 -> aa #[label = \\\"Workflow\\\"]\\n      lab0 -> ab #[label = \\\"Explore\\\"]\\n      lab0 -> ac #[label = \\\"Explore\\\"]\\n      lab0 -> ad #[label = \\\"Transform\\\"]\\n      lab0 -> ae #[label = \\\"Transform\\\")]\\n      \\n      #aa -> Workflow\\n      #ab -> Explore\\n      #ac -> Explore\\n      #ad -> Transform\\n      #ae -> Transform\\n      \\n      }\\n      \\n      subgraph lab_1 {\\n    \\n      # adding node statements\\n      \\n      node [shape = circle]\\n      \\n      lab1 [label = \\\"Lab 1\\\", penwidth = 2.0]\\n      \\n      #Explore\\n      #Transform\\n      #Visualize\\n      #Workflow\\n      #Communicate\\n\\n      node [shape = box]\\n\\n      ba [label = \\\"Construct derived variables\\\\nbord5, male, mweight, mbmi\\\"]\\n      bb [label = \\\"Visualize with\\\\nhistograms and boxplots\\\"]\\n      bc [label = \\\"Calculate frequency counts\\\\nand percentages\\\"]\\n      bd [label = \\\"Create data dictionary\\\"]\\n      be [label = \\\"Consider implications of\\\\nmissing values\\\"]\\n      \\n      # edge definitions\\n      \\n      lab1 -> {ba, bb, bc, bd, be}\\n      \\n      #ba -> Transform\\n      #bb -> Visualize\\n      #bc -> Explore\\n      #bd -> Workflow\\n      #be -> Communicate\\n      \\n      \\n      }\\n      \\n      {rank = same; lab0 -> lab1}\\n      \\n      subgraph lab_2 {\\n      \\n      # adding node statements\\n      \\n      node [shape = circle]\\n      \\n      lab2 [label = \\\"Lab 2\\\", penwidth = 2.0]\\n      \\n      #Explore\\n      #Transform\\n      #Workflow\\n      #Visualize\\n      #Communicate\\n\\n      node [shape = box]\\n      \\n      ca [label = \\\"Construct derived variables\\\\nsize, belowavg, pnc, rural\\\\neducation, death, and time\\\"]\\n      cb [label = \\\"Update data dictionary\\\"]\\n      cc [label = \\\"Exclude observations where child was alive\\\\nbut less than 60 months of age\\\"]\\n      cd [label = \\\"Generate frequency histograms and\\\\nboxplo1ts for death and time\\\"]\\n      ce [label = \\\"Calculate frequency counts\\\\nandpercentages\\\"]\\n      cf [label = \\\"Consider implications of\\\\nanomalous and missing data\\\"]\\n      \\n      # define edges\\n      \\n      lab2 -> {ca, cb, cc, cd, ce, cf}\\n      \\n      #ca -> Transform\\n      #cb -> Workflow\\n      #cc -> Visualize\\n      #cd -> Explore\\n      #ce -> Explore\\n      #cf -> Communicate\\n      \\n      }\\n      \\n      {rank = same; lab1 -> lab2}\\n      \\n      subgraph lab_3 {\\n      \\n      # adding node statements\\n      \\n      node [shape = circle]\\n      \\n      lab3 [label = \\\"Lab 3\\\", penwidth = 2.0]\\n      \\n      #Explore\\n      #Workflow\\n      #Communicate\\n      #Model\\n\\n      node [shape = box]\\n      \\n      da [label = \\\"Define research question:\\\\nWhat are the factors that influence\\\\nthe outcome of child mortality by 5 years of age?\\\"]\\n      db [label = \\\"Generate statistics\\\\nof continuous variables\\\\nmbmi and mage, by mortality status\\\"]\\n      dc [label = \\\"Generate stratified frequency counts and\\\\npercentages of categorical variables, by mortality status\\\"]\\n      dd [label = \\\"Calculate T-tests (continuous vars)\\\\nand Chi-square tests (categorical vars) across stratified levels\\\"]\\n      de [label = \\\"Generate measures of association\\\\n(RD, RR, OR) of exposures and outcomes\\\"]\\n      \\n      df [label = \\\"Interpret results\\\"]\\n      \\n      # define edges\\n      \\n      lab3 -> {da, db, dc, dd, de, df}\\n      \\n      #da -> Workflow\\n      #db -> Explore\\n      #dc -> Explore\\n      #dd -> Model\\n      #de -> Model\\n      #df -> Communicate\\n      \\n      }\\n      \\n      {rank = same; lab2 -> lab3}\\n      \\n      \\n\\n      subgraph lab_4 {\\n    \\n      \\n      # adding node statements\\n      \\n      node [shape = circle]\\n      \\n      lab4 [label = \\\"Lab 4\\\", penwidth = 2.0]\\n      \\n            \\n      #Explore\\n      #Communicate\\n      #Model\\n\\n      node [shape = box]\\n      \\n      ea [label = \\\"Calculate measures of association (RD, RR) between\\\\nbirth order and child mortality by 5 years\\\\nwithin strata of other variables\\\"]\\n      eb [label = \\\"Test for confounding and effect measure modification\\\\ndue to stratifying variables\\\"]\\n      ec [label = \\\"Interpret results\\\"]\\n\\n      \\n      # define edges\\n      \\n      lab4 -> {ea, eb, ec}\\n      \\n      #ea -> Explore\\n      #eb -> Model\\n      #ec -> Communicate\\n      \\n      }\\n      \\n      {rank = same; lab3 -> lab4}\\n      \\n      }\\n    \",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}\r\n\r\n\r\n\r\n",
      "last_modified": "2021-11-16T11:05:31-05:00"
    },
    {
      "path": "mind_map_first.html",
      "title": "Mapping Our Steps",
      "description": "This page gives a birds-eye view of our analysis, where we've come from and where we're headed.\n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\n\r\n{\"x\":{\"diagram\":\"\\n\\n      digraph lab_0 {\\n      \\n      graph [rankdir = LR]\\n      \\n      # adding node statements\\n      \\n      node [shape = circle]\\n      \\n      lab0 [label = \\\"Lab 0\\\"]\\n      \\n      Workflow\\n      Explore\\n      Transform\\n      \\n      node [shape = box]\\n      \\n      a [label = \\\"Create workspace\\\"]\\n      b [label = \\\"Inspect dimensions\\\"]\\n      c [label = \\\"Generate summary statistics\\\"]\\n      d [label = \\\"Create derived variable for mothers age\\\"]\\n      e [label = \\\"Create categorical variable for mothers age\\\"]\\n      \\n      \\n      \\n      # add definition using node IDs\\n      \\n      lab0 -> {a, b, c, d, e}\\n      \\n      a -> Workflow\\n      b -> Explore\\n      c -> Explore\\n      d -> Transform\\n      e -> Transform\\n      \\n      }\\n\\n    \\n      \\n      \",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}\r\n\r\n\r\n{\"x\":{\"diagram\":\"\\n\\n      digraph lab_1 {\\n      \\n      graph [rankdir = LR]\\n      \\n      # adding node statements\\n      \\n      node [shape = circle]\\n      \\n      lab1 [label = \\\"Lab 1\\\"]\\n      \\n      Explore\\n      Transform\\n      Visualize\\n      Workflow\\n      Communicate\\n\\n      node [shape = box]\\n\\n      a [label = \\\"Construct derived variables\\\\nbord5, male, mweight, mbmi\\\"]\\n      b [label = \\\"Visualize with\\\\nhistograms and boxplots\\\"]\\n      c [label = \\\"Calculate frequency counts\\\\nand percentages\\\"]\\n      d [label = \\\"Create data dictionary\\\"]\\n      e [label = \\\"Consider implications of\\\\nmissing values\\\"]\\n      \\n      # edge definitions\\n      \\n      lab1 -> {a, b, c, d, e}\\n      \\n      a -> Transform\\n      b -> Visualize\\n      c -> Explore\\n      d -> Workflow\\n      e -> Communicate\\n      \\n      \\n      }\\n      \\n      \",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}\r\n\r\n\r\n{\"x\":{\"diagram\":\"\\n\\n      digraph lab_2 {\\n      \\n      graph [rankdir = LR]\\n      \\n      # adding node statements\\n      \\n      node [shape = circle]\\n      \\n      lab2 [label = \\\"Lab 2\\\"]\\n      \\n      Explore\\n      Transform\\n      Workflow\\n      Visualize\\n      Communicate\\n\\n      node [shape = box]\\n      \\n      a [label = \\\"Construct derived variables\\\\nsize, belowavg, pnc, rural\\\\neducation, death, and time\\\"]\\n      b [label = \\\"Update data dictionary\\\"]\\n      c [label = \\\"Exclude observations where child was alive\\\\nbut less than 60 months of age\\\"]\\n      d [label = \\\"Generate frequency histograms and\\\\nboxplo1ts for death and time\\\"]\\n      e [label = \\\"Calculate frequency counts\\\\nandpercentages\\\"]\\n      f [label = \\\"Consider implications of\\\\nanomalous and missing data\\\"]\\n      \\n      # define edges\\n      \\n      lab2 -> {a, b, c, d, e, f}\\n      \\n      a -> Transform\\n      b -> Workflow\\n      c -> Visualize\\n      d -> Explore\\n      e -> Explore\\n      f -> Communicate\\n      \\n      }\\n      \\n      \",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}\r\n\r\n\r\n{\"x\":{\"diagram\":\"\\n\\n      digraph lab_3 {\\n      \\n      graph [rankdir = LR]\\n      \\n      # adding node statements\\n      \\n      node [shape = circle]\\n      \\n      lab3 [label = \\\"Lab 3\\\"]\\n      \\n      Explore\\n      Workflow\\n      Communicate\\n      Model\\n\\n      node [shape = box]\\n      \\n      a [label = \\\"Define research question:\\\\nWhat are the factors that influence\\\\nthe outcome of child mortality by 5 years of age?\\\"]\\n      b [label = \\\"Generate statistics\\\\nof continuous variables\\\\nmbmi and mage, by mortality status\\\"]\\n      c [label = \\\"Generate stratified frequency counts and\\\\npercentages of categorical variables, by mortality status\\\"]\\n      d [label = \\\"Calculate T-tests (continuous vars)\\\\nand Chi-square tests (categorical vars) across stratified levels\\\"]\\n      e [label = \\\"Generate measures of association\\\\n(RD, RR, OR) of exposures and outcomes\\\"]\\n      \\n      f [label = \\\"Interpret results\\\"]\\n      \\n      # define edges\\n      \\n      lab3 -> {a, b, c, d, e, f}\\n      \\n      a -> Workflow\\n      b -> Explore\\n      c -> Explore\\n      d -> Model\\n      e -> Model\\n      f -> Communicate\\n      \\n      }\\n      \\n      \",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}\r\n\r\n\r\n{\"x\":{\"diagram\":\"\\n\\n      digraph lab_4 {\\n      \\n      graph [rankdir = LR]\\n      \\n      # adding node statements\\n      \\n      node [shape = circle]\\n      \\n      lab4 [label = \\\"Lab 4\\\"]\\n      \\n            \\n      Explore\\n      Communicate\\n      Model\\n\\n      node [shape = box]\\n      \\n      a [label = \\\"Calculate measures of association (RD, RR) between\\\\nbirth order and child mortality by 5 years\\\\nwithin strata of other variables\\\"]\\n      b [label = \\\"Test for confounding and effect measure modification\\\\ndue to stratifying variables\\\"]\\n      c [label = \\\"Interpret results\\\"]\\n\\n      \\n      # define edges\\n      \\n      lab4 -> {a, b, c}\\n      \\n      a -> Explore\\n      b -> Model\\n      c -> Communicate\\n      \\n      }\\n      \\n      \",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2021-11-16T11:06:16-05:00"
    },
    {
      "path": "submit.html",
      "title": "Lab Submission Guidelines",
      "description": "Submit your lab according to these guidelines to ensure you receive full points.\n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\nMaterials to submit\r\nFile names\r\n\r\nNotes:\r\nSubmitting\r\nRounding\r\nGrading:\r\n\r\n\r\nSubmit your documents electronically via the SAKAI DropBox by 14:00 ET on the date given for each respective lab (SAKAI time-stamps DropBox submissions).\r\nMaterials to submit\r\nR Markdown (.Rmd) file with:\r\nyour name in the YAML header\r\nrequisite code under each task subheading\r\n\r\n.html file - the file that results from hitting “Knit” on your R Markdown document\r\nExcel (.xls/.xlsx) file with tables (except for Lab 0, where this doesn’t apply)\r\nFile names\r\nName all files so that the beginning of the file name is “LabX”, where “X” is the number that corresponds with that lab assignment (no spaces between “Lab” and “X”).\r\nEXAMPLE:\r\nIf Liz Turner is submitting her lab materials for Lab 2, her files might have the following names: * Lab2_LTurner.Rmd * Lab2_LTurner.html * Lab2_LTurner.xlsx\r\nNotes:\r\nSubmitting\r\nThe Rmarkdown file should run from start to finish with no errors, including calling in the analysis dataset, and should produce all of the results and graphs to complete this assignment.\r\nLate policy: 5 point deduction per 24 hour period past due date and time.\r\nRounding\r\nWhen completing tables, follow the rounding patterns specified below. This pattern will help in grading this assignment.\r\nDo not round intermediate values. Only round final answers for submission.\r\nIf rounding for a given table is not specified or not clear, ask your lab instructor for clarification.\r\nUnless specified within the task instructions, use 3 decimal places for risks and differences and 2 decimal places for rates and ratios.\r\nTo aid in our grading of these assignments, the last figure kept should be rounded UP when the first figure dropped is >= 5. For example, in rounding to 2 decimal places, 0.235 should be reported as 0.24 and 0.245 should be reported as 0.25. Note that this method will introduce a small bias to higher numbers, but another method, “rounding-to-even”, confuses many users.\r\nGrading:\r\nRMarkdown files will be graded on an all or nothing. If your markdown file runs from start to finish with no errors, you will receive full credit. If it hits an error message, you will receive NO credit\r\n(for the purposes of grading, if any “chunk” of code in your file contains special options, we will ask you to resubmit a file omitting them).\r\n\r\nR MARKDOWN FILE: Your markdown file MUST have the following components:\r\nan informative YAML header (see guide to Lab 0[TODO])\r\na set of commands calling in all relevant packages\r\na line of code calling in the analysis dataset\r\neach task’s code performed under the appropriate subheading\r\n\r\nTABLES: a certain number of points are allocated for each item. Errors include but are not limited to: incorrect values, omissions, and inappropriate rounding. For ease of grading, please do not alter the dimensions of any table. If you are confused on which values go where, please reach out to your instructors for clarification.\r\nSHORT ANSWER QUESTIONS: will generally be graded as follows: 20% of allocated points for some sign of sentience; 40% of allocated points for getting at least part of the answer; 60% of allocated points for a pretty good answer; 80% of allocated points for an excellent, solid answer; 100% of allocated points for an out-of-the-ballpark answer.\r\n\r\n\r\n\r\n",
      "last_modified": "2021-11-16T11:06:17-05:00"
    }
  ],
  "collections": []
}

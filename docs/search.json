{
  "articles": [
    {
      "path": "705_lab_2.html",
      "title": "Lab 2 (705)",
      "description": "__Due:__ `r read.csv('files/due_dates.csv')[3,2]` by `r read.csv('files/due_dates.csv')[3,3]`\n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\nLAB MATERIALS\r\nLab 2 Goals\r\nLab 2 Grading scheme\r\nTask 1: Load libraries and\r\ndata\r\nTask 2: Code derived\r\nvariables\r\nReference\r\nand index levels of a categorical variable\r\nTask 2a: size\r\nTask 2b: belowavg\r\nTask 2c: pnc\r\nTask 2d: rural\r\nTask 2e:\r\neducation\r\nTask 2f: death\r\nTask 2g: time\r\n\r\nTask 3: Close the cohort\r\nTask 3a: Exclude\r\nirrelevant observations\r\nTask 3b: Re-code\r\ntime\r\nTask 3c: Recode\r\ntime to [time + 1]\r\nTask 3d: Recode\r\ndeath\r\nTask 3e: Save new\r\ndataset\r\n\r\nTask 4:\r\nFrequency histograms (using CLOSED COHORT)\r\nTask 5:\r\nBoxplots of time (using CLOSED COHORT)\r\nTask\r\n6: Frequency counts and percentages (using CLOSED COHORT)\r\nTask 7:\r\nUpdate data dictionary (using FULL COHORT)\r\nTask 8: Short Answer\r\n\r\n\r\n\r\n\r\nLAB MATERIALS\r\nR Markdown\r\nfile for Lab 2 Click link to download. Fill it in with your\r\nanswers to the following lab tasks. When you’re ready to submit, name it\r\nas Lab2_FirstinitialYourlastname.Rmd, and submit it using\r\nthe Sakai dropbox.\r\nExcel document for\r\nTables 1 & 2\r\nLab_2_kenya.rds - data file available on Sakai\r\nLab 2 Goals\r\nGenerate derived variables\r\nIdentify and recode special values\r\nCreate a closed cohort\r\nGenerate graphics for continuous variables\r\nRun descriptive statistics for continuous and categorical\r\nvariables\r\nGenerate a complete data dictionary\r\nLab 2 Grading scheme\r\nCompetency\r\nPoints\r\n.Rmd file runs without error\r\n10\r\nTask 4 (Histograms)\r\n15 (5 each)\r\nTask 5 (Boxplots)\r\n15 (5 each)\r\nTask 6 (Table 1)\r\n20\r\nTask 7 (Data Dictionary - old\r\nvariables)\r\n5\r\nTask 7 (Data Dictionary - new\r\nvariables)\r\n10\r\nTask 8 (short answer)\r\n25\r\nTotal\r\n100\r\n\r\nLate policy: 5 point deduction per 24 hour period past due date and time\r\nTask 1: Load libraries and\r\ndata\r\nINSTRUCTIONS: For this assignment, use the dataset\r\nthat we have supplied you in the Sakai Resources tab –\r\nLab_2_kenya.rds.\r\n\r\nFind it under Resources > Lab > Datasets > Lab_2_kenya.rds\r\nFor packages, you will need {tidyverse} and {skimr}\r\nTask 2: Code derived\r\nvariables\r\nINSTRUCTIONS: Construct the 7 derived variables\r\nlisted below using %>% and mutate(). Use\r\ncase_when() when you need your new variable to assign\r\nvalues on a conditional basis.\r\nFor each new variable, be sure to examine the component variables for\r\nmissing values (i.e. NA) and be sure to set derived\r\nvariable values appropriately (e.g., check to make sure that your NA\r\nvalues have not been mistakenly added to another category).\r\nAlso be sure that variables are of the correct\r\ntype. Convert all categorical variables to\r\nfactors with factor(), and label them\r\nappropriately. Please code each new variable within the code chunk named\r\nafter that variable.\r\nReference\r\nand index levels of a categorical variable\r\nThis distinction between reference and index levels\r\nof a categorical variable will become important as we move into\r\ncalculating measures of association like absolute risk, risk\r\ndifferences, risk ratios, and odds ratios.\r\nWe will go into this in greater depth in forthcoming lab assignments,\r\nbut for now it is simply important to recognize that a “reference level”\r\nis the designated “baseline” against which other levels of a categorical\r\nvariable (our “index levels”) are being compared.\r\nIn these labs, we have asked you to code categorical variables in\r\nsuch a way that the reference level of those variables is coded as 0,\r\nand thus takes on the lowest level when it’s converted to a\r\nfactor categorical variable. R automatically recognizes the\r\nlowest level of any factor variable as that variable’s reference group,\r\nagainst which characteristics of participants in other levels of that\r\nvariable are compared.\r\nIn terms of two-by-two contingency tables, index and reference levels\r\nare often presented like this:\r\n\r\nOutcome A \r\nOutcome B\r\n\r\nExposure\r\nDisease\r\nNo Disease\r\nTotal\r\nIndex group\r\nA1\r\nB1\r\nN1\r\nReference group\r\nA0\r\nB0\r\nN0\r\nThe following equations then apply, paying special attention to the\r\nlocation of the reference group:\r\nRisk (Index)\r\n\\(R_1 = A_1 /\r\nN_1\\)\r\nRisk (Reference)\r\n\\(R_0 = A_0 /\r\nN_0\\)\r\nRisk Difference\r\n\\(RD = R_1 -\r\nR_0\\)\r\nRisk Ratio\r\n\\(RR = R_1 /\r\nR_0\\)\r\nIncidence Odds Ratio\r\n\\(IOR =\r\n\\frac{A_1}{B_1} \\div \\frac{A_0}{B_0}\\)\r\nTask 2a: size\r\nsize: Categorical variable describing\r\nsize of child at birth (subjectively described by mother). Use variable\r\nm18 to code the new variable with the following levels and\r\nlabels (note that greater values indicate smaller size):\r\nLevel\r\nLabel\r\n0\r\nvery large\r\n1\r\nlarger than average\r\n2\r\naverage\r\n3\r\nsmaller than average\r\n4\r\nvery small\r\nTask 2b: belowavg\r\nbelowavg: Dichotomous variable\r\nindicating if the child’s size was below average. Use the newly derived\r\nvariable size to code the new variable with the following\r\nlevels and labels:\r\nLevel\r\nLabel\r\n0\r\naverage, larger than average, or very\r\nlarge\r\n1\r\nsmaller than average or very small\r\nTask 2c: pnc\r\npnc: Dichotomous categorical variable\r\nfor any prenatal care. Use the variable m2n to code the new\r\nvariable with the following levels and labels.\r\nLevel\r\nLabel\r\n0\r\nno prenatal care (reference)\r\n1\r\nreceived prenatal care (index)\r\nTask 2d: rural\r\nrural: Dichotomous indicator of rural\r\nresidence. Use the variable v025 to code the new variable\r\nwith the following levels and labels:\r\nLevel\r\nLabel\r\n0\r\nurban\r\n1\r\nrural\r\nTask 2e: education\r\neducation: Categorical educational\r\nlevel obtained by mother. Use the variable s109 to code the\r\nnew variable with the following levels and labels.\r\nLevel\r\nLabel\r\n0\r\ndid not attend school\r\n1\r\nprimary school only\r\n2\r\npost-primary education\r\nTask 2f: death\r\ndeath: Dichotomous categorical variable\r\nfor whether or not a child was dead or alive at the time of interview.\r\nBased on the variables b5, code death with the\r\nfollowing levels and labels:\r\nLevel\r\nLabel\r\n0\r\nAlive at interview (reference)\r\n1\r\nDead at interview (index)\r\nTask 2g: time\r\ntime: Continuous variable for the age\r\nat death OR age at interview for children still alive.\r\nYou will use case_when() to create this derived variable\r\naccording to the following conditions:\r\nIf a child was dead at the time of interview, assign it the value in\r\nvariable b7 (child’s age at death).\r\nIf a child was alive at the time of interview, calculate their age\r\nusing using v008 (date of interview) and b3\r\n(date of birth). Be sure to note that times are given in months.\r\nTask 3: Close the cohort\r\nINSTRUCTIONS: Create the equivalent of a\r\nclosed cohort for the analysis of 5-year\r\nchildhood mortality from the dataset.\r\nNote: “Closed cohorts” is a topic that we will cover\r\nin more detail in Module 7 (week 11, the first week of November), so\r\nplease don’t worry if this feels unfamiliar. The steps needed to\r\ncomplete this task are described in detail below. We hope that\r\nperforming the actual task of closing a cohort before we learn about it\r\nformally in Module 7 will help to crystallize concepts around\r\nPopulations, Time, Measures of Disease Frequency and Association.\r\nFor now, we will stick with the following description:\r\nIn a closed cohort, everyone must be at risk of the outcome at entry\r\ninto the cohort, and all members of the cohort must remain at risk until\r\nthey experience the outcome or complete the entire risk period for the\r\noutcome.\r\nHere is an example of the various logical outcomes around which we\r\nwill be closing the cohort:\r\nClosed cohortA closed cohort allows for estimation of absolute (unconditional)\r\nrisks and associated effect measures. For this study, the risk period\r\nfor child mortality begins at birth and ends when a child dies, or when\r\n60 months of life have been completed. I recommend completing the steps\r\nbelow in order to generate what we want. There are other orders in which\r\nto do this, but sometimes you will get incorrect results. If you’re\r\nadventurous, we recommend trying other ways to code this; you’ll learn a\r\nlot from that exercise.\r\nANOTHER NOTE: Complete all of task 3 before\r\ncompleting the remaining tasks.\r\nTask 3a: Exclude\r\nirrelevant observations\r\nINSTRUCTIONS: For this task, you will use\r\n%>% and filter() to exclude those\r\nobservations from the dataset where the child was alive\r\n(death == 0) and follow-up time was less than 60 months\r\n(time < 60) Think about which Boolean operator you’ll\r\nneed to use to exclude those children on both of those conditions. You\r\nshould end up with 16,828 records (i.e. rows)\r\nin the new data frame.\r\nIn this step, you should also use an assignment operator\r\n(<-) to assign this closed cohort to a different object\r\nname (effectively creating a “new” data frame). That way, you will be\r\nable to work with this data frame separately, while maintaining access\r\nto the original cohort of n = 22,534.\r\nTask 3b: Re-code time\r\nINSTRUCTIONS: If a child was still alive by their\r\n5th birthday, we need to re-code their variable for time to\r\n60 (we are effectively censoring these observations). These children\r\nwere alive at 5 years of age and so should not be counted as deaths in\r\nour analysis.\r\nYou will do this in your newly closed cohort data frame using\r\nmutate() to re-generate time according to the\r\nfollowing conditions:\r\nIf time >= 60, then set time to 60\r\nOtherwise, use the original coding for time.\r\nHint: In case_when() you can use the\r\nstatement TRUE ~ time at the end of your list of\r\nconditional statements to express this sentiment of “otherwise, use\r\ntime”.\r\nTask 3c: Recode time\r\nto [time + 1]\r\nINSTRUCTIONS: For this task, re-code\r\ntime to indicate the month of life during which death or\r\ninterview occurred (add 1 month to the current value). The range on\r\ntime should now be 1 month – 61 months.\r\nTo help understand why we might do this, imagine that the interview\r\noccurred in the same month that a child was born. For them,\r\ntime is coded as 0. But for the purposes of analysis, that\r\nchild’s age should be considered as 1 month. Every observation for\r\ntime contains this error, which is why we must add 1 to\r\ntime.\r\n\r\nLinking to our in-class learning, we would formally identify this as a\r\ntype of “systematic error”\r\nJust like in 3b, you will use mutate() to simply\r\nre-generate time with the equation\r\ntime + 1.\r\nTask 3d: Recode death\r\nNow that time is re-coded, we have some conflicting information\r\nbetween the variables death and time. Consider\r\nthis question: since we are only considering the mortality status of\r\nchildren before the age of 5, should children who died after 60\r\nmonths be coded as dead or alive?\r\nThe answer: They are technically alive\r\nwithin the time frame of our analysis\r\nINSTRUCTIONS: Re-code death so that\r\nwhen time >= 61, death == \"alive\".\r\nTask 3e: Save new dataset\r\nSave this new dataset containing 16,828 records (make sure to give it\r\na new name).\r\nJust as in lab 0, use saveRDS() to complete this task,\r\nusing the following syntax:\r\n\r\n\r\nsaveRDS(dataName, file = \"NewDatasetName.rds\")\r\n\r\n\r\n\r\nTask 4:\r\nFrequency histograms (using CLOSED COHORT)\r\nGenerate frequency histograms of time, overall and\r\nseparately for levels of death. Use labs() to\r\nput meaningful axis labels and a title on each figure.\r\nTip: You can filter your data and create a\r\nvisualization in a single pipeline by connecting your\r\nfilter() command to ggplot() with\r\n%>%. As a quick example, we will use the\r\nstarwars dataset to create a histogram of character\r\nheights. But say we only want to see the distribution of Human\r\nheights. We could do it like this:\r\n\r\n\r\nstarwars %>%\r\n  filter(species == \"Human\") %>%\r\n  ggplot(aes(x = height)) +\r\n  geom_histogram(binwidth = 10, fill = '#BBAC12')\r\n\r\n\r\n\r\n\r\n\r\nPlease note that in the ggplot() command, we do not need to\r\nsupply the data = argument. Your pipe, %>%\r\nas already taken care of that behind-the scenes.\r\nOn the other hand, if you wanted to generate a series of stratified\r\nhistograms according to a variable like gender, you could\r\ninstead use facet_wrap():\r\n\r\n\r\nggplot(data = starwars, aes(x = height)) +\r\n  geom_histogram(binwidth = 20, fill = \"#AC12BB\") +\r\n  facet_wrap(. ~ gender)\r\n\r\n\r\n\r\n\r\nHelp\r\nwith histograms in ggplot2\r\nTask 5: Boxplots of\r\ntime (using CLOSED COHORT)\r\nSimilar to your histograms, generate boxplots of time,\r\noverall and separately for levels of death. Use\r\nlabs() to put meaningful axis labels and a title on each\r\nfigure.\r\nHelp\r\nwith boxplots in ggplot2\r\nTask 6:\r\nFrequency counts and percentages (using CLOSED COHORT)\r\nIf you haven’t already, download the Excel file that’s linked at\r\nthe top of this webpage. Then open the file, which is named\r\nLab2_Tables.xlsx.\r\nFill in the sheet named Table 1 with the frequency counts and\r\npercentages for the levels of the new categorical variables you have\r\ngenerated. Calculate percentages only for the non-missing values.\r\nRound percentages to 1 decimal place.\r\nRefer to the\r\nprevious lab for help on frequency counts and\r\npercentages.\r\nVariables:\r\nSize at birth (size)\r\nSize at birth categorical (belowavg)\r\nPrenatal care (pnc)\r\nResidence type (rural)\r\nMother’s education (education)\r\nDeath by 5 years (death)\r\nTask 7: Update\r\ndata dictionary (using FULL COHORT)\r\nUsing the second sheet in the Excel file, which has been named\r\nTable 2, update your data dictionary for the Kenya\r\ndataset by adding the newly created variables.\r\nTask 8: Short Answer\r\nExamine the range and proportion of missing for each of the variables\r\n7 you have created in this lab. Are there characteristics of any of\r\nthese variables that are concerning (e.g., missing, suspicious or\r\nimpossible values)? In contemplating analysis of these data, what do you\r\nthink should be done with anomalous information? What effect would\r\nmissing values have on the validity of your analyses (e.g., how might\r\nmissing or extreme values affect inferences)?\r\nProvide your answer to this question within your own RMarkdown\r\nfile\r\n\r\n\r\n\r\n",
      "last_modified": "2022-05-31T13:52:38-04:00"
    },
    {
      "path": "705_lab_3.html",
      "title": "Lab 3 (705)",
      "description": "__Due:__ `r read.csv('files/due_dates.csv')[4,2]` by `r read.csv('files/due_dates.csv')[4,3]`\n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\nLAB MATERIALS\r\nLab 3 Goals\r\nLab 3 Grading scheme\r\nTask 1: Load libraries\r\nand dataset\r\nTask 2: BIG PICTURE\r\nTask 3: Table 1\r\nTabular\r\nanalysis of continuous variables by mortality status\r\nOverall\r\nsummary statistics for each variable variable\r\nStratified summary\r\nstatistics\r\nT-tests and\r\nKruskal-Wallis tests\r\n\r\nTask 4: Table 2\r\nTabular\r\nanalysis of categorical variables by death\r\n{tableone} package\r\nCreate a vector of\r\nvariables\r\nConstruct {tableone}\r\nobjects\r\nPrint table\r\nFisher’s exact test of\r\nsignificance\r\n\r\nTask 5: Table 3\r\nRisk\r\nDifferences, Risk Ratios, and Odds Ratios\r\nTake a deep breath.\r\nFrequency counts\r\nRisk\r\nRisk\r\ndifference (RD), Risk Ratio (RR), and Odds Ratio (OR)\r\nmAssoc()\r\nfor Measures of Association\r\n\r\nTask 6: Side-by-side\r\nboxplots\r\nTask 7: Short answer\r\nTask 8: Short answer\r\nTask 9: Short answer\r\nTask 10: Short answer\r\nTask 11: Short answer\r\n\r\n\r\n\r\n\r\nLAB MATERIALS\r\nR Markdown\r\nfile for Lab 3 Click link to download. Fill it in with your\r\nanswers to the following lab tasks. When you’re ready to submit, name it\r\nas Lab3_FirstinitialYourlastname.Rmd, and submit it using\r\nthe Sakai dropbox.\r\nExcel document\r\nwith Tables 1, 2, and 3\r\nLab_3_kenya.rds - data file available on Sakai\r\nLab 3 Goals\r\nAssess the bivariate relationships between outcome and\r\ncovariables and the statistical association between each of the two\r\nvariables\r\nEstimate the epidemiologic measures of association (risk\r\ndifference, risk ratio and odds ratio) between risk factors and\r\noutcome\r\nLab 3 Grading scheme\r\nCompetency\r\nPoints\r\n.Rmd file runs without error\r\n10\r\nTask 3 (Table 1)\r\n5\r\nTask 4 (Table 2)\r\n10\r\nTask 5 (Table 3)\r\n20\r\nTask 6 (stratified boxplots)\r\n10 (5 each)\r\nTask 7 (short answer)\r\n5\r\nTask 8 (short answer)\r\n10\r\nTask 9 (short answer)\r\n5\r\nTask 10 (short answer)\r\n5\r\nTask 11 (short answer)\r\n20\r\nTotal\r\n100\r\n\r\nLate policy: 5 point deduction per 24 hour period past due date and time\r\nTask 1: Load libraries and\r\ndataset\r\nFor this assignment, we’ll need packages {tidyverse}, {skimr},\r\n{epiR}, {tableone}, and {epiAssist}\r\nFor your data, you will use the dataset Lab_3_kenya.rds,\r\nlocated in the Sakai Resources folder for this course. This has all of\r\nthe variables that we’ve created in labs 0, 1, and 2, and only contains\r\nthose observations in our closed cohort (n = 16,828).\r\nTask 2: BIG PICTURE\r\nJust\r\nthink on this for a moment. Internalize it. Come back to it:\r\nFor this and subsequent labs, we are considering\r\nChild mortality within 5 years as our outcome (response)\r\nvariable\r\nBirth order >= 5 as the main risk factor variable and\r\nOther variables as potential confounders\r\nor modifiers of the relationship between\r\nmortality and birth order.\r\nCausal diagram of birth order (E:\r\n“exposure”) on childhood mortality (D: “disease”)Task 3: Table 1\r\nTabular\r\nanalysis of continuous variables by mortality status\r\nINSTRUCTIONS: Fill in Table 1 with the\r\ndescriptive statistics for maternal BMI (mbmi) and maternal\r\nage (mage), overall and by the two levels of child\r\nmortality within 5 years (death).\r\n\r\nReminder that you can find Tables 1-3 in the Excel file provided in the\r\nLab\r\nMaterials for this lab\r\nThis table also requires you to use R to calculate statistical tests\r\n(T-test and Kruskal-Wallis) of the association between the outcome,\r\ndeath, and maternal BMI and age (i.e., comparing the\r\ndistributions of maternal BMI and maternal age for the children who died\r\nversus those who were alive).\r\nRound p-values to 3 decimal places.\r\nRead on for advice on how to generate overall and stratified summary\r\nstatistics, as well as instructions for how to perform the two\r\nstatistical tests.\r\nOverall\r\nsummary statistics for each variable variable\r\nThe skim() function provided by {skimr} makes this task a simple one.\r\nHowever, unlike previous labs where we ran a summary on the entire\r\ndataset, in this task we want individual variables. To keep your output\r\nclean, we recommend you generate your summary statistics only for the\r\nvariables of interest.\r\nWe can use a\r\npipe with skimr() to call specific variables, like this:\r\n\r\n\r\ndata %>%\r\n  skim(variable)\r\n\r\n\r\n\r\nStratified summary\r\nstatistics\r\nSince we’re already doing things in a pipe, we can use the filter\r\nfunction to retrieve summary statistics based on mortality status. To\r\nstratify by the variable death, we should add the\r\ngroup_by() function to our pipeline, specifying that we\r\nwant to “group by death, then skim the variable\r\nmbmi”:\r\n\r\n\r\ndata %>%\r\n  group_by(death) %>%\r\n  skim(mbmi)\r\n\r\n\r\n\r\nNote: Table 1 asks for counts of records with mbmi\r\nand mage data available vs. those missing, both overall and by the 2\r\nlevels of death (i.e. dead and alive). If you look closely,\r\nthis means that we need counts of counts of kids with data available on\r\nmbmi and mage, overall, within levels of\r\ndeath, and those missing values. skim() does not give you\r\nraw counts of the variable death that account for those\r\nmissing values.\r\nUse table() or a pipe, group_by() and\r\ncount() to get the counts of children in each group of\r\ndeath (i.e. by levels of death). Then use the\r\ncolumn of your skim output, n_missing, to subtract missing\r\nvalues from groups\r\nT-tests and Kruskal-Wallis\r\ntests\r\nT-tests and one of their nonparametric cousins, Kruskal-Wallis, can\r\nbe used to compare means between between groups (i.e. between\r\nsub-populations), with the null hypothesis (H0) being that\r\nall means are equal.\r\nThere are a few simple functions from the {stats} package that we can\r\nuse to manually run these tests. To implement, we use functions\r\nt.test() and kruskal.test(). Both take the\r\nsame arguments:\r\n\r\nMany functions in R can perform a task like generating tables, while\r\ncomputing a p-value as a matter of course (see Task\r\n4.\r\nA formula: depVar ~ indepVar\r\nA data frame\r\nIn this case, we are interested in knowing whether or not the means\r\nfor mbmi and mage differ by strata of\r\nchild mortality status, death.\r\nWhat this means is that when we’re building our formula, the\r\n“y”/dependent variable (the resulting means), should be on the left,\r\nwhile the “x”/independent variable (a categorical stratifying variable),\r\nshould be on the right.\r\n\r\nFormulas: In R, “formulas” follow different rules depending on the\r\nfunction. As a general rule, most take the form y ~ x. Go\r\nhere to learn more.\r\nExample of T-tests and\r\nKruskal-Wallis\r\nAs an example, we might use the starwars dataset,\r\navailable through the {dplyr} package (in the tidyverse), to inspect the\r\nbody mass of different characters by gender. Let’s take a look at the\r\nraw data:\r\n\r\nname\r\nheight\r\nmass\r\nhair_color\r\nskin_color\r\neye_color\r\nbirth_year\r\nsex\r\ngender\r\nhomeworld\r\nspecies\r\nLuke Skywalker\r\n172\r\n77\r\nblond\r\nfair\r\nblue\r\n19.0\r\nmale\r\nmasculine\r\nTatooine\r\nHuman\r\nC-3PO\r\n167\r\n75\r\nNA\r\ngold\r\nyellow\r\n112.0\r\nnone\r\nmasculine\r\nTatooine\r\nDroid\r\nR2-D2\r\n96\r\n32\r\nNA\r\nwhite, blue\r\nred\r\n33.0\r\nnone\r\nmasculine\r\nNaboo\r\nDroid\r\nDarth Vader\r\n202\r\n136\r\nnone\r\nwhite\r\nyellow\r\n41.9\r\nmale\r\nmasculine\r\nTatooine\r\nHuman\r\nLeia Organa\r\n150\r\n49\r\nbrown\r\nlight\r\nbrown\r\n19.0\r\nfemale\r\nfeminine\r\nAlderaan\r\nHuman\r\nOwen Lars\r\n178\r\n120\r\nbrown, grey\r\nlight\r\nblue\r\n52.0\r\nmale\r\nmasculine\r\nTatooine\r\nHuman\r\n\r\nOur formal question might look like this: “Does mean character height\r\ndiffer by a character’s gender?” (For the purposes of example, assume\r\nthat the 83 characters are a sample from a large population of\r\ncharacters)\r\nWe can calculate summary statistics for sub-group frequency counts\r\nand means:\r\n\r\n\r\nstarwars %>%\r\n  filter(!is.na(gender)) %>%\r\n  group_by(gender) %>%\r\n  summarize(n = n(), mean = mean(height, na.rm = TRUE))\r\n\r\n\r\n\r\n\r\ngender\r\nn\r\nmean\r\nfeminine\r\n17\r\n164.6875\r\nmasculine\r\n66\r\n176.5161\r\n\r\nWe can also visualize the respective distributions of\r\nheight by gender using density plots.\r\n\r\n\r\n\r\n\r\nThe hypotheses of the two-sample t-test ask whether the mean height of\r\nfeminine and masculine are different. To address this question, data\r\nfrom the 17 feminine and 66 masculine characters are used as samples\r\ndrawn from the larger population.\r\nOur t-test would effectively tell us whether or not the observed\r\ndifference in sample mean of height supports the conclusion that the\r\npopulation mean height is different between the two groups.\r\n\r\n\r\nt.test(height ~ gender, data = starwars)\r\n\r\n#>  Welch Two Sample t-test\r\n#>\r\n#> data:  height by gender\r\n#> t = -1.5596, df = 37.315, p-value = 0.1273\r\n#> alternative hypothesis: true difference in means between group feminine and group masculine \r\n#> is not equal to 0\r\n#> 95 percent confidence interval:\r\n#>  -27.191682   3.534423\r\n#> sample estimates:\r\n#>  mean in group feminine mean in group masculine \r\n#>                164.6875                176.5161 \r\n\r\n\r\n\r\n\r\nFunction output is indicated by lines that start with #>\r\nHowever, since the category for gender == 'feminine'\r\nonly contains 17 observations, we might decide that a nonparametric test\r\n(one that doesn’t assume a normal distribution), might be more\r\nappropriate for the small sample size. In this case we can use the\r\nKruskal-Wallis test:\r\n\r\n\r\nkruskal.test(height ~ gender, data = starwars)\r\n\r\n#> Kruskal-Wallis rank sum test\r\n#> \r\n#> data:  height by gender\r\n#> Kruskal-Wallis chi-squared = 8.6845, df = 1, p-value = 0.003209\r\n\r\n\r\n\r\n\r\nNote that our decision tree for statistical tests of continuous outcome\r\nvariables would get us to the Wilcoxon Rank Sum Test. In R, this is\r\nimplemented the same as the Kruskal Wallis test, but uses the function\r\nwilcox.test(). We have used kruskal.test()\r\nbecause this is the test we will perform for Table 1.\r\nNotice the difference in the conclusion you would draw from either\r\ntest. For the t-test, we should conclude that we cannot reject the null\r\nhypothesis, whereas the nonparametric test tells us that there is a\r\nstatistically significant difference in the mean height of the two\r\ngroups.\r\nTask 4: Table 2\r\nTabular\r\nanalysis of categorical variables by death\r\nINSTRUCTIONS: Table 2 contains three\r\ndistinct columns for frequency counts and percentages.\r\nThe first, “All Children” requires the overall\r\nfrequency counts and percentages by the categorical variables of\r\ninterest. These are the same values that you generated for Table 1 in\r\nboth labs 1 and 2.\r\nThe next two columns, “Dead” and “Alive”, require frequency counts\r\nand percentages of each of the variables of interest, stratified by\r\nmortality status at 5 years, death. Be sure to calculate\r\npercentages only for the non-missing values and round percentages to 1\r\ndecimal place.\r\nThis table also includes statistical measures (Chi-Sq and Fisher’s\r\nexact test) of the association between mortality status and the\r\nvariables in the table.\r\nRound p-values to 3 decimal places.\r\n{tableone} package\r\nWe will complete this task by first using the package {tableone}, a\r\npowerful tool for tabular analysis, especially for stratified analyses\r\nlike this one.\r\nMore on {tableone}: Find a full tutorial of {tableone}\r\nhere\r\nThe function CreateTableOne() will create a {tableone}\r\nobject that generates mean and standard deviation for continuous\r\nvariables, and frequency counts and percentages for factor\r\nvariables.\r\nCreateTableOne() requires several arguments of central\r\nimportance for it to run:\r\n1.) vars = - a vector of variable names, written as\r\nstrings\r\n2.) strata = - a categorical variable for stratifying (if\r\ndesired)\r\n3.) data = - your data frame\r\n4.) factorVars = - used to convert character variables to\r\nfactors on the fly\r\nfactorVars: You shouldn’t need to use\r\nfactorVars = for your kenya data, since we\r\nhave already converted categorical variables to factors in the previous\r\nlabs.\r\nCreate a vector of variables\r\nFor starters, we’ll need a vectorized list of our variables of\r\ninterest, written as strings (in quotes).\r\nAs an example, we might use the derived variables in our kenya\r\ndataset for size and pnc. To make our code\r\nmore readable, we will create a vector object that contains both\r\nvariable names. To do this, we put both variable names in quotes,\r\nseparating them by commas within a vector, c(). Then we\r\nneed to assign the vector a name so that we can call it later. Here,\r\nwe’ve named the vector variables.\r\n\r\n\r\nvariables <- c('size', 'pnc')\r\n\r\n\r\n\r\nConstruct {tableone} objects\r\nFirst, we will construct two different {tableone} objects: a “simple”\r\ntable with univariate frequency counts, and a “stratified” table, with\r\nbivariate counts.\r\nAfter that, we’ll use print() to view the output\r\ngenerated by those two different objects.\r\nSimple table\r\nFor a simple table of summary statistics, we only need to use\r\narguments vars = and data =. It’s best\r\npractice to assign the {tableone} object to a name, so that we can use\r\nit in a print() function in a moment.\r\n\r\n\r\nsimple_tab <- CreateTableOne(vars = variables, \r\n                             data = kenya)\r\n\r\n\r\n\r\nStratified table\r\nWe may also want to stratify our results by a second variable, like\r\nbord5.\r\n\r\n\r\nstratified_tab <- CreateTableOne(vars = variables, \r\n                                 data = kenya, \r\n                                 strata = 'bord5')\r\n\r\n\r\n\r\nPrint table\r\nWe can then use a print() function, with arguments\r\nspecific to our {tableone} object, to view our simple and stratified\r\ntables.\r\nWith categorical variables, it’s typically best to use the argument\r\nshowAllLevels = TRUE so that we see every category.\r\n\r\nSetting showAllLevels to FALSE will leave out\r\nthe reference level for each variable.\r\nPrint simple table\r\n\r\n\r\nprint(simple_tab, showAllLevels = TRUE)\r\n\r\n#>              level                  Overall      \r\n#>  n                                   16828        \r\n#>  size (%)     Very large                26 ( 7.4) \r\n#>               Larger than average       76 (21.7) \r\n#>               Average                  158 (45.1) \r\n#>               Smaller than average      65 (18.6) \r\n#>               Very small                25 ( 7.1) \r\n#>  pnc (%)      No prenatal care          15 ( 9.1) \r\n#>               Received prenatal care   149 (90.9) \r\n\r\n\r\n\r\nPrint\r\nstratified table with chi-square test of independence\r\nWhereas the simple table prints overall frequency counts and\r\npercentages, our stratified table will take the stratifying variable (in\r\nthe case of this example, bord5), and perform a chi-square\r\ntest of independence for that stratifying variable and each of the\r\nvariables of interest e.g. of size and\r\npnc:\r\n\r\n\r\nprint(stratified_tab, showAllLevels = TRUE)\r\n\r\n#>              Stratified by bord5\r\n#>               level                  1-4 in Birth Order 5+ in Birth Order  p     test\r\n#>  n                                   13263              3565                         \r\n#>  size (%)     Very large                23 ( 9.3)          3 ( 2.9)        0.157     \r\n#>               Larger than average       51 (20.6)         25 (24.5)                  \r\n#>               Average                  108 (43.5)         50 (49.0)                  \r\n#>               Smaller than average      50 (20.2)         15 (14.7)                  \r\n#>               Very small                16 ( 6.5)          9 ( 8.8)                  \r\n#>  pnc (%)      No prenatal care           4 ( 3.7)         11 (20.0)        0.002     \r\n#>               Received prenatal care   105 (96.3)         44 (80.0)       \r\n\r\n\r\n\r\nNote that each row of the stratified results sum to the same result\r\nas given by the column for “Overall” in our simple_tab.\r\nAs the default for categorical variables, the {tableone}\r\nprint() function performs a chi-square test of independence\r\nbetween `bord5’ and each of the variables of interest.\r\nThere’s one glaring issue here, however: neither of our stratified\r\nvariables meet the assumption of normality! Each contains at least one\r\ncross-tabulation with counts that are less than 5.\r\nFisher’s exact test of\r\nsignificance\r\nWhen we look at our decision tree for hypothesis testing of\r\ncategorical outcome variables, we might decide that we need to use a\r\nFisher’s Exact test when our data doesn’t meet the assumption of all\r\nexpected cell counts being at least 5 for a chi-square test of\r\nindependence.\r\n\r\nYou can use the output from chisq.test() to find expected\r\ncell counts:\r\n\r\n\r\nsize_tab <- table(kenya$bord5, kenya$pnc)\r\nX2 <- chisq.test(size_tab)\r\nX2$expected\r\n\r\n\r\n\r\nTo do this in a {tableone} object, we just need to include\r\nexact = TRUE in our print() function to\r\ncalculate a Fisher’s Exact test, the nonparametric equivalent of the\r\nchi-square test of independence.\r\n\r\n\r\nprint(stratified_tab, showAllLevels = TRUE, exact = TRUE)\r\n\r\n#>               Stratified by bord5\r\n#>               level                  1-4 in Birth Order 5+ in Birth Order p      test \r\n#>  n                                   13263              3565                          \r\n#>  size (%)     Very large                23 ( 9.3)          3 ( 2.9)        0.141 exact\r\n#>               Larger than average       51 (20.6)         25 (24.5)                   \r\n#>               Average                  108 (43.5)         50 (49.0)                   \r\n#>               Smaller than average      50 (20.2)         15 (14.7)                   \r\n#>               Very small                16 ( 6.5)          9 ( 8.8)                   \r\n#>  pnc (%)      No prenatal care           4 ( 3.7)         11 (20.0)        0.001 exact\r\n#>               Received prenatal care   105 (96.3)         44 (80.0)                   \r\n\r\n\r\n\r\nAlthough in this particular example we draw the same conclusions from\r\nthe Fisher’s exact test as we do from the chi-square test of\r\nindependence, this will not always be the case.\r\n\r\nThere are other arguments for {tableone} that allow you to specify\r\nexactly which variables do not meet the assumptions of the default\r\nparametric test (e.g. chi-square test of independence for categorical\r\nvariables), and therefore perform the nonparametric test on only those\r\nvariables. Similar steps apply for analysis of continuous variables (you\r\ncould even try using the continuous variables from Task 3!)\r\nTask 5: Table 3\r\nRisk Differences,\r\nRisk Ratios, and Odds Ratios\r\nINSTRUCTIONS: Fill in Table 3 with\r\nfrequency counts overall and by levels of child mortality within 5 years\r\nand with the stratum-specific risk of death (these are the same\r\nfrequency count values, N, as in Table 2). Then use\r\nmAssoc() to compute the risk difference, risk ratio and the\r\nodds ratio of death comparing levels for each of the 5 variables to the\r\nreference level.\r\nTable 3\r\nWe use mAssoc() in the package {epiAssist} to generate\r\nthese values. This function is a helper function for the original\r\nepi.2by2() function, which is available in the package\r\n{epiR}. epi.2by2() works the same as mAssoc(),\r\nexcept it only provides output for a single index/reference level pair.\r\nmAssoc() can do multiple index levels all at once.\r\nThe reference level for each variable is noted in the spreadsheet\r\n(“REF”). For the two 3-level variables, calculate risk differences and\r\nratios for each non-reference level relative to the reference level.\r\nRound risks to 3 decimal places and ratios to 2 decimal places.\r\nTake a deep breath.\r\nDon’t let this table overwhelm you. We’re going to break it down\r\nstep-by-step\r\nFrequency counts\r\nThe frequency counts in this table align with those generated for\r\nTable 2. Feel free to use Excel equations to transfer data from sheet 2\r\nto sheet 3.\r\nFrequency countsRisk\r\nThe value for risk should be calculated as the risk of the outcome of\r\ninterest (death == \"Dead\") according to each level of the\r\nexposure. Risk should be reported as a proportion, rounding to the third\r\ndecimal.\r\nRiskRisk\r\ndifference (RD), Risk Ratio (RR), and Odds Ratio (OR)\r\nMeasures of associationRD, RR, and OR are what are known as “measures of association”\r\nbecause they are used to compare outcomes between levels of a\r\ncategorical variable. In epidemiology, we refer to the baseline level as\r\nthe “reference level” and the comparator level as the “index level”.\r\nIn practical terms, for things like RRs and ORs, the index level goes\r\nin the numerator and the reference level goes in the denominator.\r\n\\[\\frac{index\\ \\ level}{reference\\ \\\r\nlevel}\\]\r\nYou can use your underlying knowledge of these measures of\r\nassociation to calculate our answers by hand and confirm the results\r\nthat we’ll obtain using the functions that follow.\r\nmAssoc() for\r\nMeasures of Association\r\nFor this task, we will use mAssoc() to generate all\r\nthree measures of association (RD, RR, and OR) in a singular output.\r\nTo use mAssoc(), we must take several steps to ensure\r\nthat our data is in the proper shape for our measures to be calculated\r\nappropriately:\r\n1.) Create a table object using table()\r\n2.) Rearrange the table with flipTable() to put it in the\r\nappropriate alignment for use by mAssoc()\r\n3.) Submit our rearranged table to mAssoc().\r\nmAssoc()’s main input is a table object arranged in the\r\nfollowing format:\r\n\r\n\r\n#   -----------  ----------  ----------\r\n#                Outcome +   Outcome -\r\n#   -----------  ----------  ----------\r\n#     Expose +   cell 1      cell 3          \r\n#     Expose -   cell 2      cell 4\r\n#   -----------  ----------  ----------\r\n\r\n\r\n\r\nAs a way of demonstrating, we will use the variable\r\nbord5, stratified by death to calculate our\r\nmeasures of association.\r\nStep 1: Create a table\r\nobject with table()\r\nFirst, we need to generate a table object and assign it a name. Keep\r\nin mind that we want our exposure variable (in this case,\r\nbord5) along the left side of the table and our outcome\r\nvariable (death) at the top of the table.\r\n\r\n\r\ntab_bord5 <- table(kenya$bord5, kenya$death)\r\ntab_bord5\r\n\r\n#>                      Alive  Dead\r\n#>  1-4 in Birth Order 11809  1454\r\n#>  5+ in Birth Order   3002   563\r\n\r\n\r\n\r\nStep 2: Rearrange table\r\nwith flipTable()\r\nNotice that mAssoc() takes a table object\r\nwith the cross-tabulation of outcome of interest (Outcome +) and\r\nexposure of interest (Expose +) in the top-left cell:\r\n\r\nReminder that the function mAssoc() is a wrapper function\r\nfor the epi.2by2() function from package {epiR}.\r\n\r\n\r\n#   -----------  ----------  ----------\r\n#                Outcome +   Outcome -\r\n#   -----------  ----------  ----------\r\n#     Expose +   cell 1      cell 3          \r\n#     Expose -   cell 2      cell 4\r\n#   -----------  ----------  ----------\r\n\r\n\r\n\r\nIf we refer to our tableof bord5 and death, we’ll notice that it\r\ncurrently displays the inverse of the arrangement required by\r\nmAssoc():\r\n\r\n\r\ntab_bord5\r\n\r\n#>                       Alive  Dead\r\n#>  1-4 in Birth Order  11809   1454\r\n#>  5+ in Birth Order    3002    563\r\n\r\n\r\n\r\nThat is, our tabulation of bord5 currently takes the following form,\r\nwith “Outcome-” and “Expose-” in the top-left corner. We need “Outcome+”\r\nand “Expose+” in that corner:\r\n\r\n\r\n#   -----------  ----------  ----------\r\n#                Outcome -   Outcome +\r\n#   -----------  ----------  ----------\r\n#     Expose -   cell 1      cell 3          \r\n#     Expose +   cell 2      cell 4\r\n#   -----------  ----------  ----------\r\n\r\n\r\n\r\nTo fix this, we can use flipTable(), assigning it back\r\nto the same name for the table object:\r\n\r\n\r\ntab_bord5 <- flipTable(tab_bord5)\r\n\r\ntab_bord5\r\n\r\n#>                      Dead  Alive\r\n#>  5+ in Birth Order    563   3002\r\n#>  1-4 in Birth Order  1454  11809\r\n\r\n\r\n\r\nGenerate measures\r\nof association with mAssoc()\r\nTo generate our measures of association and their accompanying\r\nconfidence intervals, all we need to do is submit our rearranged table\r\nto mAssoc(), specifying that our data is cohort data with\r\nmethod = \"cohort.count\", and setting our confidence level\r\nto 0.95 with conf.level = .95:\r\n\r\n\r\nmAssoc(tab_bord5, method = \"cohort.count\", conf.level = 0.95)\r\n\r\n#> MEASURES OF ASSOCIATION FOR:  5+ in Birth Order (index) vs.  1-4 in Birth Order (referent)\r\n#> Point estimates and 95% CIs:\r\n#> -------------------------------------------------------------------\r\n#> Inc risk ratio                                 1.44 (1.32, 1.58)\r\n#> Odds ratio                                     1.52 (1.37, 1.69)\r\n#> Attrib risk in the exposed *                   0.05 (0.04, 0.06)\r\n#> Attrib fraction in the exposed (%)            30.58 (24.05, 36.56)\r\n#> Attrib risk in the population *                0.01 (0.00, 0.02)\r\n#> Attrib fraction in the population (%)          8.54 (6.20, 10.82)\r\n#> -------------------------------------------------------------------\r\n#> Uncorrected chi2 test that OR = 1: chi2(1) = 62.125 Pr>chi2 = <0.001\r\n#> Fisher exact test that OR = 1: Pr>chi2 = <0.001\r\n#>  Wald confidence limits\r\n#>  CI: confidence interval\r\n#>  * Outcomes per population unit \r\n\r\n\r\n\r\nReading mAssoc()\r\noutput:\r\nThe output of interest from mAssoc() will be printed to\r\nthe console. You can find it by clicking on the area with a red square\r\naround it in this screenshot:\r\n\r\nIf we set units = 100, our attributable risk in the exposed\r\n(risk difference) would be interpretable as a difference of x\r\nnumber of outcomes per 100 people in the index level when compared to\r\nthe reference level. Since our population unit is set to 1, this number\r\nis simply interpreted as an individual’s risk difference of experiencing\r\nthe outcome if they are in the index level of the exposure.\r\nHere is a quick guide to the components of the output relevant to\r\nthis activity:\r\nInc risk ratio: Our basic risk ratioOdds ratio: Our basic odds ratioAttrib risk in the exposed: The risk\r\ndifference. mAssoc() takes another argument,\r\nunits =, which by default is set to 1. This regulates our\r\noutput for the measure of risk difference.\r\nmAssoc()\r\nwhen exposure variable has 3+ levels\r\nNow that you’ve seen the code for bord5 as an example,\r\nif you’re paying close attention, you may have started to wonder, “But\r\nwhat about variables like education, when there’s more than\r\none index level?”\r\nGood news: flipTable() and mAssoc() are\r\ndesigned to handle exposure variables with any number of levels. This\r\nmeans that the steps you take for bord5, male,\r\nand rural will also apply to magec and\r\neducation; simple as that.\r\nRunning mAssoc() when your exposure variable has 2+\r\nlevels will render multiple outputs for each respective index variable.\r\nThe person who wrote this function was even nice enough to label each of\r\nthose levels and whether or not they are the index or the reference\r\nlevel.\r\nTask 6: Side-by-side boxplots\r\nINSTRUCTIONS: Generate boxplots of maternal BMI\r\n(mbmi) and maternal age (mage), stratified by\r\nlevels of child mortality (death).\r\nClick\r\nhere for more help with boxplots using {ggplot2}\r\nFor this activity, we need boxplots that plot continuous variables\r\nbased on child mortality. In ggplot2, we can stratify boxplots by using\r\na second categorical variable when we specify our aesthetics\r\n(aes()). In other words, if you set mbmi to\r\nthe y-axis, you can use a categorical variable, like death\r\nas the variable on the x-axis.\r\nAdditionally, don’t forget to label your axes, give the plot a title,\r\nand save it using ggsave()\r\n\r\nWithin your call to geom_boxplot(), you can also change the\r\ncolor with fill = \"colorname\". Find a list of color names\r\nhere\r\nHere’s an example of what we are looking for, using the\r\nstarwars dataset. We’ve adjusted the dimensions a bit to\r\naccommodate one lone outlier. Care to guess who it is?\r\n\r\n\r\n\r\n\r\n\r\n\r\nFigure 1: A\r\nvery heavy slug indeed\r\n\r\n\r\n\r\nTask 7: Short answer\r\nFor Table 1, what is the null hypothesis being\r\ntested for each the two variables. Do the two statistical tests agree?\r\nIs there a statistically significant difference in the distributions of\r\neither BMI or age by child mortality? Are there substantively/clinically\r\nsignificant differences in these distributions? If statistical and\r\nsubstantive differences disagree, why might that be the case?\r\nTask 8: Short answer\r\nFor Table 2, what is the null hypothesis being\r\ntested for the variables. Do the two statistical tests agree? Is there a\r\nstatistically significant difference in any of these bivariate\r\ndistributions? Are there substantively/clinically significant\r\ndifferences in these distributions?\r\nTask 9: Short answer\r\nAge has been evaluated both as a continuous and categorical variable.\r\nDo the tests of association for these 2 parameterizations of age agree?\r\nIf not, why not?\r\nTask 10: Short answer\r\nFor Table 3, which risk factor has the strongest\r\nassociation with mortality? Provide a rationale for this assessment.\r\nWhich factors are protective and which are harmful?\r\nTask 11: Short answer\r\nPrepare a brief paragraph describing and interpreting risk, RD and RR\r\nestimates (from Table 3) for the 60 month risk of death\r\nin association with birth order and maternal age.\r\nBegin the paragraph by reporting the overall risk or proportion of\r\ndeaths in the study population. Don’t just repeat numbers that are\r\nalready shown in the table; note the values of effect estimates you feel\r\nshould be highlighted, but otherwise describe the results in words.\r\nWrite this paragraph as if you were including it in the results section\r\nof a manuscript. (175 words maximum).\r\n\r\n\r\n\r\n",
      "last_modified": "2022-05-31T13:52:42-04:00"
    },
    {
      "path": "705_lab_4.html",
      "title": "Lab 4 (705)",
      "description": "__Due:__ `r read.csv('files/due_dates.csv')[5,2]` by `r read.csv('files/due_dates.csv')[5,3]`\n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\nLAB MATERIALS\r\nLab 4 Goals\r\nLab 4 Grading scheme\r\nTask 1: Load libraries &\r\ndata\r\nTask 2: Table 1\r\nTabular\r\nanalysis of stratifying variables\r\nTask 2.1:\r\nStratified frequency counts and risk\r\nTask 2.2: Create\r\nepi.2by2() object\r\nTask 2.3:\r\nExtract estimates of risk difference\r\nTask 2.4:\r\nExtract estimates of risk ratios\r\n\r\nTask 3: Short Answer\r\nTask 4: Short Answer\r\n\r\n\r\n\r\n\r\nLAB MATERIALS\r\nR Markdown\r\nfile for Lab 4 Click link to download. Fill it in with your\r\nanswers to the following lab tasks. When you’re ready to submit, name it\r\nas Lab4_FirstinitialYourlastname.Rmd, and submit it using\r\nthe Sakai dropbox.\r\nExcel document\r\nwith Table for Task 2\r\nLab_4_kenya.rds - data file available on Sakai\r\nLab 4 Goals\r\nCalculate measures of association between birth order (i.e. the\r\nprimary exposure of interest) and child mortality by 5 years (i.e. the\r\nprimary outcome of interest) within strata of other variables\r\nAssess for potential confounding, mediation or effect measure\r\nmodification\r\nLab 4 Grading scheme\r\nCompetency\r\nPoints\r\n.Rmd file runs without error\r\n10\r\nTable 1 - Frequency counts and Risk\r\n20\r\nTable 1 - Risk difference\r\n20\r\nTable 1 - Risk ratio\r\n20\r\nTask 3 (short answer)\r\n15\r\nTask 4 (short answer)\r\n15\r\nTotal\r\n100\r\nTask 1: Load libraries &\r\ndata\r\nFor this assignment, use the dataset\r\n‘Lab_4_kenya.rds’.\r\nThis lab will also require the packages {tidyverse}, {fmsb}, {epiR},\r\nand {epiAssist}\r\nYou may need to install {fmsb}. You can do so with the following\r\ncode:\r\n\r\n\r\ninstall.packages(\"fmsb\")\r\n\r\n\r\n\r\nHopefully at this point in the semester, you have a pretty good grasp\r\non how to load libraries and data. But if you’re still unsure about\r\nsomething:\r\nHelp\r\nloading libraries. Help\r\nloading data.\r\nTask 2: Table 1\r\nINSTRUCTIONS: Generate frequency counts of 5-year\r\nmortality (death) according to child’s birth order\r\n(bord5), stratified by the following variables:\r\nmale\r\nrural\r\nmagec\r\neducation\r\nUse Table 1 to record frequency counts and strata-specific risk of\r\ndeath before 60 months, by birth order. Then, use\r\nepi.2by2() to generate stratified measures of risk\r\ndifference, risk ratios, and their accompanying confidence\r\nintervals.\r\nYou will also use the resulting epi.2by2() object to\r\nextract test statistics and p-values that test for potential effect\r\nmeasure modification by the stratifying variables.\r\nMoreover, you will use a pooled (i.e. adjusted) estimate of risk\r\ndifference and risk ratio to assess for confounding by comparing those\r\nestimates to the crude (i.e. unadjusted) estimates of risk difference\r\nand risk ratio.”\r\nBelow is a detailed guide on how to complete each step of these\r\ninstructions.\r\nTabular analysis of\r\nstratifying variables\r\nTable 1 is an example of a stratified analysis of exposure and\r\noutcome variables. Essentially, we want to know if the four variables\r\n(male, rural, magec, and\r\neducation) are confounders and/or effect measure modifiers\r\nof the relation between birth order and 5-year infant mortality. In\r\nother words, Questions of effect measure modification include whether\r\nthe degree of association of child’s birth order and mortality status is\r\ndifferent for boys and girls? What about for rural and urban families?\r\nMothers of different age categories? And so on…\r\n\r\nRemember the BIG\r\nPICTURE that was explained in the previous lab\r\nWhen you open the Excel file that contains Table 1,\r\nhere is what you will see:\r\nTable 1\r\nIf you haven’t already, download the Excel file at\r\nthe top of this page to access Table 1\r\nWe will break Table 1 into sections, and walk you\r\nthrough how to obtain these values in R.\r\nTask 2.1:\r\nStratified frequency counts and risk\r\nINSTRUCTIONS: Fill in the table with the numbers of\r\ndeaths (death, the outcome) and the 60-month risk of death\r\naccording to birth order (bord5, the exposure), stratified\r\nby:\r\nChild’s gender (variable male)\r\nRural/urban residence (variable rural)\r\nMaternal age (variable magec)\r\nMaternal education (variable education)\r\nTable 1 - Frequency counts and\r\nriskFor this task, we recommend that you create a table object with\r\ntable(), similar to the\r\none you created in lab 3, where your outcome of interest is\r\nmortality (death) and your exposure of interest is child\r\nbirth order (bord5). Except this time, include a third\r\nstratifying variable.\r\n\r\nIf you’ve been enjoying the {tidyverse}, feel free to use\r\ngroup_by() and count(). But the table object\r\nwill be useful in later steps.\r\ntable() allows for an unlimited number of stratifying\r\nlevels. You keep listing variables, and table() will keep\r\nsplitting the data into subsets.\r\nSay that we created a binary variable in our kenya\r\ndataset that indicated whether or not a mother’s BMI was above or below\r\n25 kg/m2. Let’s call that variable mbmi_25. Here\r\nis an example of a set of contingency tables of bord5 and\r\ndeath, stratified by mbmi_25:\r\n\r\n\r\nmbmi_tab <- table(kenya$bord5, kenya$death, kenya$mbmi_25)\r\n\r\nmbmi_tab\r\n\r\n#> , ,  = <25\r\n#> \r\n#>                     \r\n#>                      Alive Dead\r\n#>   1-4 in Birth Order  7931 1071\r\n#>   5+ in Birth Order   2180  409\r\n#> \r\n#> , ,  = >=25\r\n#> \r\n#>                     \r\n#>                      Alive Dead\r\n#>   1-4 in Birth Order  3700  374\r\n#>   5+ in Birth Order    773  146\r\n\r\n\r\n\r\nHere, our stratifying variable is mbmi_25, which\r\nseparates our tabulated data into two tables according to mothers who\r\nhave a BMI greater than or equal to 25, and those with BMI less than\r\n25.\r\nWe can generate row-wise proportions of our table by wrapping the\r\ntable object in the proportions() function.\r\n\r\nrow-wise proportions, a.k.a. RISK of DEATH/NOT DEATH!\r\nAs a reminder, rates (time in the denominator) are preferable measures\r\nof incidence of death when there are missing outcomes.\r\n\r\n\r\nproportions(mbmi_tab, margin = c(1, 3))\r\n\r\n\r\n#> , ,  = <25\r\n#> \r\n#>                     \r\n#>                           Alive       Dead\r\n#>   1-4 in Birth Order 0.88102644 0.11897356\r\n#>   5+ in Birth Order  0.84202395 0.15797605\r\n#> \r\n#> , ,  = >=25\r\n#> \r\n#>                     \r\n#>                           Alive       Dead\r\n#>   1-4 in Birth Order 0.90819833 0.09180167\r\n#>   5+ in Birth Order  0.84113166 0.15886834\r\n\r\n\r\n\r\n\r\nThe argument margin = designates along which we’d like to\r\nsplit our table when deriving proportions. By putting\r\nmargin = c(1,3), we restrict our calculation of proportions\r\nto row (1) and strata (3). Notice how each row\r\nsums to 1. Try playing with this configuration to see how it works.\r\nTask 2.2: Create\r\nepi.2by2() object\r\nINSTRUCTIONS: Create an epi.2by2()\r\nobject for each of the tables you generated above.\r\nepi.2by2() works just like mAssoc(). We\r\nneed to rearrange\r\nour tables so that our outcome of interest and exposure of interest\r\nare in the top-left corner of each table.\r\nGood news: The function flipTable() is designed to work\r\nwith stratified tables too!\r\nDo the following:\r\na. flipTable() so that the cell that\r\ncontains the index level of the exposure (birth order) and the outcome\r\nof interest (mortality status) is in the top-left corner of either\r\ntable.\r\nFor instance:\r\n\r\n\r\nmbmi_flip <- flipTable(mbmi_tab)\r\n\r\n\r\n\r\nb. Create an epi.2by2() object by using\r\nthe flipTable() object, with confidence level of 0.95 and\r\nunits set to 1.\r\n\r\n\r\nmbmi_epi <- epi.2by2(mbmi_flip, units = 1, conf.level = 0.95)\r\n\r\n\r\n\r\n\r\nWe set units = 1 to obtain risk difference as a proportion.\r\nThe default unit of 100 would give us measures of risk “per\r\n100 persons”.\r\nTask 2.3: Extract\r\nestimates of risk difference\r\nINSTRUCTIONS: Use the epi.2by2() object\r\nto extract the measure of risk difference for mortality in association\r\nwith bord5 within each covariable stratum, along\r\nwith test statistics and an overall (“adjusted”) pooled estimate of risk\r\ndifference.\r\nTable 1 - Risk differencesIMPORTANT: The output for epi.2by() is\r\nNOT the full extent of the information contained within an\r\nepi.2by2() object! There is more to every object than what\r\nmeets the eye.\r\n\r\n\r\n\r\nFigure 1: More than eye\r\n\r\n\r\n\r\nWhen you call an epi.2by2() object, you will notice that\r\nits printed output is a lot like that of mAssoc() from the\r\nprevious lab. Except this time, there are “crude” and “M-H”\r\nestimates:\r\nepi.2by2() output\r\nThis output is what’s called the object’s “print\r\nmethod”\r\nBut if you inspect the epi.2by2() object (either in your\r\nR Environment, or by typing the object’s name, followed by\r\n$), you will find vectors, lists, data frames, and all\r\nsorts of sub-objects.\r\n\r\nSee this\r\nlink if you are still uncertain about the functionality of\r\n$\r\nFollowing is how to locate the necessary information for this portion\r\nof Table 1\r\na. Risk difference and 95% CI\r\nTo access the risk differences of individual strata, use the\r\nfollowing path to refer to that exact component of your\r\nepi.2by2 object:\r\n\r\n\r\nobject$massoc.detail$ARisk.strata.wald\r\n\r\n\r\n\r\n\r\nConfused by the nomenclature? massoc is Measures of\r\nAssociation. ARisk is Attributable Risk.\r\nstrata refers to strata-specific measures.\r\nwald….. ask Larry or Liz.\r\nSo to obtain our measures of risk difference within separate stratum\r\nof mothers with BMI < 25 and mothers with BMI >= 25, we can use\r\nthe following code:\r\n\r\n\r\nmbmi_epi$massoc.detail$ARisk.strata.wald\r\n\r\n#>          est      lower      upper\r\n#> 1 0.03900249 0.02344295 0.05456203\r\n#> 2 0.06706667 0.04182403 0.09230931\r\n\r\n\r\n\r\nYou might notice that this returns a single estimate for each\r\nstratum. Our table, however, asks for two estimates for each. What\r\ngives??\r\nSince we have pre-determined our “reference” and “index” levels, and\r\nknow that a risk difference is defined\r\nas “risk in the level of interest minus risk in the reference level”,\r\nwhat’s the result when our reference level is also our level of\r\ninterest?\r\nCheck your work!\r\nTo check which value belongs to which strata, you can calculate it by\r\nhand using the values for risk that you obtained in Task 2.1.\r\nYou can also use the function riskdifference() from\r\npackage {fmsb} to quickly derive risk ratios and confidence\r\nintervals.\r\nTo learn how to use this function, you can view its documentation by\r\nrunning the following code in your console:\r\n\r\n\r\n?riskdifference\r\n\r\n\r\n\r\n\r\nThis function is pretty straightforward and I’m confident you can hack\r\nit 🦊\r\nb. Test of homogeneity and\r\np-value\r\nInconveniently, epi.2by2() doesn’t actually provide a\r\nchi-squared test of homogeneity of stratified risk differences. We can\r\nobtain this estimate by using the function epiHomog(),\r\nwhich takes a flipTable() object as its primary\r\nargument.\r\nIf your flipTable() objects are properly oriented, they\r\nshould suffice. Use your flipTable() objects as an argument\r\nin the function epiHomog() to return the test statistic and\r\np-value for a test of homogeneity at a significance level of 0.05:\r\n\r\n\r\nepiHomog(aFlippedTable)\r\n\r\n\r\n\r\nUsing our flipTable() object that is stratified by\r\nmother’s BMI, we would run the following code to obtain the resulting\r\noutput:\r\n\r\n\r\nepiHomog(mbmi_flip)\r\n\r\n#>   A tibble: 1 x 2\r\n#>    `Test statistic` `p-value`\r\n#>              <dbl>     <dbl>\r\n#>   1           3.44    0.0637\r\n\r\n\r\n\r\nInterpreting X2 test of\r\nhomogeneity\r\nWhat exactly is the chi-square test of homogeneity for? Well, it’s\r\nall about effect measure modification. Suppose we focus on the risk\r\ndifference measure. We use the risk difference measure to quantify the\r\nassociation of birth order with the risk of death by 60 months of age.\r\nIn particular, we use it to quantify the risk of death for those with\r\nbirth order 5+ in comparison to the risk of death for those with birth\r\norder 1-4 (i.e. the reference level) by subtracting the two risks.\r\nWhen we consider a third variable such as male, the chi-square test\r\nof homogeneity is used to evaluate whether there is evidence that the\r\nrisk difference is different for male and for female children. If so, it\r\nis important that we report the risk difference separately for male and\r\nfor female children. In an interesting feature, if there is effect\r\nmeasure modification on one effect measure scale (e.g. risk difference),\r\nthen it may very well not be present on another scale (e.g. risk ratio).\r\nThis is precisely why we talk about “effect\r\nmeasure modification” and not simply “effect\r\nmodification”.\r\nc. Pooled estimate\r\n(Mantel-Haenszel)\r\nFinally, we want the pooled estimate of risk difference. You might\r\nhave already found this in the original print method’s output of our\r\nepi.2by2() object, where it’s referred to as\r\nAttrib risk in the exposed (M-H).\r\nBut since we need to report this estimate to three decimal points\r\n(see gutter), we need to reach into our epi.2by2 object and\r\nfind the exact Mantel-Haenszel pooled estimate. You can access it by\r\ncalling the following path:\r\n\r\nAs per lab\r\nsubmission guidelines on rounding\r\n\r\n\r\nobject$massoc.detail$ARisk.mh.wald\r\n\r\n\r\n\r\n\r\nHere, mh stands for “Mantel-Haenszel”\r\nUsing our epi.2by2() object from before,\r\nmbmi_epi, we find this value and its resulting output as\r\nfollows:\r\n\r\n\r\nmbmi_epi$massoc.detail$ARisk.mh.wald\r\n\r\n#>          est      lower      upper\r\n#> 1 0.04662555 0.02684395 0.06640716\r\n\r\n\r\n\r\nWe would interpret this pooled result by saying that, when accounting\r\nfor confounding in the exposure due to a third variable\r\n(mbmi_25), risk of the outcome in the exposed was 0.05 (95%\r\nCI 0.03 to 0.07) higher than the unexposed.\r\nTask 2.4: Extract\r\nestimates of risk ratios\r\nThe process for obtaining risk ratios, test statistics for\r\nhomogeneity between strata, and pooled estimates is very similar to that\r\nof obtaining risk differences. Except this time, we don’t need a\r\nseparate function for finding the test of homogeneity –\r\nepi.2by2() generates the test statistic for homogeneity of\r\nrisk ratios itself.\r\nTable 1 - Risk RatiosRisk ratios\r\nTo extract risk ratios from an epi.2by2() object, you\r\ncan use the following path:\r\n\r\n\r\nobject$massoc.detail$RR.strata.wald\r\n\r\n\r\n\r\nAgain, when you’re finding these risk ratios, consider what we’re\r\nasking for in the table when we request the risk ratio of the reference\r\nlevel as it pertains to the reference level. What does it amount to when\r\nwe take the ratio of something to itself?\r\n\r\n\r\n\r\nFigure 2: Tautology\r\nclub\r\n\r\n\r\n\r\nOkay moving on…so our mbmi_epi object renders the\r\nfollowing output:\r\n\r\n\r\nmbmi_epi$massoc.detail$RR.strata.wald\r\n\r\n#>        est    lower    upper\r\n#> 1 1.046320 1.027315 1.065676\r\n#> 2 1.079734 1.048089 1.112334\r\n\r\n\r\n\r\nCheck your work!\r\nTo check which value belongs to which strata, you can calculate it by\r\nhand using the values for risk that you obtained in Task 2.1.\r\nYou can also use the function riskratio() from package\r\n{fmsb} to quickly derive risk ratios and confidence intervals.\r\nTo learn how to use this function, you can view its documentation by\r\nrunning the following code in your console:\r\n\r\n\r\n?riskratio\r\n\r\n\r\n\r\nThis one works the same as riskdifference(). Look it up\r\nand give it a try!\r\nTest of homogeneity and p\r\nvalue\r\nThe test of homogeneity for risk ratios is available in the following\r\nlocation within our epi.2by2() object\r\n\r\n\r\nobject$massoc.detail$wRR.homog\r\n\r\n\r\n\r\nPerforming this on our mbmi_epi object yields the\r\nfollowing result:\r\n\r\n\r\nmbmi_epi$massoc.detail$wRR.homog\r\n\r\n#>   test.statistic df   p.value\r\n#> 1       3.109451  1 0.0778392\r\n\r\n\r\n\r\nReflecting on this test statistic and the resulting p-value, given a\r\nsignificance level of 0.05, would we conclude that mother’s BMI is a\r\nconfounder in assessing the influence of birth order on the 5 year risk\r\nof mortality of children in our study cohort?\r\nHover for answer:  YOU THINK YOU GET\r\nFREEBIES?? \r\nPooled estimate\r\n(Mantel-Haenszel)\r\nThe pooled estimate appears in the printed output when you call your\r\nepi.2by2() object from the environment, and is called\r\nInc risk ratio (M-H). However, this is only reported to two\r\ndecimals. To access the actual value instead of the rounded one, you can\r\nlook for it in the following place within your epi.2by2()\r\nobject:\r\n\r\n\r\nobject$massoc.detail$RR.mh.wald\r\n\r\n\r\n\r\nUsing our mbmi_epi object renders the following\r\noutput:\r\n\r\n\r\nmbmi_epi$massoc.detail$RR.mh.wald\r\n\r\n#>        est    lower    upper\r\n#> 1 1.055389 1.039048 1.071988\r\n\r\n\r\n\r\nWe would interpret this as meaning that when we account for\r\nconfounding due to mother’s BMI, the risk of experiencing the outcome\r\namong the exposed was 1.06 (95% CI 1.04 to 1.07) times greater than the\r\noutcome of experiencing risk among the unexposed.\r\nTask 3: Short Answer\r\nPROMPT: Do you see evidence for confounding for any\r\nof the stratification variables (male, rural,\r\nmagec, and education)? Explain your response\r\nfor each variable.\r\n(Hint, you will need to refer back to the crude association estimates\r\nof risk difference and risk ratio that you generated for Table 3 of Lab\r\n3).\r\nTask 4: Short Answer\r\nPROMPT: For which covariables does it seem\r\nappropriate to report a single pooled adjusted estimate and for which\r\ndoes it seem necessary to report separate stratified estimates (i.e., do\r\nyou see evidence of effect modification)? Explain your rationale for\r\neach covariable.\r\nDoes your answer differ depending on whether you are considering the\r\nrisk difference or risk ratio as your measure of effect?\r\n\r\n\r\n\r\n",
      "last_modified": "2022-05-31T13:52:49-04:00"
    },
    {
      "path": "707_lab_01.html",
      "title": "Lab 1 (707)",
      "description": "__Due:__ `r read.csv('files/due_dates.csv')[6,2]` by `r read.csv('files/due_dates.csv')[6,3]`\n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\nLAB MATERIALS\r\nLab 1 Goals\r\nData and assignment\r\nLab 1 Grading scheme\r\nPackages\r\n\r\nCompetencies:\r\nImport data from\r\n.csv\r\nComma separated values\r\n(.csv)\r\nImport from .csv\r\n(read.csv())\r\n\r\nPlots with {ggplot2}\r\nHistograms\r\n(geom_histogram())\r\nScatter plots\r\n(geom_point())\r\nQ-Q Plots\r\n(qqnorm())\r\nNote on Q-Q Plots\r\n\r\nCreating new variables\r\nlog() transform\r\nSquaring with ^2\r\nCategorical\r\nvariables with case_when()\r\n\r\nSkewness and\r\nKurtosis ({moments} package)\r\nPearson and\r\nSpearman rank correlation coefficients\r\nBonus:\r\nCorrelation coefficient matrices\r\n\r\nLinear Regression Models\r\nUnivariate\r\n(single-variable) models with lm()\r\nMultivariable models with\r\nlm()\r\nLinear models: a brief\r\ntour\r\n\r\nPlotting line of best fit\r\nModel first, then plot\r\nPlot first, then model\r\n\r\nModel Evaluation\r\nResiduals plots\r\nHeteroskedasticity\r\n\r\nModel comparison\r\nF-test\r\n\r\n\r\n\r\n\r\n\r\n\r\nLAB MATERIALS\r\nLab 1 Goals\r\nEstimate crude measures of (linear) association using univariable\r\nlinear regression models\r\nEstimate crude measures of non-linear association using univariable\r\nlinear regression models with linear transformed and categorical\r\nvariables.\r\nEstimate adjusted measures of association using multivariable linear\r\nregression models\r\nData and assignment\r\nThe assignment and dataset are both available on Sakai.\r\nLab 1 Grading scheme\r\nCompetency\r\nPoints\r\nTable 1a - Pearson CC\r\n3\r\nTable 1b - Spearman rank CC\r\n3\r\nTable 2 - Univariate models\r\n10\r\nTable 3 - F Statistic\r\n3\r\nTask 4 - Multivariable models\r\n10\r\nFigures\r\n15 (1 point each)\r\nShort answers 1 - 7\r\n56 (8 points each)\r\nTotal\r\n100\r\nPackages\r\n{moments}\r\n{car}\r\n{tidyverse}\r\nCompetencies:\r\nImport data from .csv\r\nComma separated values\r\n(.csv)\r\nCSV stands for “comma separated values”. It’s a way of organizing\r\ndata. When a file extension reads .csv, spreadsheet\r\napplications like Excel understand that to mean that the data are stored\r\naccording to a simple set of rules.\r\nSee if you can spot the pattern.\r\nThe raw data might look like this:\r\n\r\nName, Date, App, date location, date ranking, second date\r\nJohn, 01-19-2019, Bumble, sip and paint, 7, 0\r\nNiko, 02-14-2019, Hinge, sky diving, 10, 0\r\nArwin, 02-28-2019, Bumble, ice skating, 8, 0\r\nChad, 03-12-2019, Tinder, movie night, 4, 0\r\nOleksiy, 03-15-2019, Bumble, axe throwing, 8, 0\r\nManuel, 03-19-2019, Bumble, bike ride, 9, 0\r\nVince, 03-27-2019, Ship, barcade, 7, 0\r\nAdamu, 04-01-2019, Hinge, botanical garden, 8.5, 0 \r\nRyan, 04-21-2019, Christian mingle, mass, 7.5, 0\r\n\r\nAnd a spreadsheet application (e.g. Microsoft Excel) would display\r\nthe data like this: \r\nWhich in R Studio, would look like this:\r\n\r\nName\r\nDate\r\nApp\r\ndate location\r\ndate ranking\r\nsecond date\r\nJohn\r\n01-19-2019\r\nBumble\r\nsip and paint\r\n6.9\r\n0\r\nNiko\r\n02-14-2019\r\nHinge\r\nsky diving\r\n9.9\r\n0\r\nArwin\r\n02-28-2019\r\nBumble\r\nice skating\r\n8.2\r\n0\r\nChad\r\n03-12-2019\r\nTinder\r\nmovie night\r\n4.4\r\n0\r\nOleksiy\r\n03-15-2019\r\nBumble\r\naxe throwing\r\n8.3\r\n0\r\nManuel\r\n03-19-2019\r\nBumble\r\nbike ride\r\n8.9\r\n0\r\nVince\r\n03-27-2019\r\nShip\r\nbarcade\r\n7.3\r\n0\r\nAdamu\r\n04-01-2019\r\nHinge\r\nbotanical garden\r\n8.5\r\n0\r\nRyan\r\n04-21-2019\r\nChristian mingle\r\nmass\r\n7.5\r\n0\r\n\r\n\r\nExpand for the four simple rules of .csv data\r\ncommas delineate columns\r\na new line delineates a new row\r\nthe first row of data determines column names and number of\r\ncolumns\r\nthe values between commas are data\r\nImport from .csv\r\n(read.csv())\r\nread.csv() works a lot like readRDS(),\r\nexcept it can interact with .csv files.\r\nBefore you try to import your data, do yourself a favor and open the\r\n.csv file in Excel to inspect it.\r\nThe first 10 rows of\r\nregdat.csvNow that you’ve done that, note that read.csv() allows\r\nthe following arguments relevant to importing this lab’s data:\r\nfile = \"\" - just like readRDS, you need to\r\nprovide a file name in quotes\r\nheader = - this argument takes either TRUE\r\nor FALSE. If set to TRUE, the function will\r\nuse the first row of the data as the column names (“headers”)\r\ncol.names = c() - takes a vector that is the same\r\nlength as the number of columns in the dataset. It will use the values\r\nin this vector as the column names.\r\nHere is an example:\r\n\r\n\r\ndata <- read.csv(\"regdat.csv\", header = FALSE, \r\n                 col.names = c(\"columnA\", \"columnB\", \"columnC\", \"columnD\"))\r\n\r\n\r\n\r\nDon’t forget to use <- to assign a name\r\nto your imported data frame!\r\nPlots with {ggplot2}\r\nHistograms\r\n(geom_histogram())\r\nIf you need a refresher on creating histograms with {ggplot2}, you\r\ncan find it here.\r\nScatter plots\r\n(geom_point())\r\nWe create scatter plots by assigning an x and\r\ny aesthetic (aes()) to a ggplot object, and\r\nthen adding a layer called geom_point()\r\n\r\n\r\n\r\nQ-Q Plots (qqnorm())\r\nA Q-Q plot allows us to compare a variable’s observed distribution\r\nagainst its “theoretical” distribution. It does this by taking a\r\ncontinuous variable, converting each value to a Z-score (using the\r\nvariable’s mean and standard deviation), and then plotting the Z-score\r\nagainst the original observed value.\r\nTo do this in R, we might start by creating a qqnorm\r\nobject using qqnorm():\r\nAs an example, here are first 10 values of\r\nregdat$weight. “Observed Values” are the actual data, and\r\n“Z-values” are the Z-score for each of those values of\r\nweight:\r\n\r\nObserved Values\r\nZ Values\r\n85.0\r\n-0.6979123\r\n105.0\r\n0.1272606\r\n108.0\r\n0.2675710\r\n92.0\r\n-0.4018917\r\n112.5\r\n0.7389858\r\n112.0\r\n0.4482006\r\n104.0\r\n0.0847131\r\n69.0\r\n-1.9916133\r\n94.5\r\n-0.2348232\r\n68.5\r\n-2.0751279\r\n\r\nWe can use qqnorm() to calculate and plot these values\r\nautomatically!\r\nHere is the Q-Q plot for regdat$weight, with histogram\r\nfor reference. Notice that the y-axis is labelled “Sample Quantiles” and\r\nits range is the minimum and maximum weight in regdat, while the x-axis\r\nis labelled “Theoretical Quantiles” and ranges from -3 to 3. Why\r\nmight that be?\r\n\r\n\r\nqqnorm(regdat$weight, main = \"Normal Q-Q Plot of Weight (lbs.)\")\r\nhist(regdat$weight)\r\n\r\n\r\n\r\n\r\n\r\nIf weight were normally distributed, we would expect a\r\nstraight diagonal line. See the “Note on Q-Q plots” below for an example\r\nof this.\r\nNote on Q-Q Plots\r\nFor a quick comparison, we can use a simple model to visualize how a\r\ntruly normal distribution would appear on a Q-Q plot:\r\nThis\r\npage from U Virginia is an excellent resource.\r\nIf we extract the mean and standard deviation from our height, we can\r\nconstruct a normal distribution and plot it.\r\n\r\n\r\n# calculate mean of height\r\nx_hat <- mean(regdat$height)\r\n# calculate sd of height\r\nx_sd <- sd(regdat$height)\r\n# simulate a normal distribution using height's mean and sd\r\nnormal <- rnorm(500, mean = x_hat, sd = x_sd)\r\n# scatter plot\r\nplot(normal)\r\n# histogram\r\nhist(normal)\r\n# Q-Q plot\r\nqqnorm(normal)\r\n\r\n\r\n\r\n\r\nBonus: qqnorm(), but\r\nwith ggplot()\r\nFor those interested:\r\nRemember the discussion about how most objects contain more\r\ninformation than initially meets the eye?\r\nThe same goes for qqnorm objects. They aren’t just a\r\nbunch of dots. They contain vectors of the theoretical and observed\r\n(“Sampled”) quantiles. If we convert the qqnorm object to a\r\ndata.frame, we can use ggplot() to call those\r\nspecific columns for plotting. Let’s look at how we would make a Q-Q\r\nplot for the natural log of weight:\r\n\r\n\r\n# calculate ln_wt\r\nregdat$ln_wt <- log(regdat$weight)\r\n\r\n# create qqnorm() object and convert to data.frame() object\r\nqqnorm_data <- data.frame(qqnorm(regdat$ln_wt, plot.it = FALSE))\r\n\r\n# plot in ggplot\r\nggplot(data = qqnorm_data, aes(x = x, y = y)) +\r\n  geom_point(shape = 1, size = 3) + \r\n  labs(title = \"Q-Q plot for ln(weight)\") +\r\n  xlab(\"Theoretical Quantiles for ln_wt\") +\r\n  ylab(\"Sample Quantiles for ln_wt\")\r\n\r\n\r\n\r\n\r\nCreating new variables\r\nlog() transform\r\nIn R, log() takes the natural log of a given value:\r\n\r\n\r\nlog(10)\r\n\r\n\r\n[1] 2.302585\r\n\r\n#> [1] 2.302585\r\n\r\n\r\n\r\nIf given a vector of values, log() will take the natural\r\nlog of each of those values individually:\r\n\r\n\r\nsimple_vector <- c(1, 10, 100, 1000)\r\n\r\nlog(simple_vector)\r\n\r\n#> [1] 0.000000 2.302585 4.605170 6.907755\r\n\r\n\r\n\r\nNot relevant to Lab 01, but you can also specify what base you want\r\nyour log to be in with the argument base =:\r\n\r\n\r\nlog(simple_vector, base = 10)\r\n\r\n#> [1] 0 1 2 3\r\n\r\n\r\n\r\nWhy do a log transformation?\r\nImagine we have a variable that is highly right-skewed. The histogram\r\nof that variable might look something like this:\r\n\r\n\r\n\r\nWe can use a natural log transformation to make the variable’s\r\ndistribution more “normal”, and therefore suitable for linear\r\nregression:\r\n\r\n\r\n\r\n\r\nFor more on transforming data into a normal distribution, see this\r\nlink\r\nThe natural log transformation uses ‘Euler’s number’ (2.718) as its\r\n‘base’ (in contrast, a base 10 log transformation uses 10 as the base).\r\nSo:\r\n\\[\r\n\\begin{eqnarray}\r\nln(2) &=& 0.693\\\\\r\n\\rm{because} \\ 2.718^.693 &=& 2\r\n\\end{eqnarray}\r\n\\]\r\nUsing a log transformation can help to ‘pull in’ outlier values,\r\nwhich has the benefit of making the distribution more Normally\r\ndistributed and reduce skew, although it doesn’t affect kurtosis.\r\nSquaring with ^2\r\nIn R, you can raise a base by any exponent using ^. For\r\nexample, 103 would be written like this:\r\n\r\n\r\n10^3\r\n\r\n#> [1] 1000\r\n\r\n\r\n\r\nIf you give R a vector of numbers, followed by an exponent, it will\r\nperform the operation on each value individually:\r\n\r\n\r\nsimple_vector <- c(3, 3.3, 12, 1)\r\nsimple_vector^2\r\n\r\n#> [1]   9.00  10.89 144.00   1.00\r\n\r\n\r\n\r\nRecall that data frames are just lists of vectors!\r\nYou might consider using %>% and mutate()\r\nto create a squared variable.\r\nCategorical variables with\r\ncase_when()\r\nFor a refresher on how to generate categorical variables from\r\ncontinuous variables, you can refer back to when we created\r\nmagec in last 705, Lab 0.\r\nSkewness and Kurtosis\r\n({moments} package)\r\nIn evaluating the skewness and kurtosis values, a variable that is\r\nperfectly normally distributed will have a skewness of 0 and a kurtosis\r\nof 3.\r\nSo for example, if a variable has a skewness of -0.05 and a kurtosis\r\nof 2.2, we would conclude that the data are slightly left skewed\r\n(negative value) and there is some kurtosis.\r\nMore specifically, because kurstosis, 2.2 < 3, we conclude that\r\nthere are less data in the tails. If the value for kurtosis had been\r\ngreater than 3, we would have concluded that the variable has more data\r\nin the tails than expected under a normal distribution.\r\nHere\r\nis a link if you’d like to read more about it.\r\nIn R, we can calculate both skewness and kurtosis using a package\r\ncalled {moments} and its handy functions skewness() and\r\nkurtosis():\r\n\r\n\r\nskewness(regdat$weight)\r\n\r\nkurtosis(regdat$weight)\r\n\r\n#> [1] 0.5843876\r\n#> [1] 3.765937\r\n\r\n\r\n\r\n\r\nYou will need to install {moments} with\r\ninstall.packages(\"moments\")\r\nPearson and\r\nSpearman rank correlation coefficients\r\nWe use correlation coefficients to measure the strength and direction\r\nof the linear association between two continuous variables.\r\nCorrelation coefficients can range from -1 to 1. Values closer to 1\r\nrepresent a strong positive correlation. Those closer\r\nto -1 represent a strong negative correlation. The\r\ncloser the correlation coefficient, R, is to 0, the weaker the\r\nassociation between the two variables.\r\n\r\n\r\n\r\nTo calculate in R, use the function:\r\n\r\ncor(x, y, \r\n    method = c(\"pearson\", \"kendall\", \"spearman\"))` \r\n\r\nSpecify your x and y variables and set the method to your cheeky brit\r\nof choice.\r\n\r\nThe Pearson is the parametric test. Both\r\nSpearman and Kendall refer to\r\nrank-based (non-parametric) measures of association for when our data\r\nare non-normal.\r\nIf no method is specified, “pearson” is the default.\r\nBonus:\r\nCorrelation coefficient matrices\r\nIf you want to have a very particular kind of fun, try the\r\nfollowing:\r\nTake your regdat data and create a new data frame of\r\nonly continuous variables from regdat. Give it a new name,\r\nlike cont_data.\r\nUse R to run cor(cont_data), where the data frame of\r\ncontinuous variable vectors is the only argument.\r\n\r\nYou can use {dplyr}’s select() to remove\r\nvariables by name. But there are also many\r\nmany\r\nways to do this.\r\nWhat did it do? (Go look at Table 1 in the assignment.)\r\nLinear Regression Models\r\nBuilding models in R is fun!\r\nUnivariate\r\n(single-variable) models with lm()\r\nRemember the concept of R\r\nformulas?\r\nRegression models make use of formulas in a very intuitive way. Here\r\nis the general format:\r\n\r\n\r\nlm(response_var ~ predictor_var, data = a_data_frame)\r\n\r\n\r\n\r\nlm stands for “linear model”. If we assign a name to our\r\nmodel, we can access various components within the model, which we can\r\nindex with $.\r\nWe can also just use summary() to view an overview of\r\nthe model’s most important components.\r\n\r\n\r\nm1 <- lm(Sepal.Length ~ Petal.Length, data = iris)\r\nsummary(m1)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = Sepal.Length ~ Petal.Length, data = iris)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-1.24675 -0.29657 -0.01515  0.27676  1.00269 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)   4.30660    0.07839   54.94   <2e-16 ***\r\nPetal.Length  0.40892    0.01889   21.65   <2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.4071 on 148 degrees of freedom\r\nMultiple R-squared:   0.76, Adjusted R-squared:  0.7583 \r\nF-statistic: 468.6 on 1 and 148 DF,  p-value: < 2.2e-16\r\n\r\nMultivariable models with\r\nlm()\r\nYou can also use lm() to program models with multiple\r\nlinear predictor variables. You can formulate your equation as you build\r\nyour model, using arithmetic operators on the right-hand side of the\r\nformula:\r\n\r\n\r\nlm(response_var ~ predictor_var1 + predictor_var2, data = a_data_frame)\r\n\r\n\r\n\r\n\r\n\r\nmlm1 <- lm(Sepal.Length ~ Petal.Length + Species, data = iris)\r\n\r\nsummary(mlm1)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = Sepal.Length ~ Petal.Length + Species, data = iris)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-0.75310 -0.23142 -0.00081  0.23085  1.03100 \r\n\r\nCoefficients:\r\n                  Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)        3.68353    0.10610  34.719  < 2e-16 ***\r\nPetal.Length       0.90456    0.06479  13.962  < 2e-16 ***\r\nSpeciesversicolor -1.60097    0.19347  -8.275 7.37e-14 ***\r\nSpeciesvirginica  -2.11767    0.27346  -7.744 1.48e-12 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.338 on 146 degrees of freedom\r\nMultiple R-squared:  0.8367,    Adjusted R-squared:  0.8334 \r\nF-statistic: 249.4 on 3 and 146 DF,  p-value: < 2.2e-16\r\n\r\nLinear models: a brief tour\r\nIn this section, we’ll show you how to access a model’s coefficients,\r\nconfidence intervals, residuals, and other values that might come in\r\nhandy.\r\nRemember how we said that you can extract numbers from your model\r\nwhen you assign it a name? If we inspect m1 using\r\n$ to index the object by name, we’ll see some useful items\r\nat our disposal.\r\nYou can obtain the model’s coefficients with\r\ncoef(your_model). For example:\r\n\r\n\r\ncoef(m1)\r\n\r\n\r\n (Intercept) Petal.Length \r\n   4.3066034    0.4089223 \r\n\r\nAnd you can calculate 95% confidence intervals with\r\nconfint(your_model, level = 0.95). For example:\r\n\r\n\r\nconfint(m1,  level = .95)\r\n\r\n\r\n                 2.5 %    97.5 %\r\n(Intercept)  4.1516972 4.4615096\r\nPetal.Length 0.3715907 0.4462539\r\n\r\nAnd don’t forget that you can inspect your model with\r\nsummary():\r\n\r\n\r\nsummary(m1)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = Sepal.Length ~ Petal.Length, data = iris)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-1.24675 -0.29657 -0.01515  0.27676  1.00269 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)   4.30660    0.07839   54.94   <2e-16 ***\r\nPetal.Length  0.40892    0.01889   21.65   <2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.4071 on 148 degrees of freedom\r\nMultiple R-squared:   0.76, Adjusted R-squared:  0.7583 \r\nF-statistic: 468.6 on 1 and 148 DF,  p-value: < 2.2e-16\r\n\r\nOur model contains the columns of data (variables) used in its\r\ncalculation:\r\n\r\n\r\nhead(m1$model)\r\n\r\n\r\n  Sepal.Length Petal.Length\r\n1          5.1          1.4\r\n2          4.9          1.4\r\n3          4.7          1.3\r\n4          4.6          1.5\r\n5          5.0          1.4\r\n6          5.4          1.7\r\n\r\nIt also has the resulting fitted values:\r\n\r\n\r\nhead(m1$fitted.values)\r\n\r\n\r\n       1        2        3        4        5        6 \r\n4.879095 4.879095 4.838202 4.919987 4.879095 5.001771 \r\n\r\nThe fitted values are what result when we plug each row into the\r\nmodel equation. We can use the first row of our data to observe this\r\ndirectly:\r\n\r\n\r\nx <- m1$model[1,2] # Petal.Length of row 1\r\n\r\nB0 <- m1$coefficients[1] # beta-naught\r\n\r\nB1 <- m1$coefficients[2] # beta-one\r\n\r\n# formula:\r\nfitted <- B0 + B1 * x\r\nfitted\r\n\r\n\r\n(Intercept) \r\n   4.879095 \r\n\r\nThe resulting value is the “fitted” value, or the estimate of the\r\nresponse variable according to our model, given the value of the\r\npredictor, Petal.Length (which for the first row in our\r\ndata, is 1.4).\r\nNotice that our model also contains residuals, or the differences\r\nbetween our observed values for y (our response variable), and the\r\nvalues predicted by our model:\r\n\r\n\r\nhead(m1$residuals)\r\n\r\n\r\n         1          2          3          4          5          6 \r\n 0.2209054  0.0209054 -0.1382024 -0.3199868  0.1209054  0.3982287 \r\n\r\nWe can find the first row’s residual value with the\r\nfitted value that we calculated above:\r\n\r\n\r\n# observed Sepal.Length of row 1 minus the predicted/fitted value:\r\nm1$model[1,1] - fitted\r\n\r\n\r\n(Intercept) \r\n  0.2209054 \r\n\r\nPlotting line of best fit\r\n{ggplot2} provides a wide variety of flexible tools for visualizing\r\nmodels. There are two main ways we might visualize a regression model.\r\nThe best way to do this is to fit the model and then plot the prediction\r\n(often referred to as the line of best fit)\r\nYou can also use functions within {ggplot2} to generate a line of\r\nbest fit. This is a technique that we will encounter later in the\r\nsemester that is handy in special circumstances. But for now,\r\nplease use the method that follows.\r\nHere\r\nis a highly useful tutorial for how to plot different models in\r\n{ggplot2}. It includes splines, Loess, and multiple variations on\r\nquadratics.\r\nModel first, then plot\r\nWe can plot our data and line of best fit by using a model as the\r\ndata object required by {ggplot2}.\r\nRemember our example model from earlier:\r\n\r\n\r\nCall:\r\nlm(formula = Sepal.Length ~ Petal.Length, data = iris)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-1.24675 -0.29657 -0.01515  0.27676  1.00269 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)   4.30660    0.07839   54.94   <2e-16 ***\r\nPetal.Length  0.40892    0.01889   21.65   <2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.4071 on 148 degrees of freedom\r\nMultiple R-squared:   0.76, Adjusted R-squared:  0.7583 \r\nF-statistic: 468.6 on 1 and 148 DF,  p-value: < 2.2e-16\r\n\r\nm1 will serve as our data object within\r\nggplot(). We can start by creating a scatter plot of the\r\nresponse and predictor variables from our formula. But instead of using\r\nthe original data frame (regdat), we’ll use\r\ndata = m1:\r\n\r\n\r\np <- ggplot(data = m1, aes(x = Petal.Length, y = Sepal.Length)) +\r\n  geom_point()\r\n\r\np\r\n\r\n\r\n\r\n\r\n\r\nNotice that we have assigned a name to our plot, p. By\r\ndoing this, we can add layers to the original plot without needing to\r\nreproduce the code each time.\r\nFrom here, we can add a geom_smooth() layer to fit an\r\nordinary least squares regression line to our data. Notice that within\r\nthe model m1, we have access to a variable,\r\n.fitted. We need to assign this to the y-axis of our plot.\r\nThis will plot a line for Petal.Length against our\r\nfitted/predicted values!\r\n\r\n\r\np + geom_smooth(aes(y = .fitted))\r\n\r\n\r\n\r\n\r\nUnderstanding how this method works will become more important as our\r\nmodels increase in complexity. It also will helps in plotting residuals\r\n(see next\r\nsection).\r\nPlot first, then model\r\nTo perform this method, we need to start with a scatter plot using\r\n{ggplot2}:\r\n\r\n\r\np2 <- ggplot(data = toy, aes(x = ran1, y = pos)) +\r\n  geom_point() +\r\n  labs(x = \"Age (years)\", y = \"Points\", title = \"This is fake data\")\r\n\r\np2\r\n\r\n\r\n\r\n\r\nOnce we have our plot, we just need to add another layer,\r\nstat_smooth(), and designate a method and formula. Here,\r\nwe’re building a univariate, linear model. Observe how we parameterize\r\nthese details within the stat_smooth() function:\r\n\r\n\r\np2 + stat_smooth(method = \"lm\", formula = y ~ x)\r\n\r\n\r\n\r\n\r\nNotice that if we don’t designate a method,\r\nstat_smooth() will default to plotting a smoothed “loess”\r\nline, which we’ll learn more about later in the semester:\r\n\r\n\r\np2 + stat_smooth(formula = y ~ x)\r\n\r\n\r\n\r\n\r\nFor those curious, we can also print our model’s equation in the\r\nplot. The code is a bit dense, but straightforward:\r\n\r\n\r\nrequire(ggpmisc)\r\n\r\np2 + stat_smooth(method = \"lm\", formula = y ~ x) +\r\n  stat_poly_eq(formula = y ~ x, \r\n                aes(label = paste(..eq.label.., \r\n                                  ..rr.label.., \r\n                                  sep = \"~~~\")), \r\n                parse = TRUE)\r\n\r\n\r\n\r\n\r\n\r\nFrom this\r\npost on SO.\r\nModel Evaluation\r\nOnce we’ve built a model, there are various measures we might use to\r\nevaluate how well that model fits our data. Here we discuss two of those\r\nmeasures: residuals and\r\nheteroscedasticity.\r\nResiduals plots\r\nOur model object contains a vector of residuals, which are the\r\nobserved Y minus the predicted Y:\r\n\r\n\r\nhead(m1$residuals)\r\n\r\n\r\n         1          2          3          4          5          6 \r\n 0.2209054  0.0209054 -0.1382024 -0.3199868  0.1209054  0.3982287 \r\n\r\nWe could generate a histogram of our residuals. In the following\r\ncode, notice that {ggplot2} finds our residuals when we refer to them as\r\n.resid:\r\n\r\n\r\nggplot(data = m1, aes(x = .resid)) + \r\n  geom_histogram(binwidth = .1, fill = \"seagreen\") +\r\n  xlab('Residuals') +\r\n  ggtitle(\"Histogram of residuals of Iris Petal Length regressed on Sepal Length\")\r\n\r\n\r\n\r\n\r\nBut we can also use that vector of residuals, .resid to\r\ncreate a scatter plot of our fitted data against our residuals:\r\n\r\n\r\nggplot(data = m1, aes(x = .fitted, y = .resid)) +\r\n  geom_point() +\r\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\r\n  labs(x = \"Fitted values\", \r\n       y = \"Residuals\", \r\n       title = \"Residual plot of iris petal length regressed on sepal length\")\r\n\r\n\r\n\r\n\r\n\r\nNotice that we have added a dashed horizontal line at y = 0\r\nby adding a layer to our plot:\r\n+ geom_hline(yintercept = 0, linetype = \"dashed\")\r\nThis tells us if there is any bias in our model’s fit.\r\nOne such trend we might observe is heteroskedasticity,\r\nHeteroskedasticity\r\nSimply put, heteroskedasticity is when the variability of Y (our\r\nresponse variable) is different depending on X (our predictor\r\nvariable).\r\nIn the following plot, where along the x-axis would we be able to\r\naccurately predict Y using a linear model?\r\n\r\n\r\n\r\n\r\n\r\nggplot(het, aes(x = x, y = z)) +\r\n  geom_point() +\r\n  labs(x = \"X\", y = \"Y\", title = \"More fake data\")\r\n\r\n\r\n\r\n\r\nWikipedia\r\nis telling me that this is Greek for “different” (hetero)\r\n“dispersion” (skedasis)\r\nThe data above are\r\nheteroskedastilisticexpialidocious.\r\nLink, for the Disney lovers\r\nBreusch-Pagan/Cook-Weisberg\r\nWe can use a chi-squared test to assess how\r\nheteroskedastic our data really are. We can do this using\r\nncvTest() from the package {car}.\r\nHere’s a test of our iris model, whose residuals we visualized\r\nabove:\r\n\r\n\r\nncvTest(m1)\r\n\r\n\r\nNon-constant Variance Score Test \r\nVariance formula: ~ fitted.values \r\nChisquare = 2.493218, Df = 1, p = 0.11434\r\n\r\n\r\nFor proof of method, see “References”\r\nsection in documentation\r\nMeanwhile, here’s a Breusch-Pagan test of the heteroskedastilistic\r\ndata above. Notice the p-value:\r\n\r\n\r\nncvTest(lm(z~x, data = het))\r\n\r\n\r\nNon-constant Variance Score Test \r\nVariance formula: ~ fitted.values \r\nChisquare = 27.79159, Df = 1, p = 1.3511e-07\r\n\r\nModel comparison\r\nWe can use an F-statistic to compare models when we add or omit\r\nvariables. Section 9.2 of OpenIntro contains a great introduction to\r\nmodel selection. In it, they recommend two different model selection\r\nstrategies:\r\nBackwards elimination - where we generate a\r\nfully saturated model, then take one variable out at a time.\r\nForward selection - where we start with a single\r\npredictor variable, and add one new variable at a time.\r\nWith each step in either of these processes, we need some kind of\r\ntest to help us evaluate if the removal/addition of a variable has\r\nimproved the model.\r\nIn OpenIntro, they use R2. But we can also use an\r\nF-test.\r\nF-test\r\nUse anova(aModel, anotherModel) to compute the\r\nF-statistic for the comparison of two models.\r\nFurther\r\nup on this page, we created a model that aimed to predict\r\niris sepal length using measures of iris petal length.\r\nHere’s that model’s summary:\r\n\r\n\r\nsummary(m1)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = Sepal.Length ~ Petal.Length, data = iris)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-1.24675 -0.29657 -0.01515  0.27676  1.00269 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)   4.30660    0.07839   54.94   <2e-16 ***\r\nPetal.Length  0.40892    0.01889   21.65   <2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.4071 on 148 degrees of freedom\r\nMultiple R-squared:   0.76, Adjusted R-squared:  0.7583 \r\nF-statistic: 468.6 on 1 and 148 DF,  p-value: < 2.2e-16\r\n\r\nWe may wonder if using a quadratic formula renders a better\r\nmodel:\r\n\r\n\r\nm1_quadratic <- lm(Sepal.Length ~ Petal.Length + I(Petal.Length^2), data = iris)\r\nsummary(m1_quadratic)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = Sepal.Length ~ Petal.Length + I(Petal.Length^2), \r\n    data = iris)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-1.0684 -0.2348  0.0121  0.2049  0.9146 \r\n\r\nCoefficients:\r\n                  Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)        5.05833    0.14036  36.038  < 2e-16 ***\r\nPetal.Length      -0.16435    0.09427  -1.743   0.0834 .  \r\nI(Petal.Length^2)  0.08146    0.01318   6.181 5.96e-09 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.3639 on 147 degrees of freedom\r\nMultiple R-squared:  0.8095,    Adjusted R-squared:  0.8069 \r\nF-statistic: 312.3 on 2 and 147 DF,  p-value: < 2.2e-16\r\n\r\nCompare the two outputs and notice that our Adjusted R-squared has\r\nincreased. This is a good sign.\r\nBut an F-statistic quantifies the difference in model fit between two\r\nmodels:\r\n\r\n\r\nanova(m1, m1_quadratic, test = \"F\")\r\n\r\n\r\nAnalysis of Variance Table\r\n\r\nModel 1: Sepal.Length ~ Petal.Length\r\nModel 2: Sepal.Length ~ Petal.Length + I(Petal.Length^2)\r\n  Res.Df    RSS Df Sum of Sq      F    Pr(>F)    \r\n1    148 24.525                                  \r\n2    147 19.466  1    5.0593 38.206 5.955e-09 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\n\r\nIf you want to know more about how this test is performed, this\r\narticle by Danielle Navarro is a great place to look.\r\nThe main thing of importance when performing an F-test on two models\r\nis that the smaller of the two models (the “null” model) cannot contain\r\nvariables that are not in the larger model (the “alternative” or “full”\r\nmodel).\r\n\r\n\r\n\r\n",
      "last_modified": "2022-05-31T13:53:01-04:00"
    },
    {
      "path": "707_lab_02_03.html",
      "title": "Lab 2 & 3 (707)",
      "description": "__Lab 02 Due:__ `r read.csv('files/due_dates.csv')[7,2]` by `r read.csv('files/due_dates.csv')[7,3]`\n<br><\/br>\n(__Note:__ There are no written questions to turn in for Lab 2.)\n<br><\/br>\n__Lab 03 Due:__ `r read.csv('files/due_dates.csv')[8,2]` by `r read.csv('files/due_dates.csv')[8,3]`\n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\nLAB MATERIALS\r\nLab 02 Goals\r\nLab 2 & 3 Grading\r\nscheme\r\nData and assignment\r\nPackages\r\nReadings and Resources\r\n\r\nCompetencies\r\nTabular analysis\r\n(frequency counts)\r\nSummary of GLM\r\nLog-linear and Logit\r\nmodels with glm()\r\nConfidence\r\nLimit Difference and Ratios (CLD/CLR)\r\nR Commands for models\r\nGenerate\r\npredicted values from linear models\r\nLinear Combinations\r\nfrom GLM models\r\n\r\nNominal,\r\nordinal, and disjoint indicator variables\r\nWhat is a disjoint\r\nindicator\r\nExample of indicators in\r\na model\r\nGenerate indicator\r\nvariables in R:\r\n\r\nPlotting risks versus\r\nexposures\r\nRun a model and predict\r\nPlot predictions\r\n\r\nThe Likelihood Ratio Test\r\n(LRT)\r\nCalculate manually\r\nCalculate in R with\r\nlrtest()\r\n\r\nCategorical\r\nvs. Continuous Coding\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nLAB MATERIALS\r\nLab 02 Goals\r\nEstimate crude measures of disease frequency and 95% confidence\r\nintervals (95% CI) using generalized linear models.\r\nEstimate crude measures of effect and 95% CI using generalized\r\nlinear models.\r\nLearn how to code categorical variables for models and cross\r\ntabulations, including disjoint indicator variables for categorical\r\nvariables with more than 2 categories.\r\nAssess for potential confounding and effect measure\r\nmodification.\r\nLearn how to properly specify a model using backward, stepwise\r\nregression techniques.\r\nLab 2 & 3 Grading scheme\r\n\r\nLab 2\r\nCompetency\r\nPoints\r\n.Rmd file runs without error\r\n10\r\nTable 1\r\n10\r\nTable 2a\r\n10\r\nTable 2b\r\n5\r\nTable 3\r\n10\r\nFigures\r\n15 (5 points each)\r\nTotal\r\n60\r\nLab 3\r\nCompetency\r\nPoints\r\n.Rmd file runs without error\r\n10\r\nTable 4\r\n5\r\nTable 5\r\n10\r\nTable 6\r\n5\r\nShort answer questions\r\n20 (4 points each)\r\nTotal\r\n55\r\n\r\nData and assignment\r\nThe assignment and dataset are both available on Sakai\r\nPackages\r\n{tidyverse}\r\n{fastDummies}\r\n{lmtest}\r\n{biostat3}\r\nReadings and Resources\r\n\r\nModel Forms\r\n\r\n\r\n\r\n\r\nEffect Measure Modification\r\n\r\n\r\n\r\n\r\nPrecision and Validity\r\n\r\n\r\n\r\n\r\n\r\nCompetencies\r\nThe following sections contain the competencies and code that you\r\nwill need to complete labs 2 and 3.\r\nTabular analysis (frequency\r\ncounts)\r\nYou will generate the values for tabular analysis using the same code\r\nas we used last semester. Please refer to last semester’s lab 1 resource\r\nguide for guidance:\r\nLink\r\nto 705 lab 1, tabular analysis\r\nSummary of GLM\r\nGLM (or Generalized Linear Models) are a family of modeling methods\r\nthat can fit linear and non-linear models. They can be classified\r\naccording to the distribution of the outcome\r\nvariable (i.e. dependent/response) and the\r\nlink function, which specifies the\r\nrelationship between the dependent variable (Y) and a linear combination\r\nof covariates (\\(\\beta_1\\)…\\(\\beta_i\\)), as summarized below.\r\nOutcome\r\nRegression Model\r\nDistribution\r\nLink(\\(g(Y)\\))\r\nForm\r\nContinuous\r\nlinear\r\nNormal\r\nidentity\r\n\\(Y = \\beta_0 +\r\n\\beta_1X_1...\\beta_kX_k\\)\r\nBinary\r\nlinear risk\r\nbinomial\r\nidentity\r\n\\(R = \\beta_0 +\r\n\\beta_1X_1...\\beta_kX_k\\)\r\nContinuous\r\nlinear\r\nbinomial\r\nlog\r\n\\(ln(R) =\r\n\\beta_0 + \\beta_1X_1...\\beta_kX_k\\)\r\nContinuous\r\nlinear\r\nbinomial\r\nlogit\r\n\\(logit(R) =\r\n\\beta_0 + \\beta_1X_1...\\beta_kX_k\\)\r\nContinuous\r\nlinear\r\nPoisson\r\nlog\r\n\\(ln(Y) =\r\n\\beta_0 + \\beta_1X_1...\\beta_kX_k\\)\r\nIn the table above:\r\nY = a continuous dependent variable (outcome) or\r\ncount variable (for Poisson models)\r\nR = a probability of a binomial outcome, e.g. risk\r\n(incidence proportion) or prevalence\r\nDistribution refers to the outcome variable\r\nLink is the functional relation between the\r\ndependent variable and the linear combination of covariates (which is\r\nreferred to as the linear predictor: \\(\\beta_0\r\n+ \\beta_1X_1 + \\beta_kX_k\\))\r\nAdditional resources:\r\nHere is a PDF of model forms and\r\nestimation for linear, log and logit risk models.\r\nLog-linear and Logit\r\nmodels with glm()\r\nNote: this\r\nSO question and this\r\nstackexchange query are very helpful, and may be of interest to some\r\nstudents.\r\nIn 707 Lab 1, you may remember we fit a linear regression model using\r\nthe sum of squared errors algorithm (SSE). Our code looked like\r\nthis:\r\n\r\n\r\nm2_lm <- lm(weight~height, data=regdat)\r\n\r\n\r\n\r\nWell, turns out, we can fit the same model using the\r\nglm() function. This will fit the same model but will use a\r\nlikelihood-based estimation algorithm instead of\r\nSSE:\r\n\r\n\r\nm2_glm <- glm(weight ~ height, data = regdat, family = 'guassian'(link = 'identity'))\r\n\r\n\r\n\r\nYou will notice that in order to use the ‘glm’ function, we also have\r\nto specify the distribution of the response variable using the\r\nfamily = option and the link function using\r\nlink =.\r\nFor Lab 2, our only response variable is death, which is binary, so\r\nour family is going to be \"binomial\"; however,\r\nour link function will differ depending on the measure of effect we want\r\nto estimate:\r\nDesired effect measure\r\nLink function name\r\nRisk difference\r\nlink = \"identity\"\r\nRisk ratio\r\nlink = \"log\"\r\nOdds ratio\r\nlink = \"logit\"\r\nNote that we will have to exponentiate our\r\nregression coefficients for the log and logit models since these models\r\nare fit on the log scale; For example, here is a model on the log-linear\r\nscale:\r\n\r\n\r\n\r\n\r\n\r\n# fit model\r\nm3 <- glm(death ~ education_1 + education_2, \r\n          data = kenya, family = 'binomial'(link = 'log'))\r\n\r\n# log-scale coefficients\r\ncoef(m3)\r\n\r\n\r\n(Intercept) education_1 education_2 \r\n-1.87180218 -0.07410797 -0.52609310 \r\n\r\n# risk ratio-scale coefficients\r\nexp(coef(m3))\r\n\r\n\r\n(Intercept) education_1 education_2 \r\n  0.1538462   0.9285714   0.5909091 \r\n\r\n\r\nNote that we do not have to exponentiate our regression coefficients\r\nfrom the linear risk model since there is no transformation.\r\nThe log-scale coefficient is interpreted as the change in the\r\nlog-scale change in \\(y\\) given a\r\none-unit change in \\(x\\). In the above\r\nexample, since our model variables are disjoint and therefore binary, we\r\nwould say that the presence of education_2 (A mother with a\r\npost-primary education) would result in a -0.526 reduced risk of death\r\non the log scale.\r\nBut once we have exponentiated our coefficients, we can interpret\r\nthem on the risk ratio scale:\r\nAccording to our model, the exposure of post-primary education\r\n(education_2) has a risk ratio of 0.59 (a 41% decrease in\r\nrisk) when compared with the reference exposure of No education.\r\nConfidence Limit\r\nDifference and Ratios (CLD/CLR)\r\nConfidence Limit Difference (CLD) and Confidence Limit Ratio (CLR)\r\nare quick and useful measures of the precision of a parameter estimate\r\nand make it easier to compare estimates across studies. For example, two\r\nstudies may report similar point estimates, but one may have a much\r\nwider 95% CI, meaning its value is less precise.\r\n\\(CLD = [upper\\ 95\\%\\ CI] - [lower\\ 95\\%\\\r\nCI]\\)\r\n\\(CLR = \\frac{[upper\\ 95\\%\\ CI]}{[lower\\\r\n95\\%\\ CI]}\\)\r\nR Commands for models\r\nGenerate predicted\r\nvalues from linear models\r\nThe predict() functions are useful for computing\r\npredicted values based on your model fit. This can generate a vector of\r\npredictions based on your data used to fit the model. You can also use\r\ndata that is separate from the model fitting.\r\nThe only requirement is that the newdata column names\r\nneed to be identical to the variables in your model.\r\npredict() is also useful for manually generating\r\nconfidence intervals of your fitted/predicted values.\r\nHow to use predict.lm()\r\nFirst, let’s make a model object:\r\n\r\n\r\nmodel <- glm(weight ~ height, data = regdat)\r\n\r\n\r\n\r\nNext, use predict.glm() to generate predicted values and\r\ntheir confidence intervals using our model object:\r\n\r\n\r\nmodel_predict <- predict.glm(model, interval = \"confidence\")\r\n\r\n\r\n\r\n\r\nx\r\n104.87070\r\n108.69307\r\n92.25687\r\n105.63518\r\n105.63518\r\n92.25687\r\n\r\nWe could also join model_predict back to our original\r\ndata for\r\nthe purpsoe of plotting a line of best fit with confidence\r\nintervals.\r\nWe can use cbind() to combine multiple columns of the\r\nsame length into a single data frame:\r\n\r\n\r\nregdat <- cbind(regdat, model_predict)\r\n\r\n\r\n\r\n\r\nsex\r\nage\r\nheight\r\nweight\r\nmodel_predict\r\nf\r\n155\r\n62.3\r\n105.0\r\n104.87070\r\nf\r\n153\r\n63.3\r\n108.0\r\n108.69307\r\nf\r\n161\r\n59.0\r\n92.0\r\n92.25687\r\nf\r\n191\r\n62.5\r\n112.5\r\n105.63518\r\nf\r\n171\r\n62.5\r\n112.0\r\n105.63518\r\nf\r\n185\r\n59.0\r\n104.0\r\n92.25687\r\n\r\nLinear Combinations from\r\nGLM models\r\nThe lincom() function from package {biostat3} can be\r\nused to generate point estimates and confidence intervals for any\r\ncombination of covariable values in the model; these are referred to as\r\ncontrasts.\r\nRun a model\r\nFirst, let’s use glm() to run a linear risk model to\r\npredict the risk of birth order on death:\r\n\r\n\r\nm1 <- glm(death ~ bord5, data = kenya, family =binomial(link = \"identity\"))\r\n\r\nm1\r\n\r\n\r\n\r\nCall:  glm(formula = death ~ bord5, family = binomial(link = \"identity\"), \r\n    data = kenya)\r\n\r\nCoefficients:\r\n(Intercept)        bord5  \r\n    0.13699      0.01116  \r\n\r\nDegrees of Freedom: 99 Total (i.e. Null);  98 Residual\r\nNull Deviance:      80.99 \r\nResidual Deviance: 80.97    AIC: 84.97\r\n\r\nHere is our model equation as defined by the above output:\r\n\\[\r\n\\begin{eqnarray}\r\nR &=& \\beta_0 + \\beta_1*X \\\\\r\ndeath &=& (Intercept) + bord5*X\\\\\r\n&=& 0.1369863  + 0.0111618*X\r\n\\end{eqnarray}\r\n\\]\r\nPredict risk when bord5 == 0\r\nLet’s use the lincom() command to calculate the\r\npredicted risk of death for a child with birth order 1 - 4\r\n(bord5 == 0):\r\n\r\n\r\nlincom(model = m1, specification = \"(Intercept) + 0*bord5\", level = 0.95)\r\n\r\n\r\n                       Estimate      2.5 %    97.5 %   Chisq\r\n(Intercept) + 0*bord5 0.1369863 0.05811226 0.2158603 11.5873\r\n                       Pr(>Chisq)\r\n(Intercept) + 0*bord5 0.000664037\r\n\r\nNotice how we use the specification = argument to\r\nconstruct our linear combination by referring to the variables in our\r\nmodel. Since \\(0 * bord5\\) will always\r\nresult in 0, we can simplify our specification even further and achieve\r\nthe same result:\r\n\r\n\r\nlincom(model = m1, specification = \"(Intercept)\", level = 0.95)\r\n\r\n\r\n             Estimate      2.5 %    97.5 %   Chisq  Pr(>Chisq)\r\n(Intercept) 0.1369863 0.05811226 0.2158603 11.5873 0.000664037\r\n\r\nPredict risk when bord5 == 1\r\nNow, let’s use lincom() to calculate the predicted risk\r\nof death for a child with birth order >= 5\r\n(bord5 == 1):\r\n\r\n\r\nlincom(model = m1, specification = \"(Intercept) + 1*bord5\", level = 0.95)\r\n\r\n\r\n                       Estimate      2.5 %    97.5 %    Chisq\r\n(Intercept) + 1*bord5 0.1481481 0.01415075 0.2821455 4.695652\r\n                      Pr(>Chisq)\r\n(Intercept) + 1*bord5 0.03023902\r\n\r\nCalculating\r\nrisk difference (bord5 == 1 vs. bord5 == 0)\r\nNext, we can use lincom() to calculate the risk\r\ndifference of death for birth order >= 5 vs. 1 - 4.\r\nHere is the model equation for risk difference in this example:\r\n\\[\r\nRD = \\beta_0 + \\beta_1 - \\beta_0\r\n\\] which reduces to \\(RD =\r\n\\beta_1\\).\r\nThis means that in the lincom() command, we only need to\r\ninclude our parameter for \\(\\beta_1\\):\r\n\r\n\r\nlincom(model = m1, specification = \"bord5\", level = 0.95)\r\n\r\n\r\n        Estimate      2.5 %    97.5 %      Chisq Pr(>Chisq)\r\nbord5 0.01116185 -0.1443258 0.1666495 0.01979595  0.8881085\r\n\r\nMultivariable and logit\r\nmodels\r\nWhen you are working with log or logit risk models, use the argument\r\neform = TRUE to generate estimates that are already\r\nexponentiated.\r\neform i.e. “exponentiated form”\r\nYou may have noticed that the estimates above are quite trivial. If\r\nyou were paying attention, the estimate when bord5 == 0 was\r\nidentical to our intercept.\r\nWith more complex, multivariable models, the lincom()\r\ncommand becomes very handy!\r\nNominal,\r\nordinal, and disjoint indicator variables\r\nCategorical variables can be represented as having values 1, 2, 3, …,\r\nbut one must be careful with such representations. Nominal variables\r\nhave no inherent ordering (e.g., male = 0, female = 1, intersex = 2) and\r\nordinal variables may be qualitatively ordered but may not have uniform\r\nlinear spacing (e.g., low = 0, medium = 1, high = 2). Including such\r\nvariables in models as linear terms means that the model is\r\nmis-specified and can lead to erroneous inference because the\r\nrelationship between the outcome and categorical is assumed to be\r\nlinear.\r\nWhat is a disjoint indicator\r\nDisjoint indicator (a.k.a., dummy) variables derived from nominal or\r\nordinal categories removes the linear assumption and allows more\r\nflexibility in the shape of the outcome-predictor association.\r\nDisjoint indicator variables are derived from categoricals by\r\ngenerating \\(k\\) new variables, one for\r\neach of the \\(k\\) levels of the\r\ncategorical, as illustrated in the table below. You could leave out one\r\nof the indicator variables (the reference level), but I prefer to code\r\nall levels to allow flexibility in changing the reference level as\r\nneeded.\r\nOriginal Coding\r\nOriginal Labels\r\neducation_0\r\neducation_1\r\neducation_2\r\neducation == 0\r\nno primary school\r\n1\r\n0\r\n0\r\neducation == 1\r\nprimary school only\r\n0\r\n1\r\n0\r\neducation == 2\r\npost-primary education\r\n0\r\n0\r\n1\r\nExample of indicators in a\r\nmodel\r\nHere, we will build a linear risk regression model for education,\r\ncoded with indicators.\r\nRisk(preterm | education) = \\(\\beta_0 +\r\n\\beta_1X_1 + \\beta_2X_2\\)\r\nwhere \\(X_1\\)~ =\r\ned_primary and \\(X_2\\)~ =\r\ned_post and the referent category is no primary\r\nschool\r\nIn this model\r\nNo education: \\(R_0 = [\\beta_0 + 0*\\beta_1\r\n+ 0*\\beta_2] = \\beta_0\\)\r\nPrimary school: \\(R_0 = [\\beta_0 +\r\n1*\\beta_1 + 0*\\beta_2] = \\beta_0 + \\beta_1\\)\r\nPost primary: \\(R_0 = [\\beta_0 + 0*\\beta_1\r\n+ 1*\\beta_2] = \\beta_0 + \\beta_2\\)\r\n\r\nTo calculate RDs with “no primary school” as the reference group and\r\n“primary school only” as the index group:\r\n\\[\r\n\\begin{eqnarray}\r\nRD[primary\\ vs.\\ no\\ primary]&=& R_1 - R_0\\\\\r\n&=& [\\beta_0 + 1*\\beta_1 + 0*\\beta_2] - [\\beta_0 + 0*\\beta_1 +\r\n0*\\beta_2]\\\\\r\n&=& [\\beta_0 + \\beta_1] - [\\beta_0] = \\beta_1\r\n\\end{eqnarray}\r\n\\]\r\nTo calculate RDs with “no primary school” as the reference group\r\nand “post-primary education” as the index group:\r\nRD[post-primary vs. no primary]:\r\n\\[\r\n\\begin{eqnarray}\r\nRD[post\\ primary\\ vs.\\ no\\ primary]&=& R_2 - R_0\\\\\r\n&=& [\\beta_0 + 0*\\beta_1 + 1*\\beta_2] - [\\beta_0 + 0*\\beta_1 +\r\n0*\\beta_2]\\\\\r\n&=& [\\beta_0 + 1* \\beta_2] - [\\beta_0] = \\beta_2\r\n\\end{eqnarray}\r\n\\]\r\nRisk Ratio and Odds Ratio are calculated similarly\r\nGenerate indicator variables\r\nin R:\r\nThere are at least 3 ways to make or model disjoint indicator\r\nvariables in R.\r\nOption 1: Manual indicator\r\nvariables\r\nFor Task 2 and 3 in this lab, we would like you to\r\nmanually construct indicator variables. You can use\r\nmutate() and case_when() or\r\nifelse() to create conditional statements that transform\r\nyour nice categorical variables to a matrix of ones and zeroes. Here is\r\nan example of how we would do this using education in our\r\ndata frame:\r\n\r\n\r\nkenya <- kenya %>%\r\n  mutate(\r\n        education_0 = ifelse(education == 0, 1, 0),\r\n        education_1 = ifelse(education == 1, 1, 0),\r\n        education_2 = ifelse(education == 2, 1, 0))\r\n\r\n\r\n\r\nThis has created three new binary variables,\r\neducation_0, education_1, and\r\neducation_2, that each indicate the presence or absence of\r\nthat specific education level for each individual in the data:\r\n\r\neducation\r\neducation_0\r\neducation_1\r\neducation_2\r\n1\r\n0\r\n1\r\n0\r\n2\r\n0\r\n0\r\n1\r\n0\r\n1\r\n0\r\n0\r\n0\r\n1\r\n0\r\n0\r\n1\r\n0\r\n1\r\n0\r\n0\r\n1\r\n0\r\n0\r\n1\r\n0\r\n1\r\n0\r\n\r\nOption 2: fastDummies\r\nFor Task 4, feel free to try {fastDummies}. For\r\nexample, if we want to create disjoint indicator variables for the\r\nstrata of education by bord5, we might run the\r\nfollowing code to create the interaction variable:\r\n\r\n\r\nkenya <- kenya %>%\r\n  mutate(education_bord5 = interaction(education, bord5))\r\n\r\n\r\n\r\nThis has created a new variable, education_bord5, that\r\ncontains 6 unique categories, one for each of the 6 possible unique\r\ncombinations of bord5 and education:\r\nbord5\r\neducation\r\nInteraction Term\r\n0\r\n0\r\n“0.0”\r\n0\r\n1\r\n“0.1”\r\n0\r\n2\r\n“0.2”\r\n1\r\n0\r\n“1.0”\r\n1\r\n1\r\n“1.1”\r\n1\r\n2\r\n“1.2”\r\n\r\n\r\n\r\nNext, we can use dummy_cols() to create the disjoint\r\nindicator variables associated with this interaction variable:\r\n\r\n\r\nkenya <- dummy_cols(kenya, select_columns = \"education_bord5\")\r\n\r\n\r\n\r\n\r\neducation_bord5\r\neducation_bord5_0.0\r\neducation_bord5_1.0\r\neducation_bord5_2.0\r\neducation_bord5_0.1\r\neducation_bord5_1.1\r\neducation_bord5_2.1\r\n1.0\r\n0\r\n1\r\n0\r\n0\r\n0\r\n0\r\n2.0\r\n0\r\n0\r\n1\r\n0\r\n0\r\n0\r\n0.0\r\n1\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0.0\r\n1\r\n0\r\n0\r\n0\r\n0\r\n0\r\n1.0\r\n0\r\n1\r\n0\r\n0\r\n0\r\n0\r\n0.0\r\n1\r\n0\r\n0\r\n0\r\n0\r\n0\r\n\r\nSince we are also fitting main effect parameters in our model\r\n(bord5 and education), any combination of\r\neducation_bord5 that contains 0 is redundant. Once we\r\nconstruct dummy variables, we will only want to include in our model\r\nthose dummy variables that do not contain 0.\r\nOption 3: factor()\r\nNOT RECOMMENDED FOR THIS LAB But feel free to use in\r\nyour own practice in the future (This is what we did in 707 Lab 1).\r\nThe functions glm() and lm() automatically\r\ntreat factor type variables as disjoint indicator\r\nvariables. So if you have factored the variable, you can simply enter\r\nthe variable with the ‘lm’ or ‘glm’ commands.\r\nFor example, the following two bits of code run the same regression\r\nmodel:\r\n\r\n\r\n# 1. using factor-type variable for education:\r\nglm(formula = death ~ factor(education), family = binomial(link = \"identity\"), data = kenya)\r\n\r\n# 2. using disjoint indicator variables for education: \r\nglm(formula = death ~ education_1 + education_2, family = binomial(link = \"identity\"), data = kenya)\r\n\r\n\r\n\r\nReference\r\nCategories and Factor-type Variables\r\nRemember last semester?\r\nWe talked about the importance of setting a “reference” category?\r\nThat concept applies here too. When doing regressions on disjoint\r\ncategorical variables, we need a baseline against which we might compare\r\nthe other levels of that variable.\r\nThe lowest level of a factor-type variable is treated as the\r\nreference category (in regressions, the intercept or \\(\\beta_0\\))\r\nWe can change the reference category of a given factor\r\nvariable by directly setting the levels when constructing the factor\r\n(using levels = c()).\r\nIf we need to change the levels of a factor, we can do so with\r\nrelevel().\r\nHere, we see our current levels of education, in order\r\nfrom 0 to 2:\r\n\r\n\r\nlevels(kenya$education)\r\n\r\n#> [1] \"No primary\"   \"Primary only\" \"Post-primary\"\r\n\r\n\r\n\r\nSay we wanted to set \"Post-primary\" as our reference\r\ncategory. We could do that with relevel()\r\n\r\n\r\nkenya$education1 <- relevel(kenya$education, ref = \"Post-primary\")\r\n\r\nlevels(kenya$education)\r\n\r\n#> [1] \"Post-primary\" \"No primary\"   \"Primary only\"\r\n\r\n\r\n\r\nPlotting risks versus\r\nexposures\r\nRefer back to the section on using\r\npredict.lm() to generate predictions and confidence\r\nintervals, where we generated upper and lower confidence limits from\r\na model along with a model’s predicted values.\r\nOnce joined back to your original data, you can use those values to\r\ngenerate a scatter plot of the risks and upper and lower confidence\r\nlimits, plotted against a predictor variable.\r\nRun a model and predict\r\nRemember that in the section on predict.lm(), we used the following\r\ncode to run a model, generate predictions and confidence intervals, and\r\nthen join those values back to our original data:\r\n\r\n\r\nmodel <- lm(weight ~ height, data = regdat)\r\n\r\nmodel_predictions <- predict.lm(model, interval = \"confidence\")\r\n\r\nregdat <- cbind(regdat, model_predictions)\r\n\r\n\r\n\r\nPlot predictions\r\nWe can use those values to plot our predictions (and their confidence\r\nintervals) against the primary predictor variable of interest (in this\r\ncase, height).\r\nFirst, let’s just plot our predictions\r\n\r\n\r\np1 <- ggplot(data = regdat, aes(x = height, y = fit)) +\r\n  geom_line() +\r\n  geom_point()\r\np1\r\n\r\n\r\n\r\n\r\nWe can add a confidence interval using the {ggplot2} function\r\ngeom_ribbon().\r\n\r\n\r\np1 + geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.2, fill = \"blue\")\r\n\r\n\r\n\r\n\r\n\r\nWhen we map our aesthetics using aes(), we will need to\r\nspecify a ymin and a ymax to determine the\r\nlower and upper bounds of the confidence intervals. In our data, these\r\nare currently named lwr and upr, respectively.\r\nThe Likelihood Ratio Test\r\n(LRT)\r\nIn general, the LRT can be used to determine whether a larger model\r\nwith extra variable(s) fits the data better than a smaller model without\r\nthe extra variable(s) (e.g., 2 models with and without an interaction\r\nterm or 2 models with and without age as a term).\r\nSpecifically, the LRT statistic tests the null hypothesis that a\r\nlarger model maximizes the likelihood of the observed data no better\r\nthan a reduced model that includes fewer covariates.\r\nCritical point – the models must be strictly comparable, so\r\nThe variables in the reduced model must be a strict subset of those\r\nin the larger model\r\nThe observations (e.g., people) in both models must be identical;\r\nthe models are not strictly comparable if you have missing data in a\r\nvariable included in the larger model because you are making estimates\r\non different datasets\r\nIf either of these conditions is not met, the LRT is invalid because\r\nyou are comparing apples to oranges.\r\nIn this section of the lab, we will use the LRT to determine whether\r\na model that allows effect estimates to vary across covariate strata\r\n(heterogeneity) fits the data better than a model that assumes a\r\nconstant RD, RR or OR (homogeneity or constancy).\r\nCalculate manually\r\nThe likelihood ratio test statistic is 2X the difference between the\r\nlog likelihoods of (the reduced model minus the full model):\r\n\\[\r\n\\begin{eqnarray}\r\nLRT\\ Statistic &=& (-2*LogLikelihood(reduced\\_model)) -\r\n(-2*LogLikelihood(full\\_model)) \\\\\r\n&=& (Deviance(reduced\\_model) - (Deviance(full\\_model))\r\n\\end{eqnarray}\r\n\\]\r\nThe LRT statistic is distributed as a Chi-square with degrees of\r\nfreedom:\r\n\\[\r\ndf = k_{full\\ model} - k_{reduced\\ model}\r\n\\]\r\nWhere k is the number of variables in the model.\r\nExtract log-likelihood from\r\nmodel:\r\nIf you’re curious, you can run this calculation in R. To do this, we\r\nneed to first extract the log likelihood from each of our models.\r\nYou can do this with the command logLik():\r\n\r\n\r\nlogLik(model)\r\n\r\n\r\n\r\nThe output of logLik() can then be plugged into the\r\nequation above.\r\nCalculate in R with\r\nlrtest()\r\nYou can use the lrtest() command from the package\r\n{lmtest} to perform a likelihood ratio test comparing the two\r\nmodels.\r\nIf they aren’t already stored in the R environment, run your full\r\nmodel and reduced model and assign them a variable name. For\r\nexample:\r\n\r\n\r\nlm_reduced <- glm(preterm ~ smoker + pnc, data = kenya, family = \"binomial\"(link = \"identity\"))\r\n\r\nlm_full <- glm(preterm ~ smoker + pnc + smoker*pnc, data = kenya, family = \"binomial\"(link = \"identity\"))\r\n\r\n\r\n\r\nThen perform the likelihood ratio test by plugging both models into\r\nthe command:\r\n\r\n\r\nlrtest(lm_reduced, lm_full)\r\n\r\n\r\n\r\nCategorical vs. Continuous\r\nCoding\r\nSome exposure variables may be either analyzed as continuous or\r\ncategorized according to:\r\ncustomary or clinical cut points (e.g., body mass index (BMI)\r\ncategories based on values used to classify people as ‘underweight’,\r\n‘normal’, ‘overweight’ or ‘obese’)\r\nempirical cut points (e.g. tertiles or quartiles), a priori cut\r\npoints relevant to biologic mechanisms\r\npotential public health interventions\r\nother factors of interest or by cut points used commonly in the\r\nliterature (this would allow comparisons between your results and what\r\nhas been published previously).\r\nThe least restrictive way to model continuous variables would be to\r\nestimate separate risks for each value (e.g. each year of maternal age).\r\nHowever, this approach would yield highly imprecise estimates and is not\r\nrecommended.\r\n\r\n\r\n\r\n",
      "last_modified": "2022-05-31T13:53:06-04:00"
    },
    {
      "path": "707_lab_04.html",
      "title": "Lab 4 (707) - Time to Event Analysis",
      "description": "__Lab 04 Due:__ `r read.csv('files/due_dates.csv')[9,2]`, by `r read.csv('files/due_dates.csv')[9,3]`\n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\nLAB MATERIALS\r\nLab 4 Goals\r\nLab 4 Grading scheme\r\nData and assignment\r\nPackages\r\n\r\nSurvival Analysis\r\nDescribing survival\r\ndata\r\nDescribing Survival\r\nData\r\nKaplan-Meier Curves\r\nCox Proportional Hazards\r\nModels\r\nEstimating Time-varying\r\neffects\r\n\r\nCalculating\r\nincidence rates and related measures of association\r\nR commands\r\nfor the tabular analysis of rates\r\nR Commands for\r\nPoisson Regression Models\r\n\r\n\r\n\r\n\r\n\r\nLAB MATERIALS\r\nLab 4 Goals\r\nDescribe survival time data utilizing summary statistics and\r\ngraphical methods\r\nFit Cox proportional hazards regression models with fixed and\r\ntime-varying covariates\r\nAssess appropriateness of the assumptions of the Cox model\r\nCalculate incidence rates and ratios using Poisson methods\r\nLab 4 Grading scheme\r\nCompetency\r\nPoints\r\n.Rmd file runs without error\r\n10\r\nTable 1\r\n5\r\nTable 2\r\n12\r\nTable 3\r\n8\r\nTable 4\r\n15\r\nFigures 1 - 10\r\n30 (3 points each)\r\nShort answer questions\r\n20 (5 points each)\r\nTotal\r\n100\r\nData and assignment\r\nThe assignment and dataset are both available on Sakai\r\nNOTE ON THE DATASET: As you load the data into R, be\r\nsure to notice that this is the original, DHS dataset, with\r\n22,534 observations (i.e. from before we closed the cohort).\r\nPackages\r\n{tidyverse}\r\n{survival}\r\n{survminer}\r\n{epiR}\r\n{fmsb}\r\n{biostat3}\r\nSurvival Analysis\r\nThe methods we have used so far have assumed a closed cohort—no loss\r\nto follow-up or censoring. In the event that we have loss to follow up\r\n(or even if we don’t), we can analyze the time-to-death using methods\r\nfrom survival analysis. The most common approach for this is to utilize\r\nthe Cox proportional hazards regression model. We will first focus on\r\nhow to describe time-to-event data, then we will turn to how to analyze\r\nand check assumptions for the Cox proportional hazards model.\r\nDescribing survival data\r\nSurvival analysis in R is similar to other types of analyses in R\r\nwith the exception that you have to first tell R that you are working\r\nwith time-to-event data. Before we begin to analyze survival data and\r\ngenerate Kaplan-Meier curves, we need to create a Surv()\r\ndata object.\r\nThis is a special data type, unique to the {survival} package. It\r\nsimultaneously indicates an observation’s time at risk, and whether it\r\nwas censored.\r\nThe Surv() function takes two objects from our data\r\nframe: the subject’s follow-up time and whether or not they experienced\r\nthe event.\r\nIn the present analysis, that information is stored in our variables\r\ntime (how many months the child has survived after birth)\r\nand death, respectively. We can create this object as a new\r\nvariable in our data frame using mutate():\r\n\r\n\r\nkenya <- kenya %>%\r\n      mutate(surv_data = Surv(time = time, event = death))\r\n\r\n\r\n\r\nBe sure to inspect that new Surv() column. Here’s how it\r\nappears next to our variables for time and\r\ndeath:\r\n\r\ntime\r\ndeath\r\nsurv_data\r\n13\r\n0\r\n13+\r\n21\r\n0\r\n21+\r\n38\r\n0\r\n38+\r\n205\r\n0\r\n205+\r\n153\r\n0\r\n153+\r\n262\r\n0\r\n262+\r\n38\r\n0\r\n38+\r\n192\r\n0\r\n192+\r\n301\r\n0\r\n301+\r\n325\r\n0\r\n325+\r\n79\r\n0\r\n79+\r\n158\r\n0\r\n158+\r\n\r\n\r\nNote that Surv() can also be created as a standalone object\r\nand does not need to be attached to a data frame as a new column.\r\nWe can then use our Surv() object to build a\r\nsurvfit() time-to-event model, which is parameterized just\r\nlike in the function lm().\r\n\r\n\r\nsurv_bord5 <- survfit(surv_data ~ bord5, data = kenya)\r\n\r\n\r\n\r\nIf we inspect this model, we’ll see that it is the tabular\r\nrepresentation of a survival curve. Use summary() to\r\ninspect it for yourself.\r\n\r\n\r\nsummary(surv_bord5)\r\n\r\n\r\n\r\n\r\nSeriously, run this summary() code for yourself and see.\r\nThe output is too long for the website. Once you do, notice how these\r\ncalculations are similar to the ones your performed manually in the\r\ntake-home assignment from class.\r\nThe summary of surv_bord5 will show us a tabulation of\r\nthe cohort’s survival probability at each time-step, along with how many\r\nwere at risk and the number of events in that interval.\r\nDescribing Survival Data\r\nSurvival data can be described both graphically and analytically.\r\nMeasures such as total time at risk, number of subjects and median\r\nsurvival time are common values reported in survival analysis\r\nreports.\r\nIf we print() the surv_fit() object\r\ngenerated above, we receive a brief set of summary statistics that would\r\nsuffice for a good portion of Table 1. But median values are NA. And\r\nthere’s no mean. Oh no!\r\n\r\nCall: survfit(formula = surv_data ~ bord5, data = kenya)\r\n\r\n           n events median 0.95LCL 0.95UCL\r\nbord5=0 1559    141     NA      NA      NA\r\nbord5=1  441     54     NA      NA      NA\r\n\r\nThis\r\nStats Exchange post explains why. Basically, our data don’t have\r\nenough events to calculate a median.\r\nIn view of this, to fill out Table 1, we will need to rely on the\r\nsummarize() function to get summary data for the variables\r\ntime and death\r\nKaplan-Meier Curves\r\nGraphically, Kaplan-Meier curves are plots of the probability of\r\nsurvival as a function of time. These are often presented for each level\r\nof the primary exposure variable and are usually unadjusted (although\r\nadjusted KM curves are possible).\r\nThe package {survminer} contains plotting functions that interact\r\nwith survfit() objects to plot survival curves. The syntax\r\nis a bit different than {ggplot2}, but the concept is the same.\r\n\r\n\r\nggsurvplot(fit = surv_bord5, data = kenya,\r\n           ylim = c(.8, 1),\r\n           conf.int = TRUE,\r\n           xlab = \"Time since birth (months)\",\r\n           ylab = \"Probability of Survival\",\r\n           title = \"Survival Probability by birth order\",\r\n           legend = \"bottom\", # this specifies legend position\r\n           legend.title = \"Birth Order (bord5)\",\r\n           legend.labs = c(\"1-4\", \"5+\")\r\n        )\r\n\r\n\r\n\r\n\r\n\r\nFor more plotting options of ggsurvplot(), check out this\r\nlink\r\nCox Proportional Hazards\r\nModels\r\n\r\n\r\n\r\n\r\nReading - Hazard Ratios\r\n\r\n\r\n\r\n\r\n\r\n\r\n(Click on the thumbnail for further reading.)\r\nIn R, Cox PH models are parameterized just like the\r\nsurvfit() function, in that they take a Surv()\r\nobject as the response variable. However, they’re different in their\r\noutput. The output is similar to the familiar regression output, with\r\ncoefficients.\r\n\r\n\r\ncox_bord5 <- coxph(Surv(time, death) ~ bord5, data = kenya, ties = \"breslow\")\r\n\r\nsummary(cox_bord5)\r\n\r\n\r\nCall:\r\ncoxph(formula = Surv(time, death) ~ bord5, data = kenya, ties = \"breslow\")\r\n\r\n  n= 2000, number of events= 195 \r\n\r\n        coef exp(coef) se(coef)     z Pr(>|z|)  \r\nbord5 0.3874    1.4732   0.1606 2.413   0.0158 *\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\n      exp(coef) exp(-coef) lower .95 upper .95\r\nbord5     1.473     0.6788     1.075     2.018\r\n\r\nConcordance= 0.537  (se = 0.016 )\r\nLikelihood ratio test= 5.48  on 1 df,   p=0.02\r\nWald test            = 5.82  on 1 df,   p=0.02\r\nScore (logrank) test = 5.89  on 1 df,   p=0.02\r\n\r\nAdjusted models\r\nTo estimate the effect of bord5, adjusting for maternal\r\neducation (coded with disjoint indicator variables), we would use the\r\nfollowing code:\r\n\r\n\r\ncox_bord5_ed <- coxph(Surv(time, death) ~ bord5 + \r\n      education_c2 + education_c3, \r\n      ties = \"breslow\",\r\n      data = kenya)\r\n\r\n\r\n\r\nAlternatively, we can parameterize education as a factor-type\r\nvariable to obtain the same result:\r\n\r\n\r\ncox_bord5_ed <- coxph(Surv(time, death) ~ bord5 + factor(education), \r\n                      ties = \"breslow\",\r\n                      data = kenya)\r\n\r\n\r\n\r\nViolation of\r\nthe Proportional Hazards Assumption\r\nThe standard Cox proportional hazards model assumes that the effect\r\nof a covariate on the time to an event (e.g. bord5) remains the same\r\nacross follow-up (i.e., is proportional or constant over time). This\r\nmight not be a reasonable assumption. We can evaluate if our data are\r\nconsistent with this assumption through graphical or test-based\r\nmethods.\r\nA common graphical method is to plot a transformation of survival for\r\ncategories of a variable (S(t), the probability of not having the event)\r\nagainst the natural logarithm of follow-up time. In particular, the\r\nln(-ln(S(t)) for two categories of a given variable should remain fairly\r\nparallel in a loose sense (never cross, although the lines may not be\r\nperfectly straight).\r\nIf two lines cross, or appear to be converging (or diverging) even\r\noutside of the range of time used, then this can indicate that the\r\nproportional hazards assumption may not be reasonable and you should\r\nexplore estimating an effect that varies over time.\r\nThese plots are only useful for categorical variables and cannot be\r\nutilized for continuous variables.\r\nIn R, we generate this type of plot similar to a Kaplan Meier, except\r\nwe need to specify our function (fun). Its default gives\r\nthe survival probability, but here we will set\r\nfun = \"cloglog\".\r\n\r\n\r\nggsurvplot(surv_bord5, fun = \"cloglog\",\r\n           conf.int = TRUE,\r\n           xlab = \"Natural log of time (months)\",\r\n           ylab = \"log-log survival\",\r\n           title = \"log-log curves by bord5\",\r\n           legend = \"bottom\", # this specifies legend position\r\n           legend.title = \"Birth Order (bord5)\",\r\n           legend.labs = c(\"1-4\", \"5+\"))\r\n\r\n\r\n\r\n\r\n\r\nNote that we have used the original survival object,\r\nsurv_bord5 to generate this plot. The coxph()\r\nmodel object cannot be used for plotting, unfortuneately.\r\nPlotting adjusted models\r\nTo plot an adjusted model, we’ll need to convert it to a\r\nsurvfit() object:\r\n\r\n\r\nsurv_bord5_ed <- survfit(cox_bord5_ed, data = kenya)\r\n\r\n\r\n\r\nYou might notice that if we try and plot the adjusted model, the plot\r\nappears unstratified:\r\n\r\n\r\nggsurvplot(surv_bord5_ed, fun = \"cloglog\",\r\n           conf.int = TRUE,\r\n           xlab = \"Time (in months) using log\",\r\n           ylab = \"log-log survival\",\r\n           title = \"log-log curves by bord5(?)\")\r\n\r\n\r\n\r\n\r\nThe strata() function allows us to indicate which\r\nvariable we wish to evaluate for the proportional hazards assumptions.\r\nThis function is used within our model equation.\r\nSo to get a proper stratified “cloglog” plot from our adjusted Cox\r\nmodel, we must re-build the model and specify which variable to stratify\r\nwith strata(), then convert to survfit(), and\r\nonly then will it plot properly:\r\n\r\n\r\ncox_bord5_ed_strata <- coxph(Surv(time, death) ~ strata(bord5) + \r\n      education_c2 + education_c3 + male + mage + mage2, \r\n      ties = \"breslow\",\r\n      data = kenya)\r\n\r\nsurv_bord5_ed_strata <- survfit(cox_bord5_ed_strata, data = kenya)\r\n\r\nggsurvplot(surv_bord5_ed_strata, fun = \"cloglog\",\r\n           conf.int = TRUE,\r\n           xlab = \"Time (in months) using log\",\r\n           ylab = \"log-log survival\",\r\n           title = \"log-log curves by bord5(!)\",\r\n           legend = \"bottom\", # this specifies legend position\r\n           legend.title = \"Birth Order (bord5)\",\r\n           legend.labs = c(\"1-4\", \"5+\"))\r\n\r\n\r\n\r\n\r\nIf we compared these adjusted log-log survival curves with those of\r\nthe plain model, where bord5 is the only predictor\r\n(formula = Surv(time, death) ~ bord5), we would find that\r\nthe plots are quite alike, but contain subtle differences in the\r\nalignment of the curves. In evaluating the proportionality assumption in\r\nCox regression, we only need the assumption to hold, conditional on\r\nmodeled covariates; so plotting these adjusted curves is useful.\r\nEstimating Time-varying\r\neffects\r\nIn the event that you find the proportional hazards assumption\r\nviolated for a particular covariate, you should report the effect of the\r\nexposure at several time points (you can think of this as effect\r\nmodification of the exposure by follow-up time). The specification of a\r\nCox model with a time-varying effect for the covariate \\(X_1\\) is:\r\n\\[\r\nln[h(t)] = ln[h_0(t) + \\beta_1X_1 + \\beta_2X_1*t]\r\n\\] Notice that this is in a form similar to our original Cox\r\nregression formula, but contains an interaction term between our main\r\neffect and \\(t\\), time.\r\nTo estimate the hazard ratio for \\(X_1 =\r\n1\\) vs. \\(X_1 = 0\\) at time\r\n\\(t = T\\):\r\n\\[\r\n\\begin{eqnarray}\r\nln(HR) &=& ln \\left[ \\frac{h(t|X=1, t=T)}{h(t|X=0, t =\r\nT)}\\right] = \\frac{ln[h(t|X=1, t=T)]}{ln[h(t|X=0, t = T)]}\\\\\r\n\\\\\r\n&=&[ln(h_0(T)) + \\beta_1(1) + \\beta_2(1)*T] - [ln(h_0(t)) +\r\n\\beta_1(0) + \\beta_2(0)*T]\\\\\r\n&=& \\beta_1(1) + \\beta_2(1) * T\\\\\r\n\\\\\r\nHR &=& exp(\\beta_1 + \\beta_2*T) \\\\\r\n95\\%\\  CI &=& exp[\\beta_1 + \\beta_2*T \\pm (1.96 * SE(\\beta_1 +\r\n\\beta_2*T))]\r\n\\end{eqnarray}\r\n\\]\r\nYou can use the Cox regression model that contains the time-varying\r\neffect lincom() command to generate time-varying effects.\r\nRemember to ‘eform’!\r\nFormal\r\nstatistical test of proportional hazards\r\nWe can use R to formally test the significance of this time-varying\r\neffect as it pertains to the assumption of proportional hazards.\r\nBy allowing the hazard ratio to vary across time, we test if this\r\ntime-varying effect is significant. To perform this, we\r\nuse the tt() function in our model formula, and then use\r\nthe argument tt = to specify the function by which\r\ntime is interacting with bord5 in our\r\nmodel:\r\n\r\n\r\ncox_tt <- coxph(Surv(time, death) ~ bord5 + male + rural + education_c2 + education_c3 + mage + mage2 + tt(bord5),\r\n      data = kenya, \r\n      ties = \"breslow\",\r\n      tt = function(x, time,...)x*time)\r\n\r\n\r\n\r\nCalculating\r\nincidence rates and related measures of association\r\nJust as the generalized linear model can be used to estimate risks,\r\nrisk differences and risk ratios, we can also use this methodology to\r\nestimate incidence rates and incidence rate ratios from the counts of\r\noutcomes over time. The Poisson distribution is used to model counts or\r\nrates and has the general form:\r\n\\[\r\nln(deaths) = \\beta_0 + \\beta_1X_1... + \\beta_kX_k + ln(time)\r\n\\] where family = “poisson” and link =\r\n“log”.\r\nNotice that the specification of the Poisson model includes the\r\nnumber of deaths and the logarithm of the exposure time. Rearranging the\r\nabove equation and a bit of algebra shows how this can be used to derive\r\nthe incidence rate (IR):\r\n\\[\r\n\\begin{eqnarray}\r\nln(deaths) - ln(time) &=& \\beta_0 + \\beta_1X_1... + \\beta_kX_k\\\\\r\nln\\left(\\frac{deaths}{time}\\right) &=& \\beta_0 +\r\n\\beta_1X_1...+\\beta_kX_k\\\\\r\nln(IR) &=& \\beta_0 + \\beta_1X_1...+\\beta_kX_k\r\n\\end{eqnarray}\r\n\\]\r\nTo estimate incidence rates for \\(X_1 =\r\n0,1\\):\r\n\\[\r\n\\begin{eqnarray}\r\nln[IR | X_1] &=& \\beta_0 + \\beta_1X_1\\\\\r\n\\\\\r\n\\textbf{when\\  X_1 = 0:} \\\\\r\nln(IR_0) &=& \\beta_0 + \\beta_1(0)\\\\\r\nIR_0 &=& exp(\\beta_0)\\\\\r\n\\\\\r\n\\textbf{when\\  X_1 = 1:} \\\\\r\nln(IR_1) &=& \\beta_0 + \\beta_1(1)\\\\\r\nIR_1 &=& exp(\\beta_1 + \\beta_1)\r\n\\end{eqnarray}\r\n\\]\r\nTo estimate the incidence rate ratio (IRR) for \\(X_1 = 1\\) vs. \\(X_1 = 0\\):\r\n\\[\r\n\\begin{eqnarray}\r\nln(IRR) = ln\\left(\\frac{IR_1}{IR_0}\\right) &=& ln(IR_1) -\r\nln(IR_0)\\\\\r\n&=&[\\beta_0 + \\beta_1(1)] - [\\beta_0 + \\beta_1(0)] \\\\\r\n&=& \\beta_1\r\n\\\\\r\n\\\\\r\nIRR &=& exp(\\beta_1)\\\\\r\n95\\%\\ CI &=& exp[\\beta_1 \\pm (1.96 * SE(\\beta_1))]\r\n\\end{eqnarray}\r\n\\]\r\nIRs and IRRs can be estimated through linear combinations of the\r\nregression parameters as we have done with previous linear models, using\r\nlincom().\r\nR commands for the\r\ntabular analysis of rates\r\nYou can use the pyears() function from the {survival}\r\npackage to generate values that are useful in calculating incidence\r\nrates and incidence rate ratios\r\n\r\n\r\nptime_bord5 <- pyears(Surv(time, death) ~ strata(bord5), \r\n                      data = kenya, scale = 1)\r\n\r\nsummary(ptime_bord5)\r\n\r\n\r\nCall: pyears(formula = Surv(time, death) ~ strata(bord5), data = kenya, \r\n    scale = 1)\r\n\r\nnumber of observations = 2000\r\n\r\n strata(bord5)    N     Events    Time   \r\n--------------- ------ -------- -------- \r\n    bord5=0      1559    141     203041 \r\n    bord5=1       441     54      40760 \r\n\r\nYou can then access values within the pyears() object\r\nlike this:\r\n\r\n\r\n# events of death in strata bord5 = 0\r\nptime_bord5$event[1]\r\n\r\n\r\nbord5=0 \r\n    141 \r\n\r\n# person time in strata bord5 = 0\r\nptime_bord5$pyears[1]\r\n\r\n\r\nbord5=0 \r\n 203041 \r\n\r\n# events of death in strata bord5 = 1\r\nptime_bord5$event[2]\r\n\r\n\r\nbord5=1 \r\n     54 \r\n\r\n# person time in strata bord5 = 1\r\nptime_bord5$pyears[2]\r\n\r\n\r\nbord5=1 \r\n  40760 \r\n\r\nYou can then use the functions ratedifference() and\r\nrateratio() from the package {fmsb} to calculate the\r\nincidence rate difference and incidence rate ratio, respectively.\r\nBoth functions take the same arguments:\r\n\r\n\r\nratedifference(a, b, PT1, PT0, conf.level = 0.95)\r\nrateratio(a, b, PT1, PT0, conf.level = 0.95)\r\n\r\n\r\n\r\nWhere a is the number of disease occurence\r\namong the exposed,\r\nb is the number of diease occurence among\r\nthe unexposed,\r\nPT1 is the observed person-time in the\r\nexposed cohort,\r\nAnd PT0 is the observed person-time in the\r\nunexposed cohort.\r\nR Commands for Poisson\r\nRegression Models\r\nThe command for a Poisson regression model has the following\r\nform:\r\n\r\n\r\npoisson_bord5 <- glm(death ~ bord5 + offset(log(time)), \r\n                     family = \"poisson\"(link = \"log\"), data = kenya)\r\n\r\n\r\n\r\nAccess the model summary just like any normal glm()\r\nmodel:\r\n\r\n\r\nsummary(poisson_bord5)\r\n\r\n\r\n\r\nNote that the exposure time is explicitly specified in the GLM\r\noptions using offset(). We must use the natural log of\r\ntime to generate the appropriate estimates since our model\r\nis fit on the log scale (link = \"log\").\r\nTo generate estimates of Incidence Rates (IR) and Incidence Rate\r\nRatios (IRR), you can use the lincom() command with your\r\npoisson models.\r\n\r\n\r\n# IR at intercept\r\nlincom(poisson_bord5, \"(Intercept)\", eform = TRUE)\r\n\r\n# IR at bord5 == \"5+\"\r\nlincom(poisson_bord5, \"(Intercept) + bord5\", eform = TRUE)\r\n\r\n# IRR and CI\r\nlincom(poisson_bord5, \"bord5\", eform = TRUE)\r\n\r\n\r\n\r\nTo have IR/IRR reported (instead of beta coefficients), be sure to\r\nset eform = TRUE in your lincom()\r\nfunction.\r\n\r\n\r\n\r\n",
      "last_modified": "2022-05-31T13:53:13-04:00"
    },
    {
      "path": "about.html",
      "title": "About",
      "description": "Reach out to your instructors for assistance. Learn more about them here.",
      "author": [],
      "contents": "\r\n\r\nContents\r\nLarry\r\nNour\r\nNathan\r\nThe Catscot\r\n\r\nLarry\r\n\r\n\r\n\r\nLarry has been growing out his hair.\r\nNour\r\n\r\n\r\n\r\nNour, in cosplay as a butterfly. Pretty spot on, eh?\r\nOH: Thursday, 9am - 10am\r\nNathan\r\n\r\n\r\n\r\nNathan is a professional potato.\r\nOH: Tuesday, 12:30am - 1:30pm\r\nThe Catscot\r\n\r\n\r\n\r\nOur little mascot in the upper-left hand corner was Designed courtesy\r\nof 699pic at pngtree.com\r\n\r\n\r\n\r\n",
      "last_modified": "2022-05-31T13:53:14-04:00"
    },
    {
      "path": "broken.html",
      "title": "Broken Code",
      "author": [],
      "contents": "\r\nSome tips for code that won’t\r\nrun\r\nDon’t hesitate to ask for help if you are having problems with\r\nsoftware, code, etc. But be warned, your question should be specific and\r\ndetailed, not just “Why won’t this line of code run?”\r\nIn my experience, unsent emails are sometimes better (and faster)\r\nthan a TA. What looks like a hairy knot in your code is often quick to\r\nuntangle when you start putting things in writing. This is why we ask\r\nthat any email you send be both detailed and specific.\r\nHere are some other things that may help in getting your code to\r\nwork:\r\nCheck that you haven’t mixed up == and\r\n=\r\nGo for a walk (seriously, maybe you’ve been sitting for too\r\nlong)\r\nGoogle your error message – there’s almost always someone else who\r\nhas encountered a similar problem\r\nDon’t delete and make changes to the same line of code. Copy/paste\r\ninto a new chunk. Formulate a hypothesis. Make changes. Take notes on\r\nwhat changed. Revise your hypothesis. Repeat.\r\nLearn the R\r\nDebugger (The code for 705’s lab probably won’t need the debugger,\r\nbut if you’re getting deep into things, ask Nathan about it)\r\nYou might also flip through this\r\narticle on commmon problems in R Markdown, which might provide some\r\nanswers.\r\n\r\n\r\n\r\n",
      "last_modified": "2022-05-31T13:53:15-04:00"
    },
    {
      "path": "data_dict.html",
      "title": "Data Dictionary",
      "description": "Data Dictionary for Kenya's 2008 Demographic and Health Survey.\n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\nData Dictionary Help\r\nKenya DHS Data Dictionary\r\n(file)\r\nCentury Month Code\r\n\r\n\r\n\r\n\r\n\r\nData Dictionary Help\r\nGuidelines\r\nfor Data Collection & Data Entry - Theresa A Scott, MS\r\n(Vanderbilt University)\r\nKenya DHS Data Dictionary (file)\r\n\r\n\r\n\r\nCentury Month Code\r\nAll dates in the data file are expressed in terms of months and years\r\nand also as century month codes. A century month code (CMC) is the\r\nnumber of the month since the start of the century.\r\nFor example, January 1900 is CMC 1, January 1901 is CMC 13, January\r\n1980 is CMC 961, September 1994 is CMC 1137.\r\nThe CMC for a date is calculated from the month and year as follows:\r\nCMC = (YY * 12) + MM for month MM in year 19YY.\r\nTo calculate the month and year from the CMC use the following\r\nformulae: YY = int((CMC - 1) / 12) MM = CMC - (YY * 12)\r\nFor Dates in 2000 and after the CMC is calculated as follows: CMC =\r\n((YYYY-1900) * 12) + MM for month MM in year YYYY.\r\nTo calculate the month and year from the CMC use the following\r\nformulae: YYYY = int((CMC - 1) / 12)+1900 MM = CMC - ((YYYY-1900) *\r\n12)\r\n\r\n\r\n\r\n",
      "last_modified": "2022-05-31T13:53:16-04:00"
    },
    {
      "path": "fun_dict.html",
      "title": "Function Dictionary",
      "description": "A searchable table of all relevant functions for 705 and 707 labs\n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2022-05-31T13:53:17-04:00"
    },
    {
      "path": "guide_705_lab_0.html",
      "title": "Lab 0 (705)",
      "description": "__Due:__ `r read.csv('files/due_dates.csv')[1,2]` by `r read.csv('files/due_dates.csv')[1,3]`\n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\nLAB MATERIALS\r\nSlides\r\nLab 0 Goals\r\nTask 1: Establish a\r\nworkflow\r\nCreate\r\nfolders (directories!) and projects to store your work\r\nCreate a New Project in\r\nRStudio\r\n\r\nTask 2:\r\nFamiliarize yourself with R Markdown\r\nTask 3:\r\nInstall packages and load libraries\r\nInstalling packages\r\nLoad libraries\r\n\r\nTask 4: Load data\r\nObjects\r\nThe assignment operator\r\n(<-)\r\n\r\nTask 5: Explore the data\r\nTask 6: Create variable\r\nmage\r\nPipes (%>%)\r\n\r\nTask 7: Frequency\r\ndistributions of mage\r\nTask 8: Create variable\r\nmagec\r\ncase_when()\r\nConvert a\r\ncharacter variable to a factor\r\n\r\nTask 9: Cross-tab of\r\nmage and magec\r\nMethod 1\r\nMethod 2\r\n\r\nTask 10: Save new\r\ndataset\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nNote: This lab is not graded (i.e. does not\r\ncontribute to your course evaluation, so don’t fret)\r\nLAB MATERIALS\r\nR Markdown\r\nfile for Lab 0 Click link to download. Fill it in with your\r\nanswers to the following lab tasks. Once you’re finished, rename it as\r\nLab0_FirstinitialYourlastname.Rmd, and submit it using the\r\nSakai dropbox.\r\nLab_0_kenya.rds - data file available in the Resources folder of\r\nthe Sakai course webpage\r\nSlides\r\n\r\n\r\n\r\n\r\n\r\nLab 0 Goals\r\nBy the end of this lab, you will have demonstrated a basic\r\nfamiliarity with the R coding environment and R Markdown files. You will\r\nbe able to:\r\nStart a new project and import data\r\nUnderstand useful R terminology\r\nUse summary statistics to describe the Kenya dataset\r\nDerive an ordinal categorical variable from a continuous one\r\nGenerate histograms and boxplots using ggplot2\r\nCreate a simple cross-tabulation of two variables\r\nTask 1: Establish a workflow\r\nIn R, we organize our work by projects. It is best practice to keep\r\nno more than one project in any single folder on your computer. First,\r\nwe should establish the file folders from which we’ll be working.\r\nThis should always be your first step when beginning an\r\nanalysis.\r\nCreate\r\nfolders (directories!) and projects to store your work\r\nIf you haven’t already, now would be a good time to:\r\nCreate a new folder on your computer, calling it “705 Lab”.\r\nWithin that folder, create a folder for this lab, called “Lab\r\n0”.\r\nFinally, within that folder, create a folder for your data, called\r\n“data”\r\nSave the dataset titled Lab_0_kenya.rds (downloaded\r\nfrom Sakai) into the folder called “data”\r\nThe resulting file path for your data file should look like this:\r\n“/705 Lab/Lab 0/data/Lab_0_kenya.rds”\r\nPlease make this a habit. We will expect you to do this for\r\nevery lab, as it will keep your work organized and will keep\r\nyou happy.\r\nCreate a New Project in\r\nRStudio\r\nNow open RStudio and take a deep breath. Don’t panic. This will all\r\nbe very familiar in a few short months.\r\nInitiate a new project by going to File and\r\nclicking New Project.\r\n Then\r\nselect Existing Directory and hit\r\nBrowse. Navigate to the folder titled\r\nLab 0. Open it, then hit\r\nOpen.\r\nIf you’ve done this correctly, your new folder, “Lab 0” should appear\r\nin the bottom-right R Studio window pane, under the tab “Files”\r\nLikewise, the folder name, “Lab 0” should appear in the upper-right hand\r\ncorner, indicating the current project’s name.\r\n\r\nCurrent project indicator is marked as “G” in this\r\nhelpful screenshot of the R Studio environment\r\nTask 2: Familiarize\r\nyourself with R Markdown\r\nNow, you will open a special kind of document known as “R Markdown”.\r\nThis is a text editor (like Word or Google Docs), but with a twist. You\r\ncan implement the instructions you have written in the R programming\r\nlanguage directly within the document. That is, you can “run\r\ncode” directly within the document. This makes data analysis an\r\ninteractive, iterative (and therefore fun?) process that usually\r\nproceeds as follows:\r\nThe\r\nword “run” can have so\r\nmany meanings. So exactly what do we mean when we say “run\r\ncode”?\r\nIt’s a term meant to describe the process of allowing a computing\r\nprogram or software to operate upon something you’ve created in\r\nyour computing environment. In R, you can “run code directly in the R\r\nconsole by typing your code and hitting enter. R Markdown can\r\nalso run chunks of code, which we’ll see in a moment.\r\nWrite some code to generate new variables and perform statistical\r\nanalysis\r\nRun the code\r\nObserve how your dataset behaved\r\nTake a few notes (for Science)\r\nTweak code\r\nRepeat\r\nFor each lab, we will provide you with a skeleton Markdown file. If\r\nyou haven’t already, download that file (“lab0_705_fall2021.Rmd”,\r\navailable at the top of this page) and save it to your folder named “Lab\r\n0”.\r\nNow open the file in RStudio. You can just double-click on it from\r\nthe file folder. It will appear in the RStudio window pane, but might\r\nfeel a little pinched. Luckily, RStudio allows Markdowns to pop-out.\r\nClick the white square at the top of the Lab 0 document (circled in\r\nred in the photo below) to do just that:\r\nPop-outTask 3: Install\r\npackages and load libraries\r\nInstalling packages\r\nPackages are collections of functions. As we’ll see shortly, we use\r\nfunctions as code to inspect, manipulate, and analyze our data. They are\r\nthe verbs of the R language.\r\nThere are packages that come built-in with R. These have names like\r\n{base}, {utils}, and {stats}.\r\nSince R is open-source, users are able to create their own packages\r\nso that other R users can use them. These packages are available in\r\nplaces like the\r\nComprehensive R Archive Network (CRAN for short) and GitHub.\r\nLucky for us, packages are easily retrieved from the R Console. If\r\nyou haven’t already, run the following code from the R Console, one line\r\nat a time, to download the packages that we’ll be needing for this\r\nsemester. Unless you uninstall R, __*you should only ever have to do\r\nthis once__*:\r\n\r\n\r\ninstall.packages(\"tidyverse\")\r\n\r\n\r\n\r\n\r\n\r\ninstall.packages(\"skimr\")\r\n\r\n\r\n\r\n\r\n\r\ninstall.packages(\"tableone\")\r\n\r\n\r\n\r\n\r\n\r\ninstall.packages(\"epiR\")\r\n\r\n\r\n\r\n\r\n\r\ninstall.packages(\"devtools\")\r\n\r\n\r\n\r\n\r\n\r\ndevtools::install_github(\"potato-nathan/epiAssist\")\r\n\r\n\r\n\r\nLoad libraries\r\nUnlike installing packages, every time we open a new R session, we\r\nneed to enable a package’s use in the R environment. To enable R to use\r\na specific package and its functions, we can load them using the\r\nlibrary() function.\r\nIn a fresh code chunk, call in the {tidyverse} and {skimr} packages\r\nusing the following code:\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(skimr)\r\n\r\n\r\n\r\nNotice that when we install packages, we need to specify\r\ntheir names using quotes because the package name is not yet known to\r\nyour own copy of RStudio. On the other hand, when we load them into the\r\nR environment using library(), R automatically recognizes\r\nthem as the names of packages, so they don’t require\r\nquotations.\r\nTask 4: Load data\r\nYou will use the dataset Lab_0_kenya.rds for this lab.\r\nYou’ve hopefully already saved it to the folder Lab 0 >\r\ndata. You will use the function readRDS() to\r\nimport the data file from your computer’s folder.\r\nSince our project has been created within the “Lab\r\n0” folder, it is thus our “Working Directory”, and RStudio will\r\nautomatically start from that folder when we give it a function that\r\nasks it to look in our file directory.\r\nWithin our function, all we need to do is specify the file name, and\r\nthat it’s in the folder called “data”.\r\nUse the following code to load your data into R and give it the name\r\nkenya.\r\n\r\n\r\nkenya <- readRDS('data/Lab_0_kenya.rds')\r\n\r\n\r\n\r\nWhen we load our data into R, it becomes what, in R, is called a\r\ndata frame, which is the R term used for a\r\ndataset object. Without going into too much detail, it’s like having a\r\nspreadsheet of data with rows (i.e. different individual records) and\r\ncolumns (i.e. variables). For those of you familiar with mathematical\r\nterminology, it’s like a matrix.”\r\nObjects\r\nIf functions are the verbs of the R language, objects are the nouns.\r\nJust like nouns, there are many different types of objects,\r\nwhich we will learn about throughout the semester. For now, you just\r\nneed to understand that an object is anything in your R environment that\r\nis able to be explored, transformed, or analyzed by R functions. Objects\r\nalso have unique names. In the code above, kenya is a data\r\nframe object.\r\nTo further illustrate, in the chunk of code below, fruit\r\nbecomes a character vector object of length 3, number\r\nbecomes a numeric vector object of length 1, and logical\r\nbecomes a logical vector object of length 1.\r\nYou can use class() to inspect an object’s type, and\r\nobject.size() to inspect an object’s size. We will learn\r\nmore about vectors\r\nin a later assignment.\r\nMost functions require specific types of objects.\r\nThe assignment operator\r\n(<-)\r\nThe <- is called the Assignment\r\nOperator. We use it to assign names to objects in our\r\ncoding environment:\r\nWe can use our assignment operator for characters, numbers, logical\r\noperators, etc.:\r\n\r\n\r\nfruit <- c(\"oranges\", \"papayas\", \"apricots\")\r\n\r\nnumber <- 99\r\n\r\nlogical <- FALSE\r\n\r\n\r\n\r\nNow that the above values are stored in our environment, we can use\r\nthem in other functions or operations as predefined variables:\r\n\r\n\r\npaste(fruit, \"are orange\", sep = \" \")\r\n#> [1] \"oranges are orange\"  \"papayas are orange\"  \"apricots are orange\"\r\n\r\nnumber + 1\r\n#> [1] 100\r\n\r\nisTRUE(logical)\r\n#> [1] FALSE\r\n\r\n\r\n\r\nWe’ve done the same thing with our dataset, giving it the name\r\nkenya. We might use the function head() to\r\nview the first six rows in the dataset. This is a quick and easy way to\r\nglance at our dataset and its accompanying variables:\r\n\r\n\r\nhead(kenya)\r\n\r\n\r\n\r\nTask 5: Explore the data\r\nFamiliarize yourself with the data by using the commands\r\nncol(), nrow(), class(),\r\nnames() and skim().\r\nAre there any string/character variables?\r\nAre there any variable or value labels?\r\nDo any variables have notes?\r\nSimilar to head(), we can feed our kenya\r\ndata frame to various functions that tell us other useful information\r\nabout it. As a tip, you can use $ in the format\r\n[datasetName]$[variableName] to refer to a specific\r\nvariable/column within a dataset.\r\nUse ncol() to print the number of columns in our data\r\nframe\r\nUse nrow() to print the number of rows\r\nUse names() to view each variable’s name.\r\nUse class() to view each variable’s “type”\r\nUse skim() to print summary statistics for each variable\r\nin the data frame\r\nTask 6: Create variable\r\nmage\r\nUsing a pipe (%>%) and the mutate()\r\nfunction, create a new variable, mage for mother’s age (as\r\nan integer) at the time of each child’s birth (note – some of these\r\nmothers have had multiple children).\r\nThis is calculated from variables b3 (month code of\r\nchild’s birth) and v011 (month code of mother’s birth). The\r\ndifference between the values of these variables is in months, so\r\ndivide by 12 to get years. See the\r\ndata dictionary for a more detailed description of month codes and\r\nhow to use them. Use as.integer() around your calculation\r\nto truncate the calculated values for mage to integers.\r\n\r\nMost datasets are accompanied by documentation or data dictionaries,\r\nwhich are a description of the variables in the data set and other\r\nrelevant pieces of information. Be sure to read through these\r\nguidelines if you haven’t already.\r\nFor this task, the mutate() function will work with the\r\nfollowing syntax.\r\n\r\n\r\n# don't forget to write over your old dataframe using `kenya <-`\r\n\r\ndata <- data %>%\r\n  mutate(newVariableName = (oldVariable1 - oldVariable2))\r\n\r\n\r\n\r\nPipes (%>%)\r\nOne of the most useful tools in the tidyverse package is\r\na little thing called a pipe.\r\nIt’s represented with the symbol %>%, and allows us to\r\nexpress a series of operations in a continuous string of code, rather\r\nthan using the assignment operator over and over.\r\nAs an example, let’s pretend we have a dataset that is a record of\r\nbirds struck by aircraft in the United Sates over the past few decades.\r\nWe want to know the frequency distribution of sky conditions for birds\r\nstruck over 5,000 feet within the borders of North Carolina.\r\nIn base R, we might code it like this:\r\n\r\n\r\nbirds_NC <- subset(birds, state == \"NC\")\r\n\r\nbirds_NC <- subset(birds_NC, height > 5000)\r\n\r\ntable(birds_NC$sky)\r\n\r\n\r\n\r\nWith a pipe, %>%, both our code and output becomes\r\nmore tidy. More importantly, it’s easy to read:\r\n\r\n\r\nbirds %>%\r\n  filter(state == \"NC\", height > 5000) %>%\r\n  group_by(sky) %>%\r\n  count()\r\n\r\n\r\n\r\nThe pipe can be interpeted as signifying “and then”.\r\nIn the above example, the pipe tells the R console to take the dataset\r\nbirds, and then filter by\r\nstate and height, and then\r\ngroup by sky, and then count the\r\nobservations in each group.\r\nTask 7: Frequency\r\ndistributions of mage\r\nSuppose you want to break down mage into an ordinal\r\ncategorical variable with three categories. First, we might inspect the\r\nfrequency distribution (in one-way frequency table) for\r\nmage. Do this using table().\r\ntable() works by identifying unique values within a\r\nvariable, and then counts their occurence.\r\nIt works on character variables, categorical (factor) variables, and\r\neven numbers.\r\nAs was mentioned earlier, we can tell R to look at specific variables\r\ninside our dataframe with the $ sign. The syntax looks like\r\nthis:\r\n\r\n\r\ndataframeName$variableName\r\n\r\n\r\n\r\nIf we want to include a count of NA values in our table, we can also\r\nuse the argument useNA = 'always' within\r\ntable().\r\nUse table() to look at kenya$mage, then\r\nconsider the following questions:\r\nAre there any missing values for mage? If so, how\r\nmany?\r\nWhich range of ages appear the most frequently in\r\nmage?\r\nTask 8: Create variable\r\nmagec\r\nUsing mage, generate a new variable with three\r\ncategories: “<18”, “18-39”, and “≥ 40”, naming the new variable\r\nmagec\r\nmagec stands for: {m}other’s\r\n{age}\r\n{c}ategorical\r\nSet the values for magec to be 0,1,2, where 0\r\ncorresponds to the youngest age group (<18).\r\nWe recommend you do this in the following steps:\r\nUse a pipe (%>%) and then mutate() to\r\ncreate a new variable, magec,\r\nWithin your mutate() command, use\r\ncase_when() to create a series of conditional statements\r\nthat assign numbers 0, 1, and 2 to each category\r\nOn a new line of code, use factor() to assign labels to\r\neach level of your new variable\r\nFinally, once you get it to work, don’t forget to use the assignment\r\noperator to save your changes to the kenya data frame.\r\n\r\nValues:\r\n0: <18\r\n1: 18-39\r\n2: ≥ 40\r\ncase_when()\r\nThis function is used to create conditional rules when creating new\r\nvariables with mutate(). It allows you to create a series\r\nof if-then (conditional) statements based on variables within your data.\r\nAt first, the syntax for case_when() might strike you as a\r\nlittle overly complicated, especially for coding binary variables. But\r\nas your variables become more complex, case_when() really\r\nshines as a highly efficient way to create new variables on a series of\r\ncomplex conditions.\r\nAn example of the syntax is as follows:\r\n\r\n\r\ncase_when(size == 'small' ~ 0, \r\n          size == 'medium' ~ 1,\r\n          size == 'large' ~ 2,\r\n          TRUE ~ NA)\r\n\r\n\r\n\r\nHere, the tildes represents a formula. To the left\r\nof the formula is a logical operation that can evaluate to either\r\nTRUE or FALSE\r\n\r\nClick here\r\nfor a list of logical operators available in the R language\r\nTo the right of the tilde, we put the value that we want to return\r\nif the logical operation evaluates to\r\nTRUE. If it is FALSE or\r\nNULL, case_when() behaves by moving on and\r\ntesting the next conditional statement.\r\nIf it were run on a dataset containing a variable called\r\nsize, the literal translation of the above code would go\r\nsomething like this:\r\n\r\nFor each row in the dataset,  \r\n    * if `size` equals 'small', then return 0\r\n    * if `size` equals 'medium', then return 1\r\n    * if `size` equals 'large', then return 2\r\n    * if `size` is any other real value, then return `NA`\r\n\r\n\r\nThis sort of literal translation of a programming language into readable\r\nEnglish is what’s known as “pseudo-code”\r\nWhen we do this within a mutate() function, the returned\r\nvalues get assigned to the new variable for each row of the dataset as\r\nthey’re evaluated.\r\nThe right side of the formula needs to always produce a value of the\r\nsame variable type, but otherwise you have a\r\nhigh degree of freedom in what can be returned when the conditional\r\nstatement is TRUE, including mathematical operations on\r\nother variables.\r\nSee the documentation\r\nfor case_when() for more examples of this function’s\r\ncapabilities\r\nConvert a character\r\nvariable to a factor\r\nConvert a variable to a factor with the following syntax:\r\n\r\n\r\ndata$variableYouWantToFactor <- factor(data$variableYouWantToFactor,\r\n                                       labels = c(\"Label for 0\", \"Label for 1\", \"Label for 2\"))\r\n\r\n\r\n\r\nTask 9: Cross-tab of\r\nmage and magec\r\nLook at a cross-tabulation (two-way table) of mage and\r\nmagec to ensure that magec was created\r\ncorrectly. Be sure missing values were handled properly (all\r\nobservations that have a missing value for mage should be\r\nassigned the R missing value “NA” for magec). Try the two\r\nseparate methods for cross-tabulation, as we will be using both for\r\nseparate purposes later in the semester:\r\nMethod 1\r\nType “?table” in the console for help with how to create a 2x2 table.\r\nNote: the order of the variables in the command controls which one is in\r\nthe rows and which is in the columns. Experiment to make your table\r\nreadable.\r\n\r\n\r\n# example code:\r\n\r\ntable(data$x, data$y, useNA = 'always')\r\n\r\n\r\n\r\nMethod 2\r\nWe can also use tidyverse functions to accomplish a\r\ntwo-way tabulation of our variables of interest. These functions will\r\nbecome increasingly relevant and useful, and are a big reason why R is\r\nsuch a popular platform for data science. We will use a pipe\r\n(%>%), group_by(), another pipe, and\r\ncount() to get the same output given by\r\ntable().\r\n\r\n\r\n# example code:\r\n\r\n# notice that we don't want to assign this operation to a name\r\n# we just want to view the output, hence the lack of \"data <- \"\r\ndata %>%\r\n  group_by(x, y) %>%\r\n  count()\r\n\r\n\r\n\r\nA translation of the above code to written instructions would go as\r\nfollows, where and then represents the grammatical\r\nequivalent of our pipe, %>%:\r\n“Take dataset, data, and then\r\ngroup_by variable x, and within those groups,\r\ngroup_by variable y, and then\r\ncount the values in each of our groups.”\r\nHere’s a link if\r\nyou’re interested in learning more about pipes. Or just take a look at\r\nthis tweet:\r\n\r\n\r\n\r\nTask 10: Save new dataset\r\nUsing function saveRDS(), save the new dataset in the\r\nsame directory as our original data, using the following format:\r\n“firstInitial_YourLastName_lab0.rds”\r\nsaveRDS() takes two primary arguments:\r\nThe dataframe object you want to save\r\nThe location in which you’d like it saved as a .rds file\r\nDon’t forget the following:\r\nThe file locations should be in quotes, so that R knows to read it\r\nas a character string\r\nYour file should be saved in your local Lab 0 folder,\r\ndata/\r\n\r\n\r\n\r\n",
      "last_modified": "2022-05-31T13:53:23-04:00"
    },
    {
      "path": "guide_705_lab_1.html",
      "title": "Lab 1 (705)",
      "description": "__Due:__ `r read.csv('files/due_dates.csv')[2,2]` by `r read.csv('files/due_dates.csv')[2,3]`\n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\nLAB MATERIALS\r\nLab 1 Goals\r\nLab 2 Grading scheme\r\nTask 1: Load packages and\r\ndata\r\nTask 2: Recode variables\r\nTask2a:\r\nbord5\r\nTask 2b:\r\nmale\r\nTask 2c:\r\nmweight\r\nTask 2d:\r\nmheight\r\nTask 2e:\r\nmbmi\r\n\r\nTask 3: Frequency\r\nhistograms\r\nCreating histograms in\r\n{ggplot2}\r\n\r\nTask 4: Boxplots\r\nCreating a boxplot in\r\n{ggplot2}\r\n\r\nTask 5: Frequency table\r\n(Table 1)\r\nCross-tabulation\r\nwith group_by() and summarize()\r\n\r\nTask 6: Data dictionary\r\n(Table 2)\r\nTask 7: Short answer\r\n\r\n\r\n\r\n\r\nLAB MATERIALS\r\nR Markdown\r\nfile for Lab 1 Click link to download. Fill it in with your\r\nanswers to the following lab tasks. When you’re ready to submit, name it\r\nas Lab1_FirstinitialYourlastname.Rmd, and submit it using\r\nthe Sakai dropbox.\r\nExcel document for\r\nTables 1 & 2\r\nLab_1_kenya.rds - data file available on Sakai\r\nLab 1 Goals\r\nGenerate derived variables\r\nIdentify and recode special values\r\nRun descriptive statistics for continuous and categorical\r\nvariables\r\nGenerate graphics for continuous variables\r\nGenerate a complete data dictionary\r\nLab 2 Grading scheme\r\nCompetency\r\nPoints\r\n.Rmd file runs without error\r\n10\r\nTask 3 (Histograms)\r\n15 (5 each)\r\nTask 4 (Boxplots)\r\n15 (5 each)\r\nTask 5 (Table 1)\r\n10\r\nTask 6 (Data Dictionary - original\r\nvariables)\r\n20\r\nTask 6 (Data Dictionary - new\r\nvariables)\r\n10\r\nTask 7 (short answer)\r\n20\r\nTotal\r\n100\r\nTask 1: Load packages and\r\ndata\r\nIn the chunk of code named load packages, use the\r\nfunction library() to load packages {skimr} and\r\n{tidyverse}.\r\nIn the following chunk, named load data, use the\r\nfunction readRDS() to load in the dataset named\r\nLab_1_kenya.rds. Use the assignment operator,\r\n<- to give it the name kenya.\r\nReminder: To run a line of code from within\r\na code chunk in R Markdown, use the keyboard shortcut\r\nctrl + enter (Windows) / cmd + enter (Mac)\r\n\r\nFind more useful keyboard shortcuts here\r\nTask 2: Recode variables\r\nSimilar to Lab 0, construct the following derived variables (Tasks 2a\r\n- 2e) using a pipe (%>%) and mutate(). For\r\neach, examine the component variables for coded special values\r\n(i.e. “NA”) and be sure to set derived variable values appropriately.\r\nFor categorical variables, you will be expected to use\r\ncase_when() to create conditional clauses, just as you did\r\nin\r\nLab 0 to derive magec.\r\nSome helpful links:\r\nUsing\r\npipes and mutate() in the previous lab\r\nUsing\r\ncase_when() to derive magec\r\nMore\r\non pipes\r\nMore\r\non factors\r\nFor the categorical variables: Label all variables\r\nand the coded values for the categorical ones using the\r\nfactor() command. That is, add meaningful labels to both\r\nthe variable name itself and to the levels (i.e. categories) of the\r\ncategorical variable to make printed output more readable.\r\nIn the .Rmd named lab1_705_fall2021.Rmd, we have provided you with an\r\nexample of the correct coding of bord5. Please use this to\r\nassist you as you re-code the other variables that follow. Please code\r\neach new variable within the code chunk named after that variable.\r\nTask2a: bord5\r\nbord5: A dichotomous categorical\r\n(i.e. binary) variable indicating birth order of the child. You will use\r\nthe variable bord (birth order). Create the categories for\r\nbord5 according to the following levels and labels:\r\nLevel\r\nLabel\r\n0\r\nbirth order 1 through 4\r\n1\r\nbirth order 5+\r\nTask 2b: male\r\nmale: A dichotomous categorical\r\n(i.e. binary) variable indicating that the child is male. Based on\r\nvariable b4. Create the categories for male\r\naccording to the following levels and labels:\r\nLevel\r\nLabel\r\n0\r\nfemale\r\n1\r\nmale\r\nTask 2c:\r\nmweight\r\nmweight: Continuous variable for\r\nmaternal weight at time of interview (in kilograms). Based on variable\r\nv437. Note that v437 contains 1 implied\r\ndecimal place. Divide by 10 to get kilograms.\r\nTask 2d:\r\nmheight\r\nmheight: Continuous maternal height at\r\ntime of interview (in meters). Based on variable v438. Note\r\nthat this variable is in centimeters and also contains 1 implied decimal\r\nplace. Divide by 10 to get meters.\r\nTask 2e: mbmi\r\nmbmi: maternal body mass index (BMI).\r\nmweight / mheight^2.\r\nTask 3: Frequency histograms\r\nGenerate frequency histograms of the continuous variables\r\nmweight,\r\nmheight and\r\nmbmi using {ggplot2}. Put meaningful axis\r\nlabels and a title on each figure. Note that since we look at histograms\r\nof each of the variables individually, these are univariable\r\nvisualizations (i.e. one variable at a time).\r\nPlease read the description below on how to generate histograms in\r\n{ggplot}, then use the code chunks named mweight hist,\r\nmheight hist, and mbmi hist to generate each\r\nplot in its own designated chunk.\r\nCreating histograms in\r\n{ggplot2}\r\nThe code for creating histograms using ggplot is fairly\r\nstraightforward, and we will demonstrate using the starwars\r\ndataset. It provides a list of Star Wars characters, with details on\r\ntheir height, weight, gender, home world, and so on:\r\nstarwars is loaded into R every time you load the rest of\r\nthe tidyverse package.\r\n\r\nname\r\nheight\r\nmass\r\nhair_color\r\nskin_color\r\neye_color\r\nbirth_year\r\nsex\r\ngender\r\nhomeworld\r\nspecies\r\nLuke Skywalker\r\n172\r\n77\r\nblond\r\nfair\r\nblue\r\n19.0\r\nmale\r\nmasculine\r\nTatooine\r\nHuman\r\nC-3PO\r\n167\r\n75\r\nNA\r\ngold\r\nyellow\r\n112.0\r\nnone\r\nmasculine\r\nTatooine\r\nDroid\r\nR2-D2\r\n96\r\n32\r\nNA\r\nwhite, blue\r\nred\r\n33.0\r\nnone\r\nmasculine\r\nNaboo\r\nDroid\r\nDarth Vader\r\n202\r\n136\r\nnone\r\nwhite\r\nyellow\r\n41.9\r\nmale\r\nmasculine\r\nTatooine\r\nHuman\r\nLeia Organa\r\n150\r\n49\r\nbrown\r\nlight\r\nbrown\r\n19.0\r\nfemale\r\nfeminine\r\nAlderaan\r\nHuman\r\nOwen Lars\r\n178\r\n120\r\nbrown, grey\r\nlight\r\nblue\r\n52.0\r\nmale\r\nmasculine\r\nTatooine\r\nHuman\r\n\r\nSay we want to use ggplot to create a histogram of\r\ncharacter height. Where do we start?\r\n“gg” stands for Grammar of graphics. {ggplot2}\r\nallows us to follow simple rules, a “grammar”, to construct visually\r\nappealing graphics and plots.\r\nRegardless of what kind of visualization we want to create, the first\r\nstep will always be to initialize a graphical field using the command\r\nggplot(). We supply this function with our dataset using\r\nthe argument data =.\r\nWe can also supply it with universal “mapping aesthetics”. What does\r\nthat mean? Generally speaking, “aesthetics” are the different\r\nvariables we want to use in our plot. Eventually, we are going to add\r\n“layers” to the plot. A universal aesthetic just means that we\r\nwant the variable to apply to ALL layers. In the case of our histogram,\r\nwe want height to always be on the x-axis. So we will\r\ndesignate height as a universal aesthetic. We designate\r\naesthetics by putting them inside the aesthetic function,\r\naes().\r\n\r\n\r\nggplot(data = starwars, mapping = aes(x = height))\r\n\r\n\r\n\r\n\r\nNotice how the above plotting field is initialized, with character\r\nheight mapped to the x axis. Luckily,\r\nhistograms only take an x aesthetic, so we can generate our histogram by\r\nsimply adding a layer using geom_histogram(). {ggplot2}\r\nuses a plus sign, + to add layers\r\nto a plot. Here, we will add a histogram layer and a labels layer\r\n(labs())\r\n\r\n\r\nggplot(data = starwars, mapping = aes(x = height)) +\r\n  geom_histogram() +\r\n  labs(x = \"Height (cm)\", y = \"Count\", title = \"Histogram of Star Wars Character Height (cm)\")\r\n\r\n\r\n\r\n\r\nNavigate to this\r\npage for {ggplot2} resources. The page also includes our\r\nown tutorial that details the concepts specifically required to\r\ncomplete these labs.\r\nPlease pay special attention to how you label your visualizations,\r\nand be specific! Statisticians and others reviewing your research tend\r\nto be quite persnickety when it comes to titles and axis labels. And\r\nwith good reason! What use is a visualization without proper labels?\r\nFind many an anecdotal answer on r/dataisugly\r\nAs time permits, we encourage you to take advantage of the external\r\nresources provided in the sidebar in order to gain a better\r\nunderstanding of plotting with {ggplot2}\r\nTask 4: Boxplots\r\nNow you’re going to generate boxplots of mweight,\r\nmheight, and mbmi for the levels of\r\nmagec. Put meaningful axis labels and a title on each\r\nfigure. Note that since we look at boxplots of each of the variables\r\naccording to the levels of magec, these are bivariable\r\nvisualizations (i.e. showing two variables at a time).\r\nPlease use the appropriately labelled code chunks provided in your R\r\nMarkdown assignment document (labelled mweight box,\r\nmheight box, etc.) to generate your boxplots.\r\nCreating a boxplot in\r\n{ggplot2}\r\nThe steps to creating a boxplot are the same as those for creating a\r\nhistogram, except we will create the boxplot layer using\r\ngeom_boxplot() instead of geom_histogram().\r\nAdditionally, in order to stratify our plot, we will need to provide the\r\nplotting space with a y aesthetic, magec.\r\nIf you need a little more guidance, learn to create a boxplot here\r\nAnd learn how to create a stratified boxplot here\r\nTask 5: Frequency table (Table\r\n1)\r\nIf you haven’t already, download the Excel file that’s linked at\r\nthe top of this webpage. Then open the file, which is named\r\nLab1_Tables.xlsx.\r\nFill in the sheet called Table 1 with the frequency\r\ncounts and percentages for the levels of the 3 categorical variables you\r\nhave generated. Calculate percentages only for the non-missing values.\r\nRound percentages to 1 decimal place.\r\nIn the code chunks named bord5 freq, etc., we would like\r\nyou to use a pipe (%>%) with group_by(),\r\nsummarize(), and mutate() to generate summary\r\nstatistics. Please read the tutorial below to gain a better\r\nunderstanding of group_by() and\r\nsummarize().\r\nHint: To generate percentages based on sub-group\r\nfrequency counts, you will need to use mutate() to create\r\nanother column on your summary data frame that divides each frequency\r\ncount by the sum() of each subgroup’s total count.\r\nCross-tabulation\r\nwith group_by() and summarize()\r\ngroup_by() works by grouping rows into discrete\r\ncategories based on categorical variables in the data frame. It then\r\nallows a function like summarize() to calculate summary\r\nstatistics on those sub-groups, including percentages and frequency\r\ncounts.\r\nAs an example, we can use the mtcars dataset, which is\r\nalways available from within R:\r\n\r\n\r\nhead(mtcars)\r\n\r\n\r\n\r\n\r\n\r\nmpg\r\ncyl\r\ndisp\r\nhp\r\ndrat\r\nwt\r\nqsec\r\nvs\r\nam\r\ngear\r\ncarb\r\nMazda RX4\r\n21.0\r\n6\r\n160\r\n110\r\n3.90\r\n2.620\r\n16.46\r\n0\r\n1\r\n4\r\n4\r\nMazda RX4 Wag\r\n21.0\r\n6\r\n160\r\n110\r\n3.90\r\n2.875\r\n17.02\r\n0\r\n1\r\n4\r\n4\r\nDatsun 710\r\n22.8\r\n4\r\n108\r\n93\r\n3.85\r\n2.320\r\n18.61\r\n1\r\n1\r\n4\r\n1\r\nHornet 4 Drive\r\n21.4\r\n6\r\n258\r\n110\r\n3.08\r\n3.215\r\n19.44\r\n1\r\n0\r\n3\r\n1\r\nHornet Sportabout\r\n18.7\r\n8\r\n360\r\n175\r\n3.15\r\n3.440\r\n17.02\r\n0\r\n0\r\n3\r\n2\r\nValiant\r\n18.1\r\n6\r\n225\r\n105\r\n2.76\r\n3.460\r\n20.22\r\n1\r\n0\r\n3\r\n1\r\n\r\n\r\nFind out what other datasets are available within R by running the\r\ncommand library(help = \"datasets\")\r\nSay we want to know the frequency counts of cars with 4, 6, and 8\r\ncylinders (variable: cyl) based on whether or not a car has\r\nan automatic or manual transmission (variable am, 0 =\r\nautomatic, 1 = manual).\r\nWe can use group_by() to create subgroups according to\r\nam and cyl. Using group_by() by\r\nitself doesn’t alter the appearance of our data frame, but our\r\nobservations will now be grouped implicitly according to transmission\r\ntype and number of cylinders.\r\n\r\n\r\nmtcars %>%\r\n  group_by(am, cyl)\r\n\r\n\r\n\r\n\r\nmpg\r\ncyl\r\ndisp\r\nhp\r\ndrat\r\nwt\r\nqsec\r\nvs\r\nam\r\ngear\r\ncarb\r\n21.0\r\n6\r\n160\r\n110\r\n3.90\r\n2.620\r\n16.46\r\n0\r\n1\r\n4\r\n4\r\n21.0\r\n6\r\n160\r\n110\r\n3.90\r\n2.875\r\n17.02\r\n0\r\n1\r\n4\r\n4\r\n22.8\r\n4\r\n108\r\n93\r\n3.85\r\n2.320\r\n18.61\r\n1\r\n1\r\n4\r\n1\r\n21.4\r\n6\r\n258\r\n110\r\n3.08\r\n3.215\r\n19.44\r\n1\r\n0\r\n3\r\n1\r\n18.7\r\n8\r\n360\r\n175\r\n3.15\r\n3.440\r\n17.02\r\n0\r\n0\r\n3\r\n2\r\n18.1\r\n6\r\n225\r\n105\r\n2.76\r\n3.460\r\n20.22\r\n1\r\n0\r\n3\r\n1\r\n\r\nWe might use summarize() to operate on these sub-groups.\r\nsummarize() works with the following syntax:\r\n\r\n\r\nmtcars %>%\r\n  group_by(am, cyl) %>%\r\n  summarize(columnName = Function(variable), \r\n            anotherColumnName = anotherFunction(variable),\r\n            etc., etc....)\r\n\r\n\r\n\r\n\r\nIf you prefer the British English spelling of “summarise”, R also\r\nrecognizes summarise().\r\nDepending on whether our variable of interest is categorical or\r\ncontinuous, we can use summary statistic functions like n()\r\n(for raw counts), mean(), sd(),\r\nmedian(), IQR(), min(), and\r\nmax() within our summarize() function. Except\r\nfor certain functions like n(), which doesn’t take any\r\narguments, the rest of these functions take a variable name specifying\r\nwhich variable should be used in the calculation.\r\nWe can start by using n() for sub-group frequency\r\ncounts. Notice that, similar to the syntax used with\r\nmutate(), we name the summary statistic to the left of the\r\nequals sign, and specify the function to the right.\r\n\r\n\r\nmtcars %>%\r\n  group_by(am, cyl) %>%\r\n  summarize(Counts = n())\r\n\r\n\r\n\r\n\r\nam\r\ncyl\r\nCounts\r\n0\r\n4\r\n3\r\n0\r\n6\r\n4\r\n0\r\n8\r\n12\r\n1\r\n4\r\n8\r\n1\r\n6\r\n3\r\n1\r\n8\r\n2\r\n\r\nWe can also use mutate() to operate on our summary table\r\nas if it were its own data frame.\r\nWithin mutate(), we create a new variable with\r\nNewVariable =, and equate it to the following operation:\r\nCounts / sum(Counts, na.rm = TRUE) (sub-group counts\r\ndivided by the sum of sub-group counts).\r\nNote: na.rm = TRUE instructs the function\r\nto remove NA values from the sum of values in our sub-group.\r\n\r\n\r\nmtcars %>%\r\n  group_by(am, cyl) %>%\r\n  summarize(Counts = n()) %>%\r\n  mutate(Proportions = Counts / sum(Counts, na.rm = TRUE))\r\n\r\n\r\n\r\n\r\nam\r\ncyl\r\nCounts\r\nProportions\r\n0\r\n4\r\n3\r\n0.1578947\r\n0\r\n6\r\n4\r\n0.2105263\r\n0\r\n8\r\n12\r\n0.6315789\r\n1\r\n4\r\n8\r\n0.6153846\r\n1\r\n6\r\n3\r\n0.2307692\r\n1\r\n8\r\n2\r\n0.1538462\r\n\r\nAlso notice that we’re able to do this all within a single string of\r\ncode using %>%!\r\nTask 6: Data dictionary (Table\r\n2)\r\nIn your Excel spreadsheet, the sheet titled Table 2\r\nis our own data dictionary for the Kenya DHS dataset, with columns added\r\nto annotate the variables and provide summary statistics. Using output\r\nfrom skim():\r\nFill in the columns in this table for the variables originally in\r\nthe dataset.\r\nAdd rows for the 7 new variables that you have created in both Labs\r\n0 and 1.\r\nTask 7: Short answer\r\nEnter your response into your own RMarkdown file, under\r\nthe heading called “Task 7: Short answer”\r\nPrompt: Examine the range and proportion of missing\r\nvalues for each of the 7 variables you have created in Labs 0 and 1. Are\r\nthere characteristics of any of these variables that are concerning\r\n(e.g., missing, suspicious or impossible values)? In contemplating\r\nanalysis of these data, what do you think should be done with anomalous\r\ninformation? What effect would missing values have on the validity of\r\nyour analyses (e.g., how might missing or extreme values affect\r\ninferences)? (Response no longer than 250 words,\r\nplease)\r\n\r\n\r\n\r\n",
      "last_modified": "2022-05-31T13:53:26-04:00"
    },
    {
      "path": "help_ggplot2.html",
      "title": "Visualizations with {ggplot2}",
      "description": "Histograms, boxplots, and pointalism using {ggplot2}.\n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\nGoals:\r\nExternal resources\r\nFor\r\nthe visually inclined: RFun, Visualization with ggplot2\r\nwith John Little\r\nFor the\r\nlinguistically inclined:\r\nFor the\r\ntheoretically inclined:\r\nFor the obsessively\r\ninclined:\r\nFor the listically\r\ninclined:\r\n\r\nCore competencies for 705\r\nlab\r\nCreate a new ggplot with\r\nggplot()\r\nAdd a dataset to the\r\nplot\r\nAssign x and y\r\naesthetics with aes():\r\nCreate a histogram\r\nwith geom_histogram()\r\nCreate a boxplot with\r\ngeom_boxplot()\r\nGenerate a\r\nstratified boxplot by adding a y aesthetic:\r\nCreate titles and\r\naxis labels with labs()\r\nBONUS:\r\nGenerating multiple stratified plots with facet_wrap() or\r\nfilter()\r\n\r\n\r\nGoals:\r\nBy following along with this document, you will know how to:\r\nRead and interpret basic code in {ggplot2}’s grammar of\r\ngraphics\r\nInitialize a plotting field using ggplot()\r\nMap x and y aesthetics to a plot’s axes using\r\naes()\r\nGenerate histograms with geom_histogram()\r\nGenerate boxplots and stratified boxplots with\r\ngeom_boxplot()\r\nCreate titles and axis labels with labs()\r\nSave a plot with ggsave()\r\nBasically, this help document will provide you with the tools\r\nnecessary to complete the labs for GLHLTH 705.\r\nExternal resources\r\nIf you’re interested in going one level deeper, we highly recommend\r\nyou check out the following resources, which will give a better\r\nintroduction to {ggplot2} than we ever could (also why we’re plugging\r\nthese at the top).\r\nAs R rule of thumb, it’s good to have multiple mediums of exposure to\r\nthe same idea. We recommend you pick the one that suits your learning\r\nstyle and come back for more later:\r\nFor\r\nthe visually inclined: RFun,\r\nVisualization with ggplot2 with John Little\r\nJohn Little is nothing short of the world’s best librarian.\r\nFor the linguistically\r\ninclined:\r\nR for Data\r\nScience, Chapter 3, by Hadley Wickham\r\nAfter a preface and introduction, this is the first actual\r\nchapter in R4DS. The rationale is that {ggplot2} is actually pretty fun\r\nand satisfying to use. It’s pretty well guaranteed to have you hooked if\r\nyou give it a chance.\r\nFor the theoretically\r\ninclined:\r\n“A Layered\r\nGrammar of Graphics” by Hadley Wickham\r\nPublished in the Journal of Computational and Graphical Statistics,\r\n2010\r\nFor the obsessively\r\ninclined:\r\nggplot2:\r\nElegant Graphics for Data Analysis, by Hadely Wickham\r\n**cough** Also by Hadley Wickham. (look it up using the Duke Library\r\nsearch engine and log in with your Duke credentials\r\nFor the listically\r\ninclined:\r\nReference page of\r\nggplot2 commands\r\nCore competencies for 705\r\nlab\r\nFor those of you who made it through that onslaught of links without\r\nclicking on a single one, welcome to the Core Competencies section. We\r\nhope that this section is somehow dry enough that you go and find your\r\nanswers in one of the resources above. But for those of you who are\r\nstill feeling stubborn:\r\nCreate a new ggplot with\r\nggplot()\r\nTo initialize a plotting space, we first need to tell R that we want\r\nto use ggplot. If we just call ggplot() and run it without\r\nany data, we get a blank field. This is our sandbox:\r\n\r\n\r\nggplot()\r\n\r\n\r\n\r\n\r\nAdd a dataset to the plot\r\nFor this example, we’ll use the dataset available in base R called\r\niris, which provides measurements and species data on a\r\nbunch of – you guessed it – irises.\r\n\r\n\r\n\r\n\r\nSepal.Length\r\nSepal.Width\r\nPetal.Length\r\nPetal.Width\r\nSpecies\r\n5.1\r\n3.5\r\n1.4\r\n0.2\r\nsetosa\r\n4.9\r\n3.0\r\n1.4\r\n0.2\r\nsetosa\r\n4.7\r\n3.2\r\n1.3\r\n0.2\r\nsetosa\r\n4.6\r\n3.1\r\n1.5\r\n0.2\r\nsetosa\r\n5.0\r\n3.6\r\n1.4\r\n0.2\r\nsetosa\r\n5.4\r\n3.9\r\n1.7\r\n0.4\r\nsetosa\r\n\r\n\r\n\r\n\r\nFigure 1: A\r\nbearded iris\r\n\r\n\r\n\r\nWe can add the dataframe to our plot by including it in the argument\r\ndata =.\r\nSuperficially, this doesn’t change our output:\r\n\r\n\r\nggplot(data = iris)\r\n\r\n\r\n\r\n\r\nBut the data frame is now a part of the plot. One way to verify this\r\nis by assigning the two previous plots a name and inspecting their size\r\nwith object.size(). The plot with the data should be\r\nbigger:\r\n\r\n\r\nwithout_data <- ggplot()\r\n\r\nobject.size(without_data)\r\n#> 3720 bytes\r\n\r\nwith_data <- ggplot(data = iris)\r\n\r\nobject.size(with_data)\r\n#> 10704 bytes\r\n\r\n\r\n\r\nAssign x and y aesthetics\r\nwith aes():\r\nNext, we need to tell ggplot which variables we’re working with, and\r\nwhere to put them (their “aesthetic mapping”). We do this using the\r\nargument aes(x = variable1, y = variable2)\r\nIf we’re creating histograms and singular boxplots, we only require a\r\nsingle variable on the x-axis. We can initialize it as follows:\r\n\r\n\r\nggplot(data = iris, mapping = aes(x = Sepal.Length))\r\n\r\n\r\n\r\n\r\nSee how ggplot assigned Sepal.Length to the x axis?\r\nCreate a histogram with\r\ngeom_histogram()\r\nOkay, let’s cut to the chase. We want a\r\nplot.\r\n{ggplot2} has a large number of plotting types and styles. Given the\r\ntypes of variables we’ve mapped to ggplot’s aesthetics, all we need to\r\ndo is choose a type of plot appropriate for that type of variable, and\r\nadd it as a new layer with +\r\n\r\n\r\nggplot(data = iris, mapping = aes(x = Sepal.Length)) +\r\n  geom_histogram()\r\n\r\n\r\n\r\n\r\nMany plots also allow us to add color with the argument\r\naes(fill = \"colorname\") (colors are always written as\r\nstrings, in quotes!).\r\nWe may also change the size of our bins with argument\r\nbinwidth = x:\r\n\r\n\r\nggplot(data = iris, mapping = aes(x = Sepal.Length)) +\r\n  geom_histogram(fill = \"#12BBAC\", binwidth = .25)\r\n\r\n\r\n\r\n\r\n\r\nRefer here\r\nfor a list of color options available in R. Along with default color\r\nnames, R also accepts hexadecimal color codings. For\r\nfun, I’ve made our histogram the same color as this website’s navbar.\r\nDid it work?\r\nCreate a boxplot with\r\ngeom_boxplot()\r\nA single boxplot functions in the same exact manner. Instead of\r\ngeom_histogram(), we add a boxplot layer with\r\ngeom_boxplot(). This time, I’ve used a default\r\ncolor name, goldenrod2, instead of a hexadecimal color\r\ncode:\r\n\r\n\r\nggplot(data = iris, mapping = aes(x = Sepal.Length)) +\r\n  geom_boxplot(fill = 'goldenrod2')\r\n\r\n\r\n\r\n\r\nGenerate\r\na stratified boxplot by adding a y aesthetic:\r\nWe can create multiple boxplots within a single plot by adding a\r\ncategorical variable as a second aesthetic. The iris\r\ndataset contains a categorical variable, Species, which\r\nwould be appropriate for this task:\r\n\r\n\r\nggplot(data = iris, mapping = aes(x = Sepal.Length, y = Species)) +\r\n  geom_boxplot(fill = '#AC12BB')\r\n\r\n\r\n\r\n\r\nWe can also display the boxplots vertically by assigning\r\nSepal.Length to y = and Species\r\nto x = :\r\n\r\n\r\nggplot(data = iris, mapping = aes(y = Sepal.Length, x = Species)) +\r\n  geom_boxplot(fill = '#AC12BB')\r\n\r\n\r\n\r\n\r\nCreate titles and axis\r\nlabels with labs()\r\nFinally, we need to make our plots fit for public use… it needs axis\r\nlabels and a title. We can specify these by adding another layer to our\r\nplot, labs(). Make sure you write your labels as\r\nstrings:\r\n\r\n\r\nggplot(data = iris, mapping = aes(y = Sepal.Length, x = Species)) +\r\n  geom_boxplot(fill =  \"#12BBAC\") + \r\n  labs(x = \"Species\", \r\n       y = \"Sepal Length\", \r\n       title = \"Boxplots of Sepal Length of Irises by Species\")\r\n\r\n\r\n\r\n\r\nBONUS:\r\nGenerating multiple stratified plots with facet_wrap() or\r\nfilter()\r\nYou might be wondering what this sort of stratification might look\r\nlife if we tried the same thing with a histogram. Can a histogram accept\r\na y aesthetic? When we try and assign a second aesthetic to a histogram,\r\nwe get the following result:\r\n\r\n\r\nggplot(data = iris, mapping = aes(x = Species, y = Sepal.Length)) +\r\n  geom_histogram(fill = \"goldenrod2\")\r\n\r\n#> Error: stat_bin() can only have an x or y aesthetic.\r\n\r\n\r\n\r\nfacet_wrap()\r\nAn easy way to generate stratified histograms is with the additional\r\nlayer, facet_wrap(), which takes a formula in the following\r\nsyntax:\r\n. ~ stratifyingVariable\r\nThe period here represents our ggplot object. We put the stratifying\r\nvariable on the right side of the formula as a way to designate it as\r\nthe “independent variable” of sorts. The output, . ,\r\ndepends on whatever categorical we assign as our faceting\r\nvariable. In this case, the histograms dependson the variable\r\nSpecies:\r\n\r\n\r\nggplot(data = iris, mapping = aes(x = Sepal.Length)) + \r\n  geom_histogram(fill = \"goldenrod2\") +\r\n  facet_wrap(. ~ Species) + \r\n    labs(x = \"Sepal Length\", \r\n         y = \"Count\",\r\n         title = \"Histograms of Sepal Length of Irises by Species\")\r\n\r\n\r\n\r\n\r\nWe might decide that we want the plots stacked vertically instead of\r\nhorizontally to help us better compare their distributions by Sepal\r\nLength. We can do that too, with the argument nrow =:\r\n\r\n\r\nggplot(data = iris, mapping = aes(x = Sepal.Length)) + \r\n  geom_histogram(fill = \"#AC12BB\") +\r\n  facet_wrap(. ~ Species, nrow = 3) +\r\n  labs(x = \"Sepal Length\",\r\n       y = \"Count\",\r\n       title = \"Histograms of Sepal Length of Irises by Species\")\r\n\r\n\r\n\r\n\r\npipe and filter()\r\nWhat if we only want the Sepal Lengths for the species Iris\r\nvirginica?\r\nOne way would be to use filter(), which we connect to\r\nour ggplot using a pipe:\r\n\r\n\r\niris %>%\r\n  filter(Species == \"virginica\") %>%\r\n  ggplot(mapping = aes(x = Sepal.Length)) +\r\n  geom_histogram(fill = \"#12BBAC\") + \r\n  labs(x = \"Sepal Length\", \r\n       title = \"Histograms of Sepal Length of Irises by Species\")\r\n\r\n\r\n\r\n\r\n\r\nNote that when we use a pipe for a ggplot, we’ve already specified the\r\ndataset at the top of the code, so we don’t need to type it again when\r\nwe call ggplot(). It already knows what data we’re working\r\nwith because iris is funneled along the pipeline!\r\n\r\n\r\n\r\n",
      "last_modified": "2022-05-31T13:53:31-04:00"
    },
    {
      "path": "help_jargon.html",
      "title": "R Jargon",
      "description": "How to talk like an R nerd. Here, we detail various R-related vocabulary, the familiarity of which will make your life a lot easier.  \n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\nR Vocabulary List\r\n$\r\n#\r\nargument\r\nassignment operator\r\nbase R\r\ncode chunk\r\nconsole\r\ndebugging\r\ndirectory\r\ndplyr\r\nfactor\r\nfunction\r\nggplot2\r\ngit\r\nHTML\r\nindex\r\nknit\r\nlibrary/packages\r\nmagrittr/pipe/%>%\r\nparameter\r\nreproducible expressions\r\n(reprex)\r\nstring/character\r\ntibble\r\ntidy data\r\ntidyverse\r\ntraceback\r\ntribble\r\nvector\r\nvignette\r\nWickham, Hadley\r\nYAML header\r\n\r\n\r\nDoes a pesky word keep popping up that’s given you the sneaking\r\nsuspicion that you have no idea what you’re doing?\r\nI’ve got good news, but also some bad news.\r\nThe bad: The person writing this probably has no\r\nidea what they’re doing either. And unless you study computer\r\nengineering, there’s a good chance you will never really “know” what you\r\nare “doing”.\r\nThe good: A quick vocabulary lesson will help do\r\naway with that sneaking suspicion, at least temporarily.\r\nR Vocabulary List\r\nSearch this document using ctrl+f for a brief definition of that word\r\nor symbol and external links for further reading. Can’t find the word\r\nhere? Try R Documentation or Google.\r\n$\r\nUsed to specify a specific variable within a list-like object (like a\r\ndata.frame, a tibble, a model, an actual list created with\r\nlist())\r\nExample:\r\n\r\n\r\n# this code uses head() to view the first six observations for the variable \"cyl\" in dataset \"mtcars\"\r\n\r\nhead(mtcars$cyl)\r\n#> [1] 6 6 4 6 8 6\r\n\r\n\r\n\r\n#\r\nIn a chunk of code, # is used to comment out text. This\r\ntells the R console to ignore that line. This is useful for making\r\ncomments in your code.\r\nOutside of code chunks, a series of hashes on a new line, followed by\r\na space and some text, indicate different levels for document\r\nsubheadings, like this:\r\nIn R Markdown, type:\r\n# one hash\r\n## two hashes\r\n### three hashes\r\n#### four hashes\r\n##### five hashes\r\nKnitting in HTML renders as:\r\n one hash \r\n two hashes \r\n three hashes \r\n four hashes \r\n five hashes \r\nargument\r\nThe various pieces of data necessary for a function to run. Many\r\nfunctions have arguments with default values.\r\nFor example, for many statistical tests of significance, the\r\nsignificance level is set to a default of 0.95. If you don’t include\r\nthis argument in your function, it will refer to the default and use\r\nit.\r\nIn a function’s documentation, under “Usage”, you can tell when an\r\nargument has a default because the argument’s name is equated to a\r\nvalue.\r\nAs another example, the function head() takes two main\r\narguments, an object, x, and n, the number of\r\nobservations you’d like for head() to print.\r\nGo to the help documentation for head() (?head) and find\r\nwhat its default is. Or maybe you already have noticed its default when\r\nyou’ve run head() in your labs.\r\nassignment operator\r\nThe <- is called the Assignment\r\nOperator. We can use it to assign names to objects in our\r\ncoding environment:\r\nWe can use our assignment operator for characters, numbers, logical\r\noperators, etc.:\r\n\r\n\r\nfruit <- c(\"oranges\", \"papayas\", \"apricots\")\r\n\r\nnumber <- 99\r\n\r\nlogical <- FALSE\r\n\r\n\r\n\r\nNow that the above values are stored in our environment, we can use\r\nthem in other functions or operations:\r\n\r\n\r\npaste(fruit, \"are orange\", sep = \" \")\r\n#> [1] \"oranges are orange\"  \"papayas are orange\"  \"apricots are orange\"\r\n\r\nnumber + 1\r\n#> [1] 100\r\n\r\nisTRUE(logical)\r\n#> [1] FALSE\r\n\r\n\r\n\r\nbase R\r\nThese are functions that are a part of the original R programming\r\nlanguage, and so do not require a call to a package using\r\nlibrary().\r\nGo\r\nhere to see a complete list of functions that come with the R Base\r\nPackage\r\ncode chunk\r\nIn R Markdown, code chunks look like this:\r\n```{r}\r\n```\r\nAnything written between these two lines can be sent to the R Console\r\nand run as code. Anything not bound within these lines is interpreted as\r\ntext, and is printed as-is in a rendered document.\r\nconsole\r\nIn lieu of running code in your R Markdown document, you can type it\r\ndirectly into the window that says “Console”.\r\nWhen you click “Run” on any R Markdown code, that code gets run in\r\nthe Console.\r\ndebugging\r\nThe process of identifying and fixing problems in your code. A\r\ndebugger is a program that walks through your code, line-by-line,\r\nallowing you to inspect elements within the environment as the code\r\nruns.\r\nThis becomes more useful when you’re writing your own functions and\r\nare confused as to why they’re behaving a certain way.\r\ndirectory\r\n“The\r\nworking directory of a process is a directory of a hierarchical file\r\nsystem”\r\nA directory is any file folder on your computer.\r\nYour root directory is the top-most directory on\r\nyour computer (in Windows, this is the folder calls “C:”, for Mac users,\r\nit’s usually labelled as “Macintosh HD”)\r\nIn R, your working directory is typically the folder\r\nthat contains whatever R\r\nProject you have open.\r\nSay you have a dataset called “data.rds” in your main working\r\ndirectory. You can import it using any number of functions by referring\r\nto its file path as simply “data.rds”.\r\nBut say you have that dataset in a series of folders within your\r\nworking directory. The folders are organized like this, which each\r\nsubsequent folder inside the last, and the data file in the folder\r\ntitled “data”:\r\nworking directory > try1 > fullAnalysis >\r\ndata\r\nThe “file path” for referring to your data file from your working\r\ndirectory would be this:\r\n“try1/fullAnalysis/data/data.rds”\r\ndplyr\r\nA package that contains a set of functions that help solve “the most\r\ncommon data manipulation challenges”. Main functions include:\r\nmutate()\r\nselect()\r\nfilter()\r\nsummarize() (or summarise())\r\narrange()\r\nHere’s the\r\nchapter from R for Data Science\r\nYou may also find this vignette\r\nhelpful\r\nfactor\r\nA data type that is the preferred way to store categorical variables\r\nin R.\r\nUsing factor(), you can convert:\r\na character to a factor: This takes all unique\r\nvalues of a character variable and assigns them an underlying number, or\r\n“levels”.\r\nFor example:\r\n\r\n\r\ndumplings <- c(\"momos\", \"pop tarts\", \"raviolis\", \"momos\", \"empanadas\", \"pierogis\")\r\n\r\nfactor(dumplings)\r\n#> [1] momos     pop tarts raviolis  momos     empanadas pierogis \r\n#> Levels: empanadas momos pierogis pop tarts raviolis\r\n\r\n\r\n\r\nan integer to a factor: This takes integers and\r\nmakes the lowest number the base level, and the next highest 1, the next\r\nhighest 2, etc. You can assign a “label” to each respective level with\r\n“labels = c()”, with a vector, c(), with the same length as the number\r\nof levels.\r\n\r\n\r\nintegers <- c(1, 2, 3, 2, 3, 2, 1)\r\n\r\nfactor(integers,\r\n       labels = c(\"chicken\", \"egg\", \"rooster\"))\r\n#> [1] chicken egg     rooster egg     rooster egg     chicken\r\n#> Levels: chicken egg rooster\r\n\r\n\r\n\r\nUnderstanding factors mostly takes time. If you want to speed that\r\nup, here’s the R for Data\r\nScience chapter on factors\r\nIn it, they reference a few articles for further reading:\r\nWrangling categorical\r\ndata in R\r\nstringsAsFActors:\r\nAn unauthorized biography\r\nstringsAsFactors\r\n=  – this one is kinda funny\r\nfunction\r\nA “self-contained” piece of code that takes a predefined type of\r\ninput data (arguments), operates on it, and returns an output or\r\nresult.\r\nggplot2\r\nThe “gg” in ggplot2 stands for “grammar of\r\ngraphics”.\r\nTo get a high-level overview of ggplot2 basics, I highly recommend this\r\nintroduction to data visualization.\r\nIf you’re really trying to nerd out, you can access the free full\r\ntext of ggplot2:\r\nElegant Graphics for Data Analysis by Hadley Wickham\r\nby looking it up using the Duke Library\r\nsearch engine and logging in with your Duke credentials\r\ngit\r\nYou may have heard of GitHub, a\r\npopular website for version control, collaboration, and code\r\nsharing.\r\nGit is the underlying software that GitHub runs on. It’s free and\r\nopen source. You won’t be expected to use a Git repository for your\r\nprojects in this class, but it’s nice to know what’s out there.\r\nThis massive book is\r\navailable for those who would like to learn more. I think the first and\r\nsecond sections, “Getting Started” and “Git Basics”, are good places to\r\nstart.\r\nHTML\r\nStands for HyperText Markup Language. It’s the standard language used\r\nfor documents that are meant to be displayed in a web browser, and is\r\nhighly customizable. When R Markdown renders to HTML, it does almost all\r\nof the heavy lifting for you.\r\nindex\r\nNot particularly important for Fall semester, but is an important\r\nconcept to understand when working with data.\r\nIn programming, an index is a numerical representation of an item’s\r\nposition in a sequence.\r\nIn R, indexes start at number 1 (as opposed to other languages that\r\nstart at 0).\r\nYou can refer to an item’s index with [ ].\r\nLETTERS gives us a character vector of every letter of the\r\nalphabet\r\n\r\n\r\nLETTERS\r\n#> [1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\" \"G\" \"H\" \"I\" \"J\" \"K\" \"L\" \"M\" \"N\" \"O\"\r\n#> [16] \"P\" \"Q\" \"R\" \"S\" \"T\" \"U\" \"V\" \"W\" \"X\" \"Y\" \"Z\"\r\n\r\n\r\n\r\nWe can refer to individual letters by calling their index:\r\n\r\n\r\nLETTERS[1]\r\n#> [1] \"A\"\r\n\r\nLETTERS[5]\r\n#> [1] \"E\"\r\n\r\nLETTERS[26]\r\n#> [1] \"Z\"\r\n\r\n\r\n\r\nData Frames and Tibbles can be indexed using syntax\r\n[row, column]. So data[1,1] would call the value in the\r\nfirst row of the first column, data[2,1] would call the\r\nvalue in the second row of the first column, and so on.\r\nknit\r\nThe button at the top of your R Markdown document that instructs the\r\ndocument to render as its designated output. The standard outputs for R\r\nMarkdown are HTML, Word, and PDF. But there are an ever expanding set of\r\nR Markdown outputs available to R users. This entire website was created\r\nusing a document output type called “Distill”\r\nlibrary/packages\r\nI’ll quote from an\r\nanswer in StackOverflow for this one:\r\n“In R, a package is a collection of R functions, data and compiled\r\ncode. The location where the packages are stored is called the\r\nlibrary.”\r\nmagrittr/pipe/%>%\r\nChapter from R for Data\r\nScience gives a nice intro.\r\nFor you nerds, here’s\r\na history of the pipe operator in R\r\nparameter\r\nUsed interchangeably with “argument”\r\nreproducible expressions\r\n(reprex)\r\nWhen you encounter a problem in your code that you just can’t figure\r\nout, it’s often best to create a reproducible\r\nexpression. This allows whoever is helping you to recreate\r\nyour problem in their own R console.\r\nCreating a “reprex” often entails trimming your code to the bare\r\nessentials and isolating whatever step in your code is causing it to hit\r\nan error.\r\nIt might also be the case that you need to create toy data. To do\r\nthis, you can use the following tools:\r\ntibble() to build a dataset from vectors of equal length\r\nrunif() to create a vector of n length of random floating point\r\nvalues\r\nseq() to create a vector of n length in a defined sequence\r\nsample() to draw n random samples, with replace = TRUE/FALSE, from a\r\npre-existing vector\r\nx:y – use a colon to generate a series of integers from x to y\r\nc() with comma separated values to designate a vector\r\nstring/character\r\nStrings are created with either single quotes or double quotes. It\r\nindicates that a value is meant to be read as-is, rather than\r\ntransformed according to R’s computational rules for numbers and logical\r\nvalues.\r\nFor example, writing the logical value, TRUE as a string makes it\r\nunrecognizable to R as logical:\r\n\r\n\r\na <- TRUE\r\nisTRUE(a)\r\n#> [1] TRUE\r\n\r\nb <- \"TRUE\"\r\nisTRUE(b)\r\n#> [1] FALSE\r\n\r\n\r\n\r\nAs simple as it sounds, strings are a topic of mind-numbing\r\ncomplexity, as the underlying encoding of text strings governs the way\r\nthat code is able to interact with data as well as other code.\r\nR for Data Science gives a nice introduction\r\nto the mechanics of strings here, but hints at their wider\r\nimplications in its\r\nchapter on data importation.\r\nRegular expressions (not to be confused with reproducible\r\nexpressions), or regex, are their own beast, and may help you understand\r\nhow databases and search engines work\r\ntibble\r\nLike a data.frame object, but with enhanced “printing”.\r\nRead this\r\nsection on tibbles from R for Data Science to learn more.\r\ntidy data\r\n“Tidy datasets are all alike, but every messy dataset is messy in its\r\nown way.” - Hadley Wickham\r\nThe first few sections in this chapter from R for\r\nData Science gives a nice introduction.\r\nThe basic ideas behind tidy data are defined by these three\r\nrules:\r\nEach variable must have its own column\r\nEach observation must have its own row\r\nEach value must have its own cell\r\nYou can read more about the underlying theory in this article that was published in the\r\nJournal of Statistical Software.\r\ntidyverse\r\n“The tidyverse is an\r\nopinionated collection of R packages designed for data science. All\r\npackages share an underlying design philosophy, grammar, and data\r\nstructures.”\r\nFamiliar packages include:\r\ndplyr\r\nggplot2\r\ntibble\r\ntraceback\r\nIf you hit an error, use traceback() to print a summary\r\nof how your program/code arrived at that error. In simple terms, it’s\r\ntracing your steps prior to your code hitting an error.\r\ntribble\r\nShort for “transposed tibble”, it’s a function that allows you to\r\ncreate a tibble by hand, with the syntax and subsequent output:\r\n\r\n\r\ntribble(\r\n  ~colA, ~colB, ~colC, ~colD,\r\n  \"a\",   1, \"Square\", \"orange\",\r\n  \"b\",   2, \"Circle\", \"maracuya\",\r\n  \"c\",   3, \"Rhombus\", \"cashew\"\r\n)\r\n\r\n\r\n\r\n\r\ncolA\r\ncolB\r\ncolC\r\ncolD\r\na\r\n1\r\nSquare\r\norange\r\nb\r\n2\r\nCircle\r\nmaracuya\r\nc\r\n3\r\nRhombus\r\ncashew\r\n\r\n?tribble in the R Console for more details\r\nvector\r\nA vector is a list of values, all of the same type.\r\nWe use c() to create a vector, separating items with\r\ncommas when we specify them individually.\r\nThe following are all valid vectors:\r\n\r\n\r\n# a vector of numbers 1, 2, 3:\r\nc(1, 2, 3)\r\n#> [1] 1 2 3\r\n\r\n# a vector of numbers 1 through 12, and then 20:\r\nc(1:12, 20)\r\n#> [1]  1  2  3  4  5  6  7  8  9 10 11 12 20\r\n\r\n# a vector of logical values:\r\nc(TRUE, FALSE, NA, TRUE)\r\n#> [1] \"TRUE\"  \"FALSE\" \"TRUE\" \r\n\r\n# a vector of 5 values randomly drawn from a uniform distribution with min 0 and max 1:\r\nrunif(5)\r\n#> [1] 0.97420369 0.04387835 0.68818071 0.13420150 0.30322080\r\n\r\n\r\n\r\nMost operations on vectors apply to each value individually:\r\n\r\n\r\nx <- c(1:5)\r\n\r\nx + 2\r\n#> [1] 3 4 5 6 7\r\n\r\nx * 2\r\n#> [1]  2  4  6  8  10\r\n\r\n\r\n\r\nAs such, a data frame is just a list of\r\nvectors of all the same length. That\r\nlist is what’s known formally as a “recursive\r\nvector”. Lists can contain other lists.\r\nThis is a somewhat complex topic. If you’re really hungry for more\r\ninfo, this chapter in\r\nR for Data Science is highly informative, but may be confusing at first\r\nfor those without any programming background.\r\nvignette\r\nA vignette is a long-form guide to a package. It highlights a\r\npackage’s main functions and their usage. Learning from vignettes is one\r\nof the best ways to self-teach yourself a skill in R.\r\nWickham, Hadley\r\nA kiwi and a statistician who probably authored 80% of the links on\r\nthis page. He is known for the tidyverse, the book R for Data Science,\r\nand his\r\ntwitter.\r\nYAML header\r\nA short blob of text at the top of your R Markdown document\r\nspecifying things like the document’s title, time and date stamp, and\r\nthe document’s output type.\r\nThe YAML header is part of what makes R Markdown such a flexible\r\ndocument. There are many ways to customize your R Markdown output. We\r\nwon’t get into those.\r\nJust know that at the top of your R Markdown document that says\r\noutput: html_document is what instructs your it to\r\nautomatically knit as an HTML file.\r\n\r\n\r\n\r\n",
      "last_modified": "2022-05-31T13:53:34-04:00"
    },
    {
      "path": "help_packages.html",
      "title": "Packages",
      "description": "Help with installing and understanding packages, at home and in the wild.\n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\nTerminology\r\nInstalling packages\r\nLoading libraries\r\n\r\nTerminology\r\nPackages and libraries\r\nPackages are a collection of R functions, data, and compiled\r\ncode.\r\nA library is the location where the packages are stored on your\r\ncomputer.\r\nUsage: “Install the {tidyverse} package from CRAN. It will\r\nautomatically save to your R library.”\r\nCRAN\r\nCRAN stands for the Comprehensive R Archive Network. Along with\r\nproviding downloads and updates to the base R computing software, CRAN\r\nis the main source for stable, published versions of packages. The\r\nfunction install.packages() automatically looks in CRAN for\r\nwhatever package you name inside the function.\r\n\r\nUsing other installation functions, it’s also possible to download\r\npackages from various other sources, like GitHub. But most of the time,\r\nthose packages are still in development, which means various features\r\ncould become inoperable over time (as opposed to making defunct code\r\nbackwards-compatible). Getting packages from CRAN is the best way to\r\nensure that your code will still run months and sometimes years into the\r\nfuture\r\nObjects, functions, and\r\narguments\r\nObject refers to any data structure in R, with\r\nspecial attributes and methods according to the object type. Most\r\nobjects are open to exploration and transformation.\r\nExamples of objects: Data frames, linear\r\nmodels, vectors, tables, variables, ggplot objects, matrices, etc.\r\nFunctions are the verbs of the R language. They\r\noperate on objects in your R environment, and behave according to their\r\narguments. On this website, functions are written in\r\nmonospace font and followed by a pair of parentheses,\r\n()\r\nExamples of functions: mean(),\r\nsummary(), factor(), exp()\r\nArguments are the settings and information supplied\r\nto a given function. Each new argument should be separated from the rest\r\nwith a comma.\r\nSome functions don’t require any arguments, like\r\nR.Version() and sessionInfo(). These\r\nfunctions, when run, supply information about the R environment, but\r\ndon’t operate on an object.\r\nOther functions only require a single argument: an object. For\r\nexample, the function head() takes many different types of\r\nobjects, and supplies the first 6 observations within a given\r\nobject.\r\nBut say you want a different number of observations.\r\nhead() allows you to manually set those observations with\r\nn =.\r\nTyping head(object, n = 10) will return the first 10\r\nobservations in a given object.\r\nMost functions come with default arguments that are submitted to the\r\nfunction implicitly. In the case of head(), the default\r\nmethod for n = is 6.\r\nALL arguments can be specified with the syntax\r\nargumentName = value. But most functions are written to\r\nrecognize certain types of values as belonging to a specific argument,\r\nand operate on them accordingly.\r\nSo head(object, 10) will also give the first 10\r\nobservations of object, since head() knows to\r\nrecognize an object of type integer as belonging to\r\nn =.\r\nYou can view any function’s arguments by going to its documentation.\r\nIn the R console, run ?functionName, and it will appear in\r\nthe help window. Under the “Usage” section, you will find information on\r\nthe function’s syntax and arguments. If an argument is alone, without an\r\nequals sign, =, that argument doesn’t have default and must\r\nbe submitted by the user to run. However, if a given argument contains\r\nan equals sign and a value to the right of the equals sign, that value\r\nis the argument’s default.\r\nDocumentation\r\nAny function or package’s documentation is the primary authority on\r\nthe appropriate usage of that function. Typically, a function’s\r\ndocumentation will contain:\r\nA brief description of the function\r\nAn overview of the functin’s usage\r\nA detailed description of each argument needed for the function to\r\nrun\r\nFurther function details\r\nValue, which indicates what gets returned when the function is\r\nrun\r\nExamples of the function in use\r\nThe documentation for any function is available via the R console by\r\nrunning ?functionName.\r\nSometimes, you can find documentation that goes into greater detail\r\nby searching the function’s documentation online.\r\nWhen you are having problems with your code, it’s smart to check the\r\nfunction’s documentation before looking elsewhere. With practice and a\r\nlittle persistence, you will become more comfortable with reading and\r\ninterpreting help documentation.\r\nInstalling packages\r\nPackages are collections of functions. As we’ll see shortly, we use\r\nfunctions as code to view, manipulate, and analyze our data.\r\nThere are packages that come built-in with R. These have names like\r\n{base}, {utils}, and {stats}.\r\nSince R is open-source, users are able to create their own packages\r\nso that other R users can use them. These packages are available in\r\nplaces like the\r\nComprehensive R Archive Network (CRAN for short) and GitHub.\r\nLucky for us, packages are easily retrieved from the R Console. If\r\nyou haven’t already, run the following code from your console to\r\ndownload the packages that we’ll be needing for Fall Semester.\r\nYou only need to do this once:\r\n\r\n\r\ninstall.packages(\"tidyverse\")\r\ninstall.packages(\"skimr\")\r\ninstall.packages(\"epiR\")\r\ninstall.packages(\"devtools\")\r\ndevtools::install_github(\"potato-nathan/epiAssist\")\r\n\r\n\r\n\r\nLoading libraries\r\nTo enable R to use a package’s functions in our current project\r\nenvironment, we need to load the packages using the\r\nlibrary() function.\r\nIn a fresh code chunk, call in the {tidyverse} and {skimr} packages\r\nusing the following code:\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(skimr)\r\n\r\n\r\n\r\nNotice that when we install packages, we need to specify\r\ntheir names using quotes, but when we load them into R using\r\nlibrary(), R recognizes them as package objects\r\nautomatically, and so don’t need quotes. You can surround them with\r\nquotes and it will load all the same.\r\n\r\n\r\n\r\n",
      "last_modified": "2022-05-31T13:53:35-04:00"
    },
    {
      "path": "help_projects.html",
      "title": "Creating a New Project",
      "description": "This page provides advice on establishing a workflow and creating new projects with RStudio\n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\nCreate folders to store\r\nyour work\r\nCreate a New Project in\r\nRStudio\r\n\r\nIn R, we organize our work by projects. It is best practice to keep\r\nno more than one project in any single folder on your computer. First,\r\nwe should establish the file folders from which we’ll be working.\r\nCreate folders to store your\r\nwork\r\nIf you haven’t already, now would be a good time to:\r\nCreate a new folder on your computer called “705 Lab”.\r\nWithin that folder, create a folder for whatever lab you’re working\r\non, “Lab X”.\r\nFinally, within that folder, create a folder for your data, called\r\n“data”\r\nSave your dataset into the folder called “data”. This helps keep\r\nthings organized as the project grows.\r\nPlease make this a habit. We will expect you to do this for\r\nevery lab, as it will keep your work organized and will keep\r\nyou happy.\r\nCreate a New Project in\r\nRStudio\r\nNow open RStudio and take a deep breath. Don’t panic. This will all\r\nbe very familiar in a few short months.\r\nInitiate a new project by going to File and\r\nclicking New Project.\r\nSelect Existing Directory:\r\nInitiate a new project in an existing\r\ndirectoryIn the following window, hit Browse.\r\nNavigate to the folder where you’d like to house your new project (or\r\ncreate a new one if you haven’t already. Open it, then hit\r\nOpen.\r\n\r\n\r\n\r\n",
      "last_modified": "2022-05-31T13:53:36-04:00"
    },
    {
      "path": "help_rstudio.html",
      "title": "Help with RStudio",
      "description": "This page provides links to introduce you to the RStudio environment.\n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\nRStudio Basics\r\nThe R Environment\r\nManaging window panes\r\nChange RStudio color\r\ntheme\r\nRMarkdown keyboard\r\nshortcuts\r\nSession\r\ninformation and help documentation\r\n\r\nExtended resource\r\nguide to RStudio\r\n\r\nRStudio Basics\r\nThe R Environment\r\n\r\n\r\n\r\nA: File (New project, New file > R Markdown)B: Pop-out window (also works in R Markdown)C: Manage window panesD: New fileE: Find packages, check for updates, manage environment\r\noptionsF: Find cheatsheets (dplyr, ggplot2, R Markdown)G: Current R ProjectH: Console historyI: Plot viewerJ: File pathK: Browse help documentationL: Click to return to working directory homeM: Click to expand or shrink window pane or…N: Drag to manage window size\r\nManaging window panes\r\nDid you lose track of one of your window panes? Use the View\r\ntab in the top bar to return to a view with all panes\r\n\r\nChange RStudio color theme\r\nIs the white RStudio background burning your retnas? Need a change of\r\nscenery? No need to go outside!\r\nIn the top bar, go to Tools > Global Options, and then\r\nselect “Appearance”:\r\nChange appearanceRMarkdown keyboard shortcuts\r\nYou can find me mashing the following keys in\r\nWindows:\r\nShortcut\r\nKeys (Windows)\r\nKeys (MacOS)\r\nRun a line of code\r\n(With typing cursor on line) ctrl +\r\nenter\r\n(With typing cursor on line) cmd +\r\nenter\r\nRun a chunk of code\r\n(With typing cursor in chunk) ctrl +\r\nshift + enter\r\n(With typing cursor in chunk) cmd +\r\nshift + enter\r\nCreate a new chunk\r\nctrl + alt + i\r\ncmd + option + i\r\nType a pipe\r\nctrl + shift + m\r\ncmd + shift + m\r\nType an assignment operator\r\nalt + -\r\noption + -\r\nStop running\r\n(In Console) esc\r\n(In Console) esc\r\nSession information\r\nand help documentation\r\nIf for some reason your code is hitting an error, here are some\r\nuseful commands you might run in your RStudio Console to help\r\ntroubleshoot:\r\nFunction Documentation\r\nCommand: ?functionName\r\nWhat it does: Pulls up the documentation for\r\nwhatever you type in place of functionName, and displays it\r\nin the Help window. A function’s documentation is often (but not always)\r\nthe final word on how a function is meant to behave in R. It will\r\nprovide information on the function’s arguments\r\n(and their default values), the resulting output (“Values”), and will\r\ngive you examples of how to implement that function.\r\nSession information\r\nCommand: sessionInfo()\r\nWhat it does: If you’re experiencing issues with a\r\ncertain package, maybe it’s time for an update. You can check your\r\ncurrent version of R using this command. Find information on specific\r\npackages by using the argument ‘package = packageName’.\r\nYou can also find which version of RStudio you have by running\r\nRStudio.Version()\r\nView Traceback\r\nCommand: traceback()\r\nWhat it does: If your code keeps hanging up on an\r\nerror, you can often find clues as to why it’s happening by viewing the\r\nunderlying code that’s causing your code to halt. Traceback will print\r\nthe list of functions that were called before the error occurred. It’s\r\nliterally “tracing your steps” right before the error happened. The\r\nlanguage itself in the traceback is usually pretty cryptic and hard to\r\nread at first. Don’t let that stop you from looking at it when you\r\nencounter problems.\r\nMemory storage information\r\nCommand: object_size()\r\nWhat it does: For the purposes of this lab, you most\r\nlikely won’t need to worry about your computer’s RAM. Nevertheless, the\r\nnerds among us might find it interesting to inspect the size of an\r\nobject that we’ve saved to our R Environment. Use\r\nobject_size(objectName) to inspect a given object.\r\nAlternatively, inspect overall RAM usage and limitations with\r\nmemory.size() and memory.limit().\r\nExtended resource guide to\r\nRStudio\r\nR Studio will be the launching pad for all of our lab assignments in\r\nthis course.\r\nThe RStudio environment itself can feel overwhelming at first.\r\nLuckily, there are excellent resources already available to help get you\r\nacquainted:\r\nFor those of you who prefer to learn by reading, this\r\nwebsite’s first tutorial, “Getting started with R/RStudio”,\r\nis a good place to start.\r\nDr. Eric Green used to run a summer workshop called “I Eat Data\r\nScience for Breakfast”. He gives a stellar tutorial to the RStudio\r\nenvironment starting at 10:49 in\r\nhis Week 1 video.\r\n\r\n\r\n\r\nFigure 1: I Eat Data Science for Breakfast\r\n\r\n\r\n\r\nIf you were checking your email at all over the summer, you may also\r\nalready be familiar with Duke Librarian, John Little, and his series\r\ncalled RFun. Feel free to review his\r\nvideo introducing the RStudio environment\r\n\r\n\r\n\r\nFigure 2: RFun with\r\nJohn Little\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2022-05-31T13:53:37-04:00"
    },
    {
      "path": "index.html",
      "title": "Biostat Lab",
      "description": "The hub for DGHI Biostat & Epi lab assignments, resource pages, and whirly gigs\n",
      "author": [],
      "contents": "\r\n\r\n\r\n\r\nFigure 1: Confounding\r\nVariables\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2022-05-31T13:53:38-04:00"
    },
    {
      "path": "LICENSE.html",
      "author": [],
      "contents": "\r\nAttribution 4.0 International\r\n=======================================================================\r\nCreative Commons Corporation (“Creative Commons”) is not a law firm\r\nand does not provide legal services or legal advice. Distribution of\r\nCreative Commons public licenses does not create a lawyer-client or\r\nother relationship. Creative Commons makes its licenses and related\r\ninformation available on an “as-is” basis. Creative Commons gives no\r\nwarranties regarding its licenses, any material licensed under their\r\nterms and conditions, or any related information. Creative Commons\r\ndisclaims all liability for damages resulting from their use to the\r\nfullest extent possible.\r\nUsing Creative Commons Public Licenses\r\nCreative Commons public licenses provide a standard set of terms and\r\nconditions that creators and other rights holders may use to share\r\noriginal works of authorship and other material subject to copyright and\r\ncertain other rights specified in the public license below. The\r\nfollowing considerations are for informational purposes only, are not\r\nexhaustive, and do not form part of our licenses.\r\n Considerations for licensors: Our public licenses are\r\n intended for use by those authorized to give the public\r\n permission to use material in ways otherwise restricted by\r\n copyright and certain other rights. Our licenses are\r\n irrevocable. Licensors should read and understand the terms\r\n and conditions of the license they choose before applying it.\r\n Licensors should also secure all rights necessary before\r\n applying our licenses so that the public can reuse the\r\n material as expected. Licensors should clearly mark any\r\n material not subject to the license. This includes other CC-\r\n licensed material, or material used under an exception or\r\n limitation to copyright. More considerations for licensors:\r\nwiki.creativecommons.org/Considerations_for_licensors\r\n\r\n Considerations for the public: By using one of our public\r\n licenses, a licensor grants the public permission to use the\r\n licensed material under specified terms and conditions. If\r\n the licensor's permission is not necessary for any reason--for\r\n example, because of any applicable exception or limitation to\r\n copyright--then that use is not regulated by the license. Our\r\n licenses grant only permissions under copyright and certain\r\n other rights that a licensor has authority to grant. Use of\r\n the licensed material may still be restricted for other\r\n reasons, including because others have copyright or other\r\n rights in the material. A licensor may make special requests,\r\n such as asking that all changes be marked or described.\r\n Although not required by our licenses, you are encouraged to\r\n respect those requests where reasonable. More considerations\r\n for the public: \r\nwiki.creativecommons.org/Considerations_for_licensees\r\n=======================================================================\r\nCreative Commons Attribution 4.0 International Public License\r\nBy exercising the Licensed Rights (defined below), You accept and\r\nagree to be bound by the terms and conditions of this Creative Commons\r\nAttribution 4.0 International Public License (“Public License”). To the\r\nextent this Public License may be interpreted as a contract, You are\r\ngranted the Licensed Rights in consideration of Your acceptance of these\r\nterms and conditions, and the Licensor grants You such rights in\r\nconsideration of benefits the Licensor receives from making the Licensed\r\nMaterial available under these terms and conditions.\r\nSection 1 – Definitions.\r\nAdapted Material means material subject to Copyright and Similar\r\nRights that is derived from or based upon the Licensed Material and in\r\nwhich the Licensed Material is translated, altered, arranged,\r\ntransformed, or otherwise modified in a manner requiring permission\r\nunder the Copyright and Similar Rights held by the Licensor. For\r\npurposes of this Public License, where the Licensed Material is a\r\nmusical work, performance, or sound recording, Adapted Material is\r\nalways produced where the Licensed Material is synched in timed relation\r\nwith a moving image.\r\nAdapter’s License means the license You apply to Your Copyright\r\nand Similar Rights in Your contributions to Adapted Material in\r\naccordance with the terms and conditions of this Public\r\nLicense.\r\nCopyright and Similar Rights means copyright and/or similar\r\nrights closely related to copyright including, without limitation,\r\nperformance, broadcast, sound recording, and Sui Generis Database\r\nRights, without regard to how the rights are labeled or categorized. For\r\npurposes of this Public License, the rights specified in Section\r\n2(b)(1)-(2) are not Copyright and Similar Rights.\r\nEffective Technological Measures means those measures that, in\r\nthe absence of proper authority, may not be circumvented under laws\r\nfulfilling obligations under Article 11 of the WIPO Copyright Treaty\r\nadopted on December 20, 1996, and/or similar international\r\nagreements.\r\nExceptions and Limitations means fair use, fair dealing, and/or\r\nany other exception or limitation to Copyright and Similar Rights that\r\napplies to Your use of the Licensed Material.\r\nLicensed Material means the artistic or literary work, database,\r\nor other material to which the Licensor applied this Public\r\nLicense.\r\nLicensed Rights means the rights granted to You subject to the\r\nterms and conditions of this Public License, which are limited to all\r\nCopyright and Similar Rights that apply to Your use of the Licensed\r\nMaterial and that the Licensor has authority to license.\r\nLicensor means the individual(s) or entity(ies) granting rights\r\nunder this Public License.\r\nShare means to provide material to the public by any means or\r\nprocess that requires permission under the Licensed Rights, such as\r\nreproduction, public display, public performance, distribution,\r\ndissemination, communication, or importation, and to make material\r\navailable to the public including in ways that members of the public may\r\naccess the material from a place and at a time individually chosen by\r\nthem.\r\nSui Generis Database Rights means rights other than copyright\r\nresulting from Directive 96/9/EC of the European Parliament and of the\r\nCouncil of 11 March 1996 on the legal protection of databases, as\r\namended and/or succeeded, as well as other essentially equivalent rights\r\nanywhere in the world.\r\nYou means the individual or entity exercising the Licensed Rights\r\nunder this Public License. Your has a corresponding meaning.\r\nSection 2 – Scope.\r\nLicense grant.\r\nSubject to the terms and conditions of this Public License, the\r\nLicensor hereby grants You a worldwide, royalty-free, non-sublicensable,\r\nnon-exclusive, irrevocable license to exercise the Licensed Rights in\r\nthe Licensed Material to:\r\nreproduce and Share the Licensed Material, in whole or in part;\r\nand\r\nproduce, reproduce, and Share Adapted Material.\r\n\r\nExceptions and Limitations. For the avoidance of doubt, where\r\nExceptions and Limitations apply to Your use, this Public License does\r\nnot apply, and You do not need to comply with its terms and\r\nconditions.\r\nTerm. The term of this Public License is specified in Section\r\n6(a).\r\nMedia and formats; technical modifications allowed. The Licensor\r\nauthorizes You to exercise the Licensed Rights in all media and formats\r\nwhether now known or hereafter created, and to make technical\r\nmodifications necessary to do so. The Licensor waives and/or agrees not\r\nto assert any right or authority to forbid You from making technical\r\nmodifications necessary to exercise the Licensed Rights, including\r\ntechnical modifications necessary to circumvent Effective Technological\r\nMeasures. For purposes of this Public License, simply making\r\nmodifications authorized by this Section 2(a)\r\nnever produces Adapted Material.\r\n\r\nDownstream recipients.\r\nOffer from the Licensor – Licensed Material. Every recipient of\r\nthe Licensed Material automatically receives an offer from the Licensor\r\nto exercise the Licensed Rights under the terms and conditions of this\r\nPublic License.\r\nNo downstream restrictions. You may not offer or impose any\r\nadditional or different terms or conditions on, or apply any Effective\r\nTechnological Measures to, the Licensed Material if doing so restricts\r\nexercise of the Licensed Rights by any recipient of the Licensed\r\nMaterial.\r\n\r\nNo endorsement. Nothing in this Public License constitutes or may\r\nbe construed as permission to assert or imply that You are, or that Your\r\nuse of the Licensed Material is, connected with, or sponsored, endorsed,\r\nor granted official status by, the Licensor or others designated to\r\nreceive attribution as provided in Section 3(a)(1)(A)(i).\r\n\r\nOther rights.\r\nMoral rights, such as the right of integrity, are not licensed\r\nunder this Public License, nor are publicity, privacy, and/or other\r\nsimilar personality rights; however, to the extent possible, the\r\nLicensor waives and/or agrees not to assert any such rights held by the\r\nLicensor to the limited extent necessary to allow You to exercise the\r\nLicensed Rights, but not otherwise.\r\nPatent and trademark rights are not licensed under this Public\r\nLicense.\r\nTo the extent possible, the Licensor waives any right to collect\r\nroyalties from You for the exercise of the Licensed Rights, whether\r\ndirectly or through a collecting society under any voluntary or waivable\r\nstatutory or compulsory licensing scheme. In all other cases the\r\nLicensor expressly reserves any right to collect such\r\nroyalties.\r\n\r\nSection 3 – License Conditions.\r\nYour exercise of the Licensed Rights is expressly made subject to the\r\nfollowing conditions.\r\nAttribution.\r\nIf You Share the Licensed Material (including in modified form),\r\nYou must:\r\nretain the following if it is supplied by the Licensor with the\r\nLicensed Material:\r\nidentification of the creator(s) of the Licensed Material and any\r\nothers designated to receive attribution, in any reasonable manner\r\nrequested by the Licensor (including by pseudonym if\r\ndesignated);\r\na copyright notice;\r\na notice that refers to this Public License;\r\na notice that refers to the disclaimer of warranties;\r\na URI or hyperlink to the Licensed Material to the extent\r\nreasonably practicable;\r\n\r\nindicate if You modified the Licensed Material and retain an\r\nindication of any previous modifications; and\r\nindicate the Licensed Material is licensed under this Public\r\nLicense, and include the text of, or the URI or hyperlink to, this\r\nPublic License.\r\n\r\nYou may satisfy the conditions in Section 3(a)(1) in any\r\nreasonable manner based on the medium, means, and context in which You\r\nShare the Licensed Material. For example, it may be reasonable to\r\nsatisfy the conditions by providing a URI or hyperlink to a resource\r\nthat includes the required information.\r\nIf requested by the Licensor, You must remove any of the\r\ninformation required by Section 3(a)(1)(A) to the extent reasonably\r\npracticable.\r\nIf You Share Adapted Material You produce, the Adapter’s License\r\nYou apply must not prevent recipients of the Adapted Material from\r\ncomplying with this Public License.\r\n\r\nSection 4 – Sui Generis Database Rights.\r\nWhere the Licensed Rights include Sui Generis Database Rights that\r\napply to Your use of the Licensed Material:\r\nfor the avoidance of doubt, Section 2(a)(1) grants You the right\r\nto extract, reuse, reproduce, and Share all or a substantial portion of\r\nthe contents of the database;\r\nif You include all or a substantial portion of the database\r\ncontents in a database in which You have Sui Generis Database Rights,\r\nthen the database in which You have Sui Generis Database Rights (but not\r\nits individual contents) is Adapted Material; and\r\nYou must comply with the conditions in Section 3(a) if You Share\r\nall or a substantial portion of the contents of the database.\r\nFor the avoidance of doubt, this Section 4 supplements and does not\r\nreplace Your obligations under this Public License where the Licensed\r\nRights include other Copyright and Similar Rights.\r\nSection 5 – Disclaimer of Warranties and Limitation of Liability.\r\nUNLESS OTHERWISE SEPARATELY UNDERTAKEN BY THE LICENSOR, TO THE\r\nEXTENT POSSIBLE, THE LICENSOR OFFERS THE LICENSED MATERIAL AS-IS AND\r\nAS-AVAILABLE, AND MAKES NO REPRESENTATIONS OR WARRANTIES OF ANY KIND\r\nCONCERNING THE LICENSED MATERIAL, WHETHER EXPRESS, IMPLIED, STATUTORY,\r\nOR OTHER. THIS INCLUDES, WITHOUT LIMITATION, WARRANTIES OF TITLE,\r\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, NON-INFRINGEMENT,\r\nABSENCE OF LATENT OR OTHER DEFECTS, ACCURACY, OR THE PRESENCE OR ABSENCE\r\nOF ERRORS, WHETHER OR NOT KNOWN OR DISCOVERABLE. WHERE DISCLAIMERS OF\r\nWARRANTIES ARE NOT ALLOWED IN FULL OR IN PART, THIS DISCLAIMER MAY NOT\r\nAPPLY TO YOU.\r\nTO THE EXTENT POSSIBLE, IN NO EVENT WILL THE LICENSOR BE LIABLE\r\nTO YOU ON ANY LEGAL THEORY (INCLUDING, WITHOUT LIMITATION, NEGLIGENCE)\r\nOR OTHERWISE FOR ANY DIRECT, SPECIAL, INDIRECT, INCIDENTAL,\r\nCONSEQUENTIAL, PUNITIVE, EXEMPLARY, OR OTHER LOSSES, COSTS, EXPENSES, OR\r\nDAMAGES ARISING OUT OF THIS PUBLIC LICENSE OR USE OF THE LICENSED\r\nMATERIAL, EVEN IF THE LICENSOR HAS BEEN ADVISED OF THE POSSIBILITY OF\r\nSUCH LOSSES, COSTS, EXPENSES, OR DAMAGES. WHERE A LIMITATION OF\r\nLIABILITY IS NOT ALLOWED IN FULL OR IN PART, THIS LIMITATION MAY NOT\r\nAPPLY TO YOU.\r\nThe disclaimer of warranties and limitation of liability provided\r\nabove shall be interpreted in a manner that, to the extent possible,\r\nmost closely approximates an absolute disclaimer and waiver of all\r\nliability.\r\nSection 6 – Term and Termination.\r\nThis Public License applies for the term of the Copyright and\r\nSimilar Rights licensed here. However, if You fail to comply with this\r\nPublic License, then Your rights under this Public License terminate\r\nautomatically.\r\nWhere Your right to use the Licensed Material has terminated\r\nunder Section 6(a), it reinstates:\r\nautomatically as of the date the violation is cured, provided it\r\nis cured within 30 days of Your discovery of the violation; or\r\nupon express reinstatement by the Licensor.\r\nFor the avoidance of doubt, this Section 6(b) does not affect any\r\nright the Licensor may have to seek remedies for Your violations of this\r\nPublic License.\r\nFor the avoidance of doubt, the Licensor may also offer the\r\nLicensed Material under separate terms or conditions or stop\r\ndistributing the Licensed Material at any time; however, doing so will\r\nnot terminate this Public License.\r\nSections 1, 5, 6, 7, and 8 survive termination of this Public\r\nLicense.\r\nSection 7 – Other Terms and Conditions.\r\nThe Licensor shall not be bound by any additional or different\r\nterms or conditions communicated by You unless expressly\r\nagreed.\r\nAny arrangements, understandings, or agreements regarding the\r\nLicensed Material not stated herein are separate from and independent of\r\nthe terms and conditions of this Public License.\r\nSection 8 – Interpretation.\r\nFor the avoidance of doubt, this Public License does not, and\r\nshall not be interpreted to, reduce, limit, restrict, or impose\r\nconditions on any use of the Licensed Material that could lawfully be\r\nmade without permission under this Public License.\r\nTo the extent possible, if any provision of this Public License\r\nis deemed unenforceable, it shall be automatically reformed to the\r\nminimum extent necessary to make it enforceable. If the provision cannot\r\nbe reformed, it shall be severed from this Public License without\r\naffecting the enforceability of the remaining terms and\r\nconditions.\r\nNo term or condition of this Public License will be waived and no\r\nfailure to comply consented to unless expressly agreed to by the\r\nLicensor.\r\nNothing in this Public License constitutes or may be interpreted\r\nas a limitation upon, or waiver of, any privileges and immunities that\r\napply to the Licensor or You, including from the legal processes of any\r\njurisdiction or authority.\r\n=======================================================================\r\nCreative Commons is not a party to its public licenses.\r\nNotwithstanding, Creative Commons may elect to apply one of its public\r\nlicenses to material it publishes and in those instances will be\r\nconsidered the “Licensor.” The text of the Creative Commons public\r\nlicenses is dedicated to the public domain under the CC0 Public Domain\r\nDedication. Except for the limited purpose of indicating that material\r\nis shared under a Creative Commons public license or as otherwise\r\npermitted by the Creative Commons policies published at\r\ncreativecommons.org/policies, Creative Commons does not authorize the\r\nuse of the trademark “Creative Commons” or any other trademark or logo\r\nof Creative Commons without its prior written consent including, without\r\nlimitation, in connection with any unauthorized modifications to any of\r\nits public licenses or any other arrangements, understandings, or\r\nagreements concerning use of licensed material. For the avoidance of\r\ndoubt, this paragraph does not form part of the public licenses.\r\nCreative Commons may be contacted at creativecommons.org.\r\n\r\n\r\n",
      "last_modified": "2022-05-31T13:53:39-04:00"
    },
    {
      "path": "mind_map.html",
      "title": "Mapping Our Steps",
      "description": "This page gives a birds-eye view of our analysis, where we've come from and where we're headed.\n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\nThe data science life\r\ncycle\r\n\r\nThe data science life cycle\r\nIn Hadley Wickam and Garrett Grolemund’s book, R for Data\r\nScience, they introduce a model of tools found in the typical\r\ndata analysis project:\r\nDS Life CycleThis process consists of:\r\nimporting your data into R;\r\ntidying your data by shaping and storing it in a\r\nconsistent format (luckily, this has largely already been done for\r\nus).\r\nOnce the data is tidy and coherent, we will need to:\r\ntransform the data by narrowing in on observations\r\nof interest, creating new variables, and calculating summary\r\nstatistics;\r\nvisualize it to gain new perspective and\r\ninsights;\r\nmodel the data with a well-defined research\r\nquestion.\r\nAnd this will all be for naught if without effectively\r\ncommunicating our findings.\r\nIn the lab section for Biostat 705 and 707, we’ll be working in a\r\nsimilar loop. To this toolbox, we might also add Workflow\r\nManagement, which encompasses the bulk of what we do.\r\nThe below visualization aims to provide a visualization of this as we\r\nencounter it in-action.\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2022-05-31T13:53:41-04:00"
    },
    {
      "path": "mind_map_first.html",
      "title": "Mapping Our Steps",
      "description": "This page gives a birds-eye view of our analysis, where we've come from and where we're headed.\n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2022-05-31T13:53:42-04:00"
    },
    {
      "path": "submit.html",
      "title": "Lab Submission Guidelines",
      "description": "Submit your lab according to these guidelines to ensure you receive full points.\n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\n\r\nContents\r\nMaterials to submit\r\nFile names\r\n\r\nNotes:\r\nSubmitting\r\nRounding\r\nGrading:\r\n\r\n\r\nSubmit your documents electronically via the SAKAI\r\nDropBox by 14:00 ET on the date given for each respective lab (SAKAI\r\ntime-stamps DropBox submissions).\r\nMaterials to submit\r\nR Markdown (.Rmd) file with:\r\nyour name in the YAML header\r\nrequisite code under each task subheading\r\n\r\n.html file - the file that results from hitting “Knit” on your R\r\nMarkdown document\r\nExcel (.xls/.xlsx) file with tables (except for Lab 0, where this\r\ndoesn’t apply)\r\nFile names\r\nName all files so that the beginning of the file name is\r\n“LabX”, where “X” is the number that corresponds with that lab\r\nassignment (no spaces between “Lab” and “X”).\r\nEXAMPLE:\r\nIf Liz Turner is submitting her lab materials for Lab 2, her files\r\nmight have the following names: * Lab2_LTurner.Rmd *\r\nLab2_LTurner.html * Lab2_LTurner.xlsx\r\nNotes:\r\nSubmitting\r\nThe Rmarkdown file should run from start to finish with no\r\nerrors, including calling in the analysis dataset, and should produce\r\nall of the results and graphs to complete this assignment.\r\nLate policy: 5 point deduction per 24 hour period past due date\r\nand time.\r\nRounding\r\nWhen completing tables, follow the rounding patterns specified below.\r\nThis pattern will help in grading this assignment.\r\nDo not round intermediate values. Only round final answers for\r\nsubmission.\r\nIf rounding for a given table is not specified or not clear, ask\r\nyour lab instructor for clarification.\r\nUnless specified within the task instructions, use 3\r\ndecimal places for risks and differences and 2 decimal places for rates\r\nand ratios.\r\nTo aid in our grading of these assignments, the last figure kept\r\nshould be rounded UP when the first figure dropped is >= 5. For\r\nexample, in rounding to 2 decimal places, 0.235 should be reported as\r\n0.24 and 0.245 should be reported as 0.25. Note that this method will\r\nintroduce a small bias to higher numbers, but another method,\r\n“rounding-to-even”, confuses many users.\r\nGrading:\r\nRMarkdown files will be graded on an all or nothing. If your\r\nmarkdown file runs from start to finish with no errors, you will receive\r\nfull credit. If it hits an error message, you will receive NO credit\r\n(for the purposes of grading, if any “chunk” of code in your file\r\ncontains special options, we will ask you to resubmit a file omitting\r\nthem).\r\n\r\nR MARKDOWN FILE: Your markdown file MUST have\r\nthe following components:\r\nan informative YAML header (see guide to Lab 0[TODO])\r\na set of commands calling in all relevant packages\r\na line of code calling in the analysis dataset\r\neach task’s code performed under the appropriate subheading\r\n\r\nTABLES: a certain number of points are allocated\r\nfor each item. Errors include but are not limited to: incorrect values,\r\nomissions, and inappropriate rounding. For ease of grading, please do\r\nnot alter the dimensions of any table. If you are confused on which\r\nvalues go where, please reach out to your instructors for\r\nclarification.\r\nSHORT ANSWER QUESTIONS: will generally be graded\r\nas follows: 20% of allocated points for some sign of sentience; 40% of\r\nallocated points for getting at least part of the answer; 60% of\r\nallocated points for a pretty good answer; 80% of allocated points for\r\nan excellent, solid answer; 100% of allocated points for an\r\nout-of-the-ballpark answer.\r\n\r\n\r\n\r\n",
      "last_modified": "2022-05-31T13:53:43-04:00"
    }
  ],
  "collections": []
}
